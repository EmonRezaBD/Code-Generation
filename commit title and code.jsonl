{"Commit title": "LibUnicode: Optimize the canonical composition algorithm implementation", "Commit body": "@@ -208,42 +208,52 @@ static void canonical_ordering_algorithm(Span<u32> code_points)\n//See Section 3.11, D115 of Version 15.0.0 of the Unicode Standard.\nstaticboolis_blocked(Span<u32> code_points,size_ta,size_tc)\n{\nif(!is_starter(code_points[a]) ||a == c -1)\nif(a == c -1)\nreturnfalse;\nautoconstc_combining_class =Unicode::canonical_combining_class(code_points[c]);\nautoconstb_combining_class =Unicode::canonical_combining_class(code_points[c -1]);\nreturnb_combining_class==0|| b_combining_class>= c_combining_class;\nreturnb_combining_class >= c_combining_class;\n}\n\n//The Canonical Composition Algorithm, as specified in Version 15.0.0 of the Unicode Standard.\n//See Section 3.11, D117; and UAX #15 https://unicode.org/reports/tr15\n//https://www.unicode.org/versions/Unicode15.0.0/ch03.pdf#G50628\nstaticvoidcanonical_composition_algorithm(Vector<u32>& code_points)\n{\nif(code_points.size() <=1)\nreturn;\nssize_tlast_starter =is_starter(code_points[0]) ?0: -1;\nfor(size_ti =1; i < code_points.size(); ++i) {\nautoconstcurrent_character = code_points[i];\n//R1. Seek back (left) to find the last Starter L preceding C in the character sequence\nfor(ssize_tj = i -1; j >=0; --j) {\nif(!is_starter(code_points[j]))\ncontinue;\n//R2. If there is such an L, and C is not blocked from L,\n//and there exists a Primary Composite P which is canonically equivalent to <L, C>,\n//then replace L by P in the sequence and delete C from the sequence.\nif(is_blocked(code_points.span(), j, i))\ncontinue;\nif(last_starter == -1) {\nif(is_starter(current_character))\nlast_starter = i;\ncontinue;\n}\n//R2. If there is such an L, and C is not blocked from L,\n//and there exists a Primary Composite P which is canonically equivalent to <L, C>,\n//then replace L by P in the sequence and delete C from the sequence.\nif(is_blocked(code_points.span(), last_starter, i)) {\nif(is_starter(current_character))\nlast_starter = i;\ncontinue;\n}\n\nautocomposite =combine_hangul_code_points(code_points[j], current_character);\nautocomposite =combine_hangul_code_points(code_points[last_starter], current_character);\n\nif(composite ==0)\ncomposite =combine_code_points(code_points[j], current_character);\nif(composite ==0)\ncomposite =combine_code_points(code_points[last_starter], current_character);\n\nif(composite !=0) {\ncode_points[j] = composite;\ncode_points.remove(i);\n--i;\nbreak;\n}\nif(composite ==0) {\nif(is_starter(current_character))\nlast_starter = i;\ncontinue;\n}\n\ncode_points[last_starter] = composite;\ncode_points.remove(i);\n--i;\n}\n}\n\n"}
{"Commit title": "Bump org.mockito:mockito-core from 4.11.0 to 5.12.0", "Commit body": "@@ -16,7 +16,7 @@ ext {\nreactiveStreamsVersion=\"1.0.4\"\njunitVersion=\"4.13.2\"\ntestNgVersion=\"7.5\"\nmockitoVersion=\"4.11.0\"\nmockitoVersion=\"5.12.0\"\njmhLibVersion=\"1.21\"\nguavaVersion=\"33.2.0-jre\"\n}\n"}
{"Commit title": "#22244fix typo", "Commit body": "@@ -70,7 +70,7 @@ tree.sequences.node.tip = MariaDB Sequences\n\nmanager.catalog.name=Catalog manager\n\nparameters.all.caches= Cachemeta data\nparameters.all.caches= Cachemetadata\nparameters.all.caches.tip= Read tables constraints and columns at the stage of reading tables.\\nThis setting may reduce metadata loading performance for small databases and increase for large databases.\n\nmeta.org.jkiss.dbeaver.ext.mysql.model.MySQLCatalog.name.name=Schema Name\n"}
{"Commit title": "AK: Use correct wide integer type for qhat check in UFixedBigIntDivision", "Commit body": "@@ -95,7 +95,7 @@ constexpr void div_mod_internal(\nqhat =div_mod_words(dividend[i -1], dividend[i], divisor_approx, rhat);\n\nautois_qhat_too_large = [&] {\nreturnUFixedBigInt<native_word_size> { qhat }.wide_multiply(divisor[divisor_len -2]) >u128{ dividend[i -2], rhat };\nreturnUFixedBigInt<native_word_size> { qhat }.wide_multiply(divisor[divisor_len -2]) >UFixedBigInt<native_word_size *2>{ dividend[i -2], rhat };\n};\nif(is_qhat_too_large()) {\n--qhat;\n"}
{"Commit title": "#20060Revert JSQLParser to 4.5", "Commit body": "@@ -9,7 +9,7 @@\n<name>DBeaver - 3rd party dependencies</name>\n\n<properties>\n<tycho-version>3.0.0</tycho-version>\n<tycho-version>3.0.1</tycho-version>\n<reficio-p2-version>1.4.1</reficio-p2-version>\n<repo-name>DBeaver CE Update</repo-name>\n</properties>\n@@ -34,7 +34,7 @@\n<id>default-cli</id>\n<configuration>\n<artifacts>\n<artifact><id>com.github.jsqlparser:jsqlparser:4.6</id></artifact>\n<artifact><id>com.github.jsqlparser:jsqlparser:4.5</id></artifact>\n<!--<artifact><id>com.manticore-projects.jsqlformatter:jsqlformatter:0.1.7</id></artifact>-->\n\n<artifact><id>org.apache.commons:commons-jexl3:3.1</id></artifact>\n"}
{"Commit title": "LibPDF: Usedraw_rect()to show debug clipping rects", "Commit body": "@@ -305,12 +305,12 @@ RENDERER_HANDLER(path_append_rect)\n\nvoidRenderer::activate_clip()\n{\nautobounding_box =state().clipping_paths.current.bounding_box();\nautobounding_box =state().clipping_paths.current.bounding_box().to_type<int>();\nm_painter.clear_clip_rect();\nif(m_rendering_preferences.show_clipping_paths) {\nm_painter.stroke_path(rect_path(bounding_box), Color::Black,1);\nm_painter.draw_rect(bounding_box, Color::Black);\n}\nm_painter.add_clip_rect(bounding_box.to_type<int>());\nm_painter.add_clip_rect(bounding_box);\n}\n\nvoidRenderer::deactivate_clip()\n"}
{"Commit title": "#18798escape wild cards during primary and imported keys loading", "Commit body": "@@ -732,8 +732,9 @@ public JDBCStatement prepareUniqueConstraintsLoadStatement(@NotNull JDBCSession\nthrowsSQLException,DBException{\nreturnsession.getMetaData().getPrimaryKeys(\nowner.getCatalog() ==null?null:owner.getCatalog().getName(),\nowner.getSchema() ==null||DBUtils.isVirtualObject(owner.getSchema()) ?null:owner.getSchema().getName(),\nforParent==null?owner.getDataSource().getAllObjectsPattern() :forParent.getName())\nowner.getSchema() ==null||DBUtils.isVirtualObject(owner.getSchema()) ?\nnull:JDBCUtils.escapeWildCards(session,owner.getSchema().getName()),\nforParent==null?owner.getDataSource().getAllObjectsPattern() :JDBCUtils.escapeWildCards(session,forParent.getName()))\n.getSourceStatement();\n}\n\n@@ -749,10 +750,11 @@ public GenericTableForeignKey createTableForeignKeyImpl(GenericTableBase table,\npublicJDBCStatementprepareForeignKeysLoadStatement(@NotNullJDBCSessionsession,@NotNullGenericStructContainerowner,@NullableGenericTableBaseforParent)throwsSQLException{\nreturnsession.getMetaData().getImportedKeys(\nowner.getCatalog() ==null?null:owner.getCatalog().getName(),\nowner.getSchema() ==null||DBUtils.isVirtualObject(owner.getSchema()) ?null:owner.getSchema().getName(),\nowner.getSchema() ==null||DBUtils.isVirtualObject(owner.getSchema()) ?\nnull:JDBCUtils.escapeWildCards(session,owner.getSchema().getName()),\nforParent==null?\nowner.getDataSource().getAllObjectsPattern() :\nforParent.getName())\nJDBCUtils.escapeWildCards(session,forParent.getName()))\n.getSourceStatement();\n}\n\n"}
{"Commit title": "LibIPC: Use a simpler encoding for arithmetic values", "Commit body": "@@ -40,11 +40,6 @@ class Encoder {\nreturnm_buffer.data.try_ensure_capacity(m_buffer.data.size() + capacity);\n}\n\nvoidappend(u8 value)\n{\nm_buffer.data.unchecked_append(value);\n}\n\nErrorOr<void>append(u8const* values,size_tcount)\n{\nTRY(extend_capacity(count));\n@@ -67,31 +62,7 @@ class Encoder {\ntemplate<Arithmetic T>\nErrorOr<void>encode(Encoder& encoder, Tconst& value)\n{\nTRY(encoder.extend_capacity(sizeof(T)));\n\nifconstexpr(sizeof(T) ==1) {\nencoder.append(static_cast<u8>(value));\n}elseifconstexpr(sizeof(T) ==2) {\nencoder.append(static_cast<u8>(value));\nencoder.append(static_cast<u8>(value >>8));\n}elseifconstexpr(sizeof(T) ==4) {\nencoder.append(static_cast<u8>(value));\nencoder.append(static_cast<u8>(value >>8));\nencoder.append(static_cast<u8>(value >>16));\nencoder.append(static_cast<u8>(value >>24));\n}elseifconstexpr(sizeof(T) ==8) {\nencoder.append(static_cast<u8>(value));\nencoder.append(static_cast<u8>(value >>8));\nencoder.append(static_cast<u8>(value >>16));\nencoder.append(static_cast<u8>(value >>24));\nencoder.append(static_cast<u8>(value >>32));\nencoder.append(static_cast<u8>(value >>40));\nencoder.append(static_cast<u8>(value >>48));\nencoder.append(static_cast<u8>(value >>56));\n}else{\nstatic_assert(DependentFalse<T>);\n}\n\nTRY(encoder.append(reinterpret_cast<u8const*>(&value),sizeof(value)));\nreturn{};\n}\n\n"}
{"Commit title": "HackStudio: Remove adjustment of text range for documentation tooltip", "Commit body": "@@ -263,11 +263,8 @@ void Editor::mousemove_event(GUI::MouseEvent& event)\n}\n\nif(span.range.contains(text_position)) {\nautoadjusted_range = span.range;\nautoend_line_length =document().line(span.range.end().line()).length();\nadjusted_range.end().set_column(min(end_line_length, adjusted_range.end().column() +1));\nautohovered_span_text =document().text_in_range(adjusted_range);\ndbgln_if(EDITOR_DEBUG,\"Hovering: {}\\\"{}\\\"\", adjusted_range, hovered_span_text);\nautohovered_span_text =document().text_in_range(span.range);\ndbgln_if(EDITOR_DEBUG,\"Hovering: {}\\\"{}\\\"\", span.range, hovered_span_text);\n\nif(is_clickable) {\nis_over_clickable =true;\n"}
{"Commit title": "#17675read enum values specifically for new added types", "Commit body": "@@ -314,6 +314,27 @@ private void readEnumValues(@NotNull DBRProgressMonitor monitor) throws DBExcept\n.toArray();\n}\n\nprivatevoidreadNewEnumValues(DBRProgressMonitormonitor)throwsDBException{\ntry(JDBCSessionsession=DBUtils.openMetaSession(monitor,this,\"Refresh enum values\")) {\ntry(JDBCPreparedStatementdbStat=session.prepareStatement(\n\"SELECT e.enumlabel\\n\"+\n\"FROM pg_catalog.pg_enum e\\n\"+\n\"WHERE e.enumtypid=?\\n\"+\n\"ORDER BY e.enumsortorder\")) {\ndbStat.setLong(1,getObjectId());\ntry(JDBCResultSetrs=dbStat.executeQuery()) {\nList<String>values=newArrayList<>();\nwhile(rs.nextRow()) {\nvalues.add(JDBCUtils.safeGetString(rs,1));\n}\nenumValues=values.toArray();\n}\n}catch(SQLExceptione) {\nthrownewDBException(\"Error reading enum values\",e,getDataSource());\n}\n}\n}\n\npublicstaticString[]getOidTypes() {\nreturnOID_TYPES;\n}\n@@ -627,9 +648,13 @@ public DBSObject refreshObject(@NotNull DBRProgressMonitor monitor) throws DBExc\n\n@Property(viewable=true,order=16,visibleIf=EnumTypeValidator.class)\npublicObject[]getEnumValues(DBRProgressMonitormonitor) {\nif(typeCategory==PostgreTypeCategory.E&&enumValues==null) {\nif(typeCategory==PostgreTypeCategory.E&&ArrayUtils.isEmpty(enumValues)) {\ntry{\nreadEnumValues(monitor);\nif(ArrayUtils.isEmpty(enumValues)) {\n// Probably new objects not cached yet. Let's read them.\nreadNewEnumValues(monitor);\n}\n}catch(DBExceptione) {\nlog.error(\"Can't read enum values of type \"+getFullTypeName());\nenumValues=newObject[]{0};\n"}
{"Commit title": "LibUnicode: Make unicode data generation logic more relocatable", "Commit body": "@@ -30,31 +30,25 @@ if (ENABLE_UNICODE_DATABASE_DOWNLOAD)\nfile(DOWNLOAD${WORD_BREAK_URL}${WORD_BREAK_PATH}INACTIVITY_TIMEOUT 10)\nendif()\n\nset(UNICODE_GENERATOR CodeGenerators/GenerateUnicodeData)\nset(UNICODE_DATA_HEADER UnicodeData.h)\nset(UNICODE_DATA_IMPLEMENTATION UnicodeData.cpp)\nset(UNICODE_DATA_HEADER LibUnicode/UnicodeData.h)\nset(UNICODE_DATA_IMPLEMENTATION LibUnicode/UnicodeData.cpp)\n\nif(CMAKE_SOURCE_DIRMATCHES\".*/Lagom\")#Lagom-only build.\nset(UNICODE_GENERATOR LibUnicode/CodeGenerators/GenerateUnicodeData)\nset(UNICODE_DATA_HEADER LibUnicode/UnicodeData.h)\nset(UNICODE_DATA_IMPLEMENTATION LibUnicode/UnicodeData.cpp)\nelseif(CMAKE_CURRENT_BINARY_DIRMATCHES\".*/Lagom\")#Lagom build within the main SerenityOS build.\nset(UNICODE_GENERATOR ../../Userland/Libraries/LibUnicode/CodeGenerators/GenerateUnicodeData)\nset(UNICODE_DATA_HEADER LibUnicode/UnicodeData.h)\nset(UNICODE_DATA_IMPLEMENTATION LibUnicode/UnicodeData.cpp)\nif(CMAKE_CURRENT_BINARY_DIRMATCHES\".*/LibUnicode\")#Serenity build.\nset(UNICODE_DATA_HEADER UnicodeData.h)\nset(UNICODE_DATA_IMPLEMENTATION UnicodeData.cpp)\nendif()\n\nadd_custom_command(\nOUTPUT${UNICODE_DATA_HEADER}\nCOMMAND${write_if_different}${UNICODE_DATA_HEADER}${UNICODE_GENERATOR}-h -u${UNICODE_DATA_PATH}-s${SPECIAL_CASING_PATH}-p${PROP_LIST_PATH}-w${WORD_BREAK_PATH}\nCOMMAND${write_if_different}${UNICODE_DATA_HEADER}$<TARGET_FILE:GenerateUnicodeData>-h -u${UNICODE_DATA_PATH}-s${SPECIAL_CASING_PATH}-p${PROP_LIST_PATH}-w${WORD_BREAK_PATH}\nVERBATIM\nDEPENDSGenerateUnicodeData\nMAIN_DEPENDENCY${UNICODE_DATA_PATH}${SPECIAL_CASING_PATH}\n)\n\nadd_custom_command(\nOUTPUT${UNICODE_DATA_IMPLEMENTATION}\nCOMMAND${write_if_different}${UNICODE_DATA_IMPLEMENTATION}${UNICODE_GENERATOR}-c -u${UNICODE_DATA_PATH}-s${SPECIAL_CASING_PATH}-p${PROP_LIST_PATH}-w${WORD_BREAK_PATH}\nCOMMAND${write_if_different}${UNICODE_DATA_IMPLEMENTATION}$<TARGET_FILE:GenerateUnicodeData>-c -u${UNICODE_DATA_PATH}-s${SPECIAL_CASING_PATH}-p${PROP_LIST_PATH}-w${WORD_BREAK_PATH}\nVERBATIM\nDEPENDSGenerateUnicodeData\nMAIN_DEPENDENCY${UNICODE_DATA_PATH}${SPECIAL_CASING_PATH}\n"}
{"Commit title": "LibUnicode: Optimize the canonical composition algorithm implementation", "Commit body": "@@ -208,42 +208,52 @@ static void canonical_ordering_algorithm(Span<u32> code_points)\n//See Section 3.11, D115 of Version 15.0.0 of the Unicode Standard.\nstaticboolis_blocked(Span<u32> code_points,size_ta,size_tc)\n{\nif(!is_starter(code_points[a]) ||a == c -1)\nif(a == c -1)\nreturnfalse;\nautoconstc_combining_class =Unicode::canonical_combining_class(code_points[c]);\nautoconstb_combining_class =Unicode::canonical_combining_class(code_points[c -1]);\nreturnb_combining_class==0|| b_combining_class>= c_combining_class;\nreturnb_combining_class >= c_combining_class;\n}\n\n//The Canonical Composition Algorithm, as specified in Version 15.0.0 of the Unicode Standard.\n//See Section 3.11, D117; and UAX #15 https://unicode.org/reports/tr15\n//https://www.unicode.org/versions/Unicode15.0.0/ch03.pdf#G50628\nstaticvoidcanonical_composition_algorithm(Vector<u32>& code_points)\n{\nif(code_points.size() <=1)\nreturn;\nssize_tlast_starter =is_starter(code_points[0]) ?0: -1;\nfor(size_ti =1; i < code_points.size(); ++i) {\nautoconstcurrent_character = code_points[i];\n//R1. Seek back (left) to find the last Starter L preceding C in the character sequence\nfor(ssize_tj = i -1; j >=0; --j) {\nif(!is_starter(code_points[j]))\ncontinue;\n//R2. If there is such an L, and C is not blocked from L,\n//and there exists a Primary Composite P which is canonically equivalent to <L, C>,\n//then replace L by P in the sequence and delete C from the sequence.\nif(is_blocked(code_points.span(), j, i))\ncontinue;\nif(last_starter == -1) {\nif(is_starter(current_character))\nlast_starter = i;\ncontinue;\n}\n//R2. If there is such an L, and C is not blocked from L,\n//and there exists a Primary Composite P which is canonically equivalent to <L, C>,\n//then replace L by P in the sequence and delete C from the sequence.\nif(is_blocked(code_points.span(), last_starter, i)) {\nif(is_starter(current_character))\nlast_starter = i;\ncontinue;\n}\n\nautocomposite =combine_hangul_code_points(code_points[j], current_character);\nautocomposite =combine_hangul_code_points(code_points[last_starter], current_character);\n\nif(composite ==0)\ncomposite =combine_code_points(code_points[j], current_character);\nif(composite ==0)\ncomposite =combine_code_points(code_points[last_starter], current_character);\n\nif(composite !=0) {\ncode_points[j] = composite;\ncode_points.remove(i);\n--i;\nbreak;\n}\nif(composite ==0) {\nif(is_starter(current_character))\nlast_starter = i;\ncontinue;\n}\n\ncode_points[last_starter] = composite;\ncode_points.remove(i);\n--i;\n}\n}\n\n"}
{"Commit title": "LibUnicode: Optimize the canonical composition algorithm implementation", "Commit body": "@@ -208,42 +208,52 @@ static void canonical_ordering_algorithm(Span<u32> code_points)\n//See Section 3.11, D115 of Version 15.0.0 of the Unicode Standard.\nstaticboolis_blocked(Span<u32> code_points,size_ta,size_tc)\n{\nif(!is_starter(code_points[a]) ||a == c -1)\nif(a == c -1)\nreturnfalse;\nautoconstc_combining_class =Unicode::canonical_combining_class(code_points[c]);\nautoconstb_combining_class =Unicode::canonical_combining_class(code_points[c -1]);\nreturnb_combining_class==0|| b_combining_class>= c_combining_class;\nreturnb_combining_class >= c_combining_class;\n}\n\n//The Canonical Composition Algorithm, as specified in Version 15.0.0 of the Unicode Standard.\n//See Section 3.11, D117; and UAX #15 https://unicode.org/reports/tr15\n//https://www.unicode.org/versions/Unicode15.0.0/ch03.pdf#G50628\nstaticvoidcanonical_composition_algorithm(Vector<u32>& code_points)\n{\nif(code_points.size() <=1)\nreturn;\nssize_tlast_starter =is_starter(code_points[0]) ?0: -1;\nfor(size_ti =1; i < code_points.size(); ++i) {\nautoconstcurrent_character = code_points[i];\n//R1. Seek back (left) to find the last Starter L preceding C in the character sequence\nfor(ssize_tj = i -1; j >=0; --j) {\nif(!is_starter(code_points[j]))\ncontinue;\n//R2. If there is such an L, and C is not blocked from L,\n//and there exists a Primary Composite P which is canonically equivalent to <L, C>,\n//then replace L by P in the sequence and delete C from the sequence.\nif(is_blocked(code_points.span(), j, i))\ncontinue;\nif(last_starter == -1) {\nif(is_starter(current_character))\nlast_starter = i;\ncontinue;\n}\n//R2. If there is such an L, and C is not blocked from L,\n//and there exists a Primary Composite P which is canonically equivalent to <L, C>,\n//then replace L by P in the sequence and delete C from the sequence.\nif(is_blocked(code_points.span(), last_starter, i)) {\nif(is_starter(current_character))\nlast_starter = i;\ncontinue;\n}\n\nautocomposite =combine_hangul_code_points(code_points[j], current_character);\nautocomposite =combine_hangul_code_points(code_points[last_starter], current_character);\n\nif(composite ==0)\ncomposite =combine_code_points(code_points[j], current_character);\nif(composite ==0)\ncomposite =combine_code_points(code_points[last_starter], current_character);\n\nif(composite !=0) {\ncode_points[j] = composite;\ncode_points.remove(i);\n--i;\nbreak;\n}\nif(composite ==0) {\nif(is_starter(current_character))\nlast_starter = i;\ncontinue;\n}\n\ncode_points[last_starter] = composite;\ncode_points.remove(i);\n--i;\n}\n}\n\n"}
{"Commit title": "Bump org.mockito:mockito-core from 4.11.0 to 5.12.0", "Commit body": "@@ -16,7 +16,7 @@ ext {\nreactiveStreamsVersion=\"1.0.4\"\njunitVersion=\"4.13.2\"\ntestNgVersion=\"7.5\"\nmockitoVersion=\"4.11.0\"\nmockitoVersion=\"5.12.0\"\njmhLibVersion=\"1.21\"\nguavaVersion=\"33.2.0-jre\"\n}\n"}
{"Commit title": "#22244fix typo", "Commit body": "@@ -70,7 +70,7 @@ tree.sequences.node.tip = MariaDB Sequences\n\nmanager.catalog.name=Catalog manager\n\nparameters.all.caches= Cachemeta data\nparameters.all.caches= Cachemetadata\nparameters.all.caches.tip= Read tables constraints and columns at the stage of reading tables.\\nThis setting may reduce metadata loading performance for small databases and increase for large databases.\n\nmeta.org.jkiss.dbeaver.ext.mysql.model.MySQLCatalog.name.name=Schema Name\n"}
{"Commit title": "AK: Use correct wide integer type for qhat check in UFixedBigIntDivision", "Commit body": "@@ -95,7 +95,7 @@ constexpr void div_mod_internal(\nqhat =div_mod_words(dividend[i -1], dividend[i], divisor_approx, rhat);\n\nautois_qhat_too_large = [&] {\nreturnUFixedBigInt<native_word_size> { qhat }.wide_multiply(divisor[divisor_len -2]) >u128{ dividend[i -2], rhat };\nreturnUFixedBigInt<native_word_size> { qhat }.wide_multiply(divisor[divisor_len -2]) >UFixedBigInt<native_word_size *2>{ dividend[i -2], rhat };\n};\nif(is_qhat_too_large()) {\n--qhat;\n"}
{"Commit title": "#20060Revert JSQLParser to 4.5", "Commit body": "@@ -9,7 +9,7 @@\n<name>DBeaver - 3rd party dependencies</name>\n\n<properties>\n<tycho-version>3.0.0</tycho-version>\n<tycho-version>3.0.1</tycho-version>\n<reficio-p2-version>1.4.1</reficio-p2-version>\n<repo-name>DBeaver CE Update</repo-name>\n</properties>\n@@ -34,7 +34,7 @@\n<id>default-cli</id>\n<configuration>\n<artifacts>\n<artifact><id>com.github.jsqlparser:jsqlparser:4.6</id></artifact>\n<artifact><id>com.github.jsqlparser:jsqlparser:4.5</id></artifact>\n<!--<artifact><id>com.manticore-projects.jsqlformatter:jsqlformatter:0.1.7</id></artifact>-->\n\n<artifact><id>org.apache.commons:commons-jexl3:3.1</id></artifact>\n"}
{"Commit title": "LibPDF: Usedraw_rect()to show debug clipping rects", "Commit body": "@@ -305,12 +305,12 @@ RENDERER_HANDLER(path_append_rect)\n\nvoidRenderer::activate_clip()\n{\nautobounding_box =state().clipping_paths.current.bounding_box();\nautobounding_box =state().clipping_paths.current.bounding_box().to_type<int>();\nm_painter.clear_clip_rect();\nif(m_rendering_preferences.show_clipping_paths) {\nm_painter.stroke_path(rect_path(bounding_box), Color::Black,1);\nm_painter.draw_rect(bounding_box, Color::Black);\n}\nm_painter.add_clip_rect(bounding_box.to_type<int>());\nm_painter.add_clip_rect(bounding_box);\n}\n\nvoidRenderer::deactivate_clip()\n"}
{"Commit title": "#18798escape wild cards during primary and imported keys loading", "Commit body": "@@ -732,8 +732,9 @@ public JDBCStatement prepareUniqueConstraintsLoadStatement(@NotNull JDBCSession\nthrowsSQLException,DBException{\nreturnsession.getMetaData().getPrimaryKeys(\nowner.getCatalog() ==null?null:owner.getCatalog().getName(),\nowner.getSchema() ==null||DBUtils.isVirtualObject(owner.getSchema()) ?null:owner.getSchema().getName(),\nforParent==null?owner.getDataSource().getAllObjectsPattern() :forParent.getName())\nowner.getSchema() ==null||DBUtils.isVirtualObject(owner.getSchema()) ?\nnull:JDBCUtils.escapeWildCards(session,owner.getSchema().getName()),\nforParent==null?owner.getDataSource().getAllObjectsPattern() :JDBCUtils.escapeWildCards(session,forParent.getName()))\n.getSourceStatement();\n}\n\n@@ -749,10 +750,11 @@ public GenericTableForeignKey createTableForeignKeyImpl(GenericTableBase table,\npublicJDBCStatementprepareForeignKeysLoadStatement(@NotNullJDBCSessionsession,@NotNullGenericStructContainerowner,@NullableGenericTableBaseforParent)throwsSQLException{\nreturnsession.getMetaData().getImportedKeys(\nowner.getCatalog() ==null?null:owner.getCatalog().getName(),\nowner.getSchema() ==null||DBUtils.isVirtualObject(owner.getSchema()) ?null:owner.getSchema().getName(),\nowner.getSchema() ==null||DBUtils.isVirtualObject(owner.getSchema()) ?\nnull:JDBCUtils.escapeWildCards(session,owner.getSchema().getName()),\nforParent==null?\nowner.getDataSource().getAllObjectsPattern() :\nforParent.getName())\nJDBCUtils.escapeWildCards(session,forParent.getName()))\n.getSourceStatement();\n}\n\n"}
{"Commit title": "LibIPC: Use a simpler encoding for arithmetic values", "Commit body": "@@ -40,11 +40,6 @@ class Encoder {\nreturnm_buffer.data.try_ensure_capacity(m_buffer.data.size() + capacity);\n}\n\nvoidappend(u8 value)\n{\nm_buffer.data.unchecked_append(value);\n}\n\nErrorOr<void>append(u8const* values,size_tcount)\n{\nTRY(extend_capacity(count));\n@@ -67,31 +62,7 @@ class Encoder {\ntemplate<Arithmetic T>\nErrorOr<void>encode(Encoder& encoder, Tconst& value)\n{\nTRY(encoder.extend_capacity(sizeof(T)));\n\nifconstexpr(sizeof(T) ==1) {\nencoder.append(static_cast<u8>(value));\n}elseifconstexpr(sizeof(T) ==2) {\nencoder.append(static_cast<u8>(value));\nencoder.append(static_cast<u8>(value >>8));\n}elseifconstexpr(sizeof(T) ==4) {\nencoder.append(static_cast<u8>(value));\nencoder.append(static_cast<u8>(value >>8));\nencoder.append(static_cast<u8>(value >>16));\nencoder.append(static_cast<u8>(value >>24));\n}elseifconstexpr(sizeof(T) ==8) {\nencoder.append(static_cast<u8>(value));\nencoder.append(static_cast<u8>(value >>8));\nencoder.append(static_cast<u8>(value >>16));\nencoder.append(static_cast<u8>(value >>24));\nencoder.append(static_cast<u8>(value >>32));\nencoder.append(static_cast<u8>(value >>40));\nencoder.append(static_cast<u8>(value >>48));\nencoder.append(static_cast<u8>(value >>56));\n}else{\nstatic_assert(DependentFalse<T>);\n}\n\nTRY(encoder.append(reinterpret_cast<u8const*>(&value),sizeof(value)));\nreturn{};\n}\n\n"}
{"Commit title": "HackStudio: Remove adjustment of text range for documentation tooltip", "Commit body": "@@ -263,11 +263,8 @@ void Editor::mousemove_event(GUI::MouseEvent& event)\n}\n\nif(span.range.contains(text_position)) {\nautoadjusted_range = span.range;\nautoend_line_length =document().line(span.range.end().line()).length();\nadjusted_range.end().set_column(min(end_line_length, adjusted_range.end().column() +1));\nautohovered_span_text =document().text_in_range(adjusted_range);\ndbgln_if(EDITOR_DEBUG,\"Hovering: {}\\\"{}\\\"\", adjusted_range, hovered_span_text);\nautohovered_span_text =document().text_in_range(span.range);\ndbgln_if(EDITOR_DEBUG,\"Hovering: {}\\\"{}\\\"\", span.range, hovered_span_text);\n\nif(is_clickable) {\nis_over_clickable =true;\n"}
{"Commit title": "#17675read enum values specifically for new added types", "Commit body": "@@ -314,6 +314,27 @@ private void readEnumValues(@NotNull DBRProgressMonitor monitor) throws DBExcept\n.toArray();\n}\n\nprivatevoidreadNewEnumValues(DBRProgressMonitormonitor)throwsDBException{\ntry(JDBCSessionsession=DBUtils.openMetaSession(monitor,this,\"Refresh enum values\")) {\ntry(JDBCPreparedStatementdbStat=session.prepareStatement(\n\"SELECT e.enumlabel\\n\"+\n\"FROM pg_catalog.pg_enum e\\n\"+\n\"WHERE e.enumtypid=?\\n\"+\n\"ORDER BY e.enumsortorder\")) {\ndbStat.setLong(1,getObjectId());\ntry(JDBCResultSetrs=dbStat.executeQuery()) {\nList<String>values=newArrayList<>();\nwhile(rs.nextRow()) {\nvalues.add(JDBCUtils.safeGetString(rs,1));\n}\nenumValues=values.toArray();\n}\n}catch(SQLExceptione) {\nthrownewDBException(\"Error reading enum values\",e,getDataSource());\n}\n}\n}\n\npublicstaticString[]getOidTypes() {\nreturnOID_TYPES;\n}\n@@ -627,9 +648,13 @@ public DBSObject refreshObject(@NotNull DBRProgressMonitor monitor) throws DBExc\n\n@Property(viewable=true,order=16,visibleIf=EnumTypeValidator.class)\npublicObject[]getEnumValues(DBRProgressMonitormonitor) {\nif(typeCategory==PostgreTypeCategory.E&&enumValues==null) {\nif(typeCategory==PostgreTypeCategory.E&&ArrayUtils.isEmpty(enumValues)) {\ntry{\nreadEnumValues(monitor);\nif(ArrayUtils.isEmpty(enumValues)) {\n// Probably new objects not cached yet. Let's read them.\nreadNewEnumValues(monitor);\n}\n}catch(DBExceptione) {\nlog.error(\"Can't read enum values of type \"+getFullTypeName());\nenumValues=newObject[]{0};\n"}
{"Commit title": "LibUnicode: Make unicode data generation logic more relocatable", "Commit body": "@@ -30,31 +30,25 @@ if (ENABLE_UNICODE_DATABASE_DOWNLOAD)\nfile(DOWNLOAD${WORD_BREAK_URL}${WORD_BREAK_PATH}INACTIVITY_TIMEOUT 10)\nendif()\n\nset(UNICODE_GENERATOR CodeGenerators/GenerateUnicodeData)\nset(UNICODE_DATA_HEADER UnicodeData.h)\nset(UNICODE_DATA_IMPLEMENTATION UnicodeData.cpp)\nset(UNICODE_DATA_HEADER LibUnicode/UnicodeData.h)\nset(UNICODE_DATA_IMPLEMENTATION LibUnicode/UnicodeData.cpp)\n\nif(CMAKE_SOURCE_DIRMATCHES\".*/Lagom\")#Lagom-only build.\nset(UNICODE_GENERATOR LibUnicode/CodeGenerators/GenerateUnicodeData)\nset(UNICODE_DATA_HEADER LibUnicode/UnicodeData.h)\nset(UNICODE_DATA_IMPLEMENTATION LibUnicode/UnicodeData.cpp)\nelseif(CMAKE_CURRENT_BINARY_DIRMATCHES\".*/Lagom\")#Lagom build within the main SerenityOS build.\nset(UNICODE_GENERATOR ../../Userland/Libraries/LibUnicode/CodeGenerators/GenerateUnicodeData)\nset(UNICODE_DATA_HEADER LibUnicode/UnicodeData.h)\nset(UNICODE_DATA_IMPLEMENTATION LibUnicode/UnicodeData.cpp)\nif(CMAKE_CURRENT_BINARY_DIRMATCHES\".*/LibUnicode\")#Serenity build.\nset(UNICODE_DATA_HEADER UnicodeData.h)\nset(UNICODE_DATA_IMPLEMENTATION UnicodeData.cpp)\nendif()\n\nadd_custom_command(\nOUTPUT${UNICODE_DATA_HEADER}\nCOMMAND${write_if_different}${UNICODE_DATA_HEADER}${UNICODE_GENERATOR}-h -u${UNICODE_DATA_PATH}-s${SPECIAL_CASING_PATH}-p${PROP_LIST_PATH}-w${WORD_BREAK_PATH}\nCOMMAND${write_if_different}${UNICODE_DATA_HEADER}$<TARGET_FILE:GenerateUnicodeData>-h -u${UNICODE_DATA_PATH}-s${SPECIAL_CASING_PATH}-p${PROP_LIST_PATH}-w${WORD_BREAK_PATH}\nVERBATIM\nDEPENDSGenerateUnicodeData\nMAIN_DEPENDENCY${UNICODE_DATA_PATH}${SPECIAL_CASING_PATH}\n)\n\nadd_custom_command(\nOUTPUT${UNICODE_DATA_IMPLEMENTATION}\nCOMMAND${write_if_different}${UNICODE_DATA_IMPLEMENTATION}${UNICODE_GENERATOR}-c -u${UNICODE_DATA_PATH}-s${SPECIAL_CASING_PATH}-p${PROP_LIST_PATH}-w${WORD_BREAK_PATH}\nCOMMAND${write_if_different}${UNICODE_DATA_IMPLEMENTATION}$<TARGET_FILE:GenerateUnicodeData>-c -u${UNICODE_DATA_PATH}-s${SPECIAL_CASING_PATH}-p${PROP_LIST_PATH}-w${WORD_BREAK_PATH}\nVERBATIM\nDEPENDSGenerateUnicodeData\nMAIN_DEPENDENCY${UNICODE_DATA_PATH}${SPECIAL_CASING_PATH}\n"}
{"Commit title": "LibGfx: Remove unused headers from BitmapFont.{cpp,h}", "Commit body": "@@ -5,20 +5,11 @@\n*/\n\n#include\"BitmapFont.h\"\n#include\"Bitmap.h\"\n#include\"Emoji.h\"\n#include<AK/StdLibExtras.h>\n#include<AK/StringBuilder.h>\n#include<AK/Utf32View.h>\n#include<AK/Utf8View.h>\n#include<AK/Vector.h>\n#include<LibCore/FileStream.h>\n#include<LibGfx/FontDatabase.h>\n#include<stdio.h>\n#include<stdlib.h>\n#include<string.h>\n#include<sys/mman.h>\n#include<unistd.h>\n\nnamespaceGfx{\n\n"}
{"Commit title": "LibGUI: Account for the row and column headers when painting a TableView", "Commit body": "@@ -73,8 +73,8 @@ void TableView::paint_event(PaintEvent& event)\ninty_offset =column_header().is_visible() ?column_header().height() :0;\n\nbooldummy;\nintfirst_visible_row =index_at_event_position(frame_inner_rect().top_left(), dummy).row();\nintlast_visible_row =index_at_event_position(frame_inner_rect().bottom_right(), dummy).row();\nintfirst_visible_row =index_at_event_position(frame_inner_rect().top_left().translated(x_offset, y_offset), dummy).row();\nintlast_visible_row =index_at_event_position(frame_inner_rect().bottom_right().translated(x_offset, y_offset), dummy).row();\n\nif(first_visible_row == -1)\nfirst_visible_row =0;\n"}
{"Commit title": "Everywhere: Force linker hash style to be gnu", "Commit body": "@@ -116,7 +116,8 @@ endforeach()\n\nset(CMAKE_INSTALL_NAME_TOOL\"\")\nset(CMAKE_SHARED_LIBRARY_SUFFIX\".so\")\nset(CMAKE_SHARED_LIBRARY_CREATE_CXX_FLAGS\"-shared\")\nset(CMAKE_SHARED_LIBRARY_CREATE_CXX_FLAGS\"-shared -Wl,--hash-style=gnu\")\nset(CMAKE_CXX_LINK_FLAGS\"-Wl,--hash-style=gnu\")\n\n#Note: MacOS has different rpath rules from linux.\n#We disable it completely for MacOS hosts to avoid having to track down all the individual flags to unset\n"}
{"Commit title": "xxhash.h as of 2c611a76f914828bed675f0f342d6c4199ffee1e:xxhash.h", "Commit body": ""}
{"Commit title": "Add --slow_usecs option to determine when the \"long op\" message is pr…", "Commit body": "@@ -1235,6 +1235,10 @@ DEFINE_int64(stats_interval_seconds, 0, \"Report stats every N seconds. This \"\nDEFINE_int32(stats_per_interval,0,\"Reports additional stats per interval when\"\n\"this is greater than 0.\");\n\nDEFINE_uint64(slow_usecs,1000000,\n\"A message is printed for operations that\"\n\"take at least this many microseconds.\");\n\nDEFINE_int64(report_interval_seconds,0,\n\"If greater than zero, it will write simple stats in CSV format\"\n\"to --report_file every N seconds\");\n@@ -2140,7 +2144,7 @@ class Stats {\n}\nhist_[op_type]->Add(micros);\n\nif(micros >20000&& !FLAGS_stats_interval) {\nif(micros >= FLAGS_slow_usecs&& !FLAGS_stats_interval) {\nfprintf(stderr,\"long op: %\"PRIu64\"micros%30s\\r\", micros,\"\");\nfflush(stderr);\n}\n"}
{"Commit title": "Reword", "Commit body": "@@ -1369,8 +1369,7 @@ struct Options : public DBOptions, public ColumnFamilyOptions {\n//NOT MAINTAINED: This function has not been and is not maintained.\n//DEPRECATED: This function might be removed in a future release.\n//In general, defaults are changed to suit broad interests. Opting\n//out of a change on upgrade should be deliberate and considered\n//rather than automatic and thoughtless.\n//out of a change on upgrade should be deliberate and considered.\nOptions*OldDefaults(introcksdb_major_version =4,\nintrocksdb_minor_version =6);\n\n"}
{"Commit title": "Add assertion", "Commit body": "@@ -338,6 +338,7 @@ class VersionStorageInfo {\n\nconstuint64_tfile_number = meta->fd.GetNumber();\n\nassert(file_locations_.find(file_number) == file_locations_.end());\nfile_locations_.emplace(file_number,FileLocation(level, pos));\n}\n}\n"}
{"Commit title": "LibUnicode: Optimize the canonical composition algorithm implementation", "Commit body": "@@ -208,42 +208,52 @@ static void canonical_ordering_algorithm(Span<u32> code_points)\n//See Section 3.11, D115 of Version 15.0.0 of the Unicode Standard.\nstaticboolis_blocked(Span<u32> code_points,size_ta,size_tc)\n{\nif(!is_starter(code_points[a]) ||a == c -1)\nif(a == c -1)\nreturnfalse;\nautoconstc_combining_class =Unicode::canonical_combining_class(code_points[c]);\nautoconstb_combining_class =Unicode::canonical_combining_class(code_points[c -1]);\nreturnb_combining_class==0|| b_combining_class>= c_combining_class;\nreturnb_combining_class >= c_combining_class;\n}\n\n//The Canonical Composition Algorithm, as specified in Version 15.0.0 of the Unicode Standard.\n//See Section 3.11, D117; and UAX #15 https://unicode.org/reports/tr15\n//https://www.unicode.org/versions/Unicode15.0.0/ch03.pdf#G50628\nstaticvoidcanonical_composition_algorithm(Vector<u32>& code_points)\n{\nif(code_points.size() <=1)\nreturn;\nssize_tlast_starter =is_starter(code_points[0]) ?0: -1;\nfor(size_ti =1; i < code_points.size(); ++i) {\nautoconstcurrent_character = code_points[i];\n//R1. Seek back (left) to find the last Starter L preceding C in the character sequence\nfor(ssize_tj = i -1; j >=0; --j) {\nif(!is_starter(code_points[j]))\ncontinue;\n//R2. If there is such an L, and C is not blocked from L,\n//and there exists a Primary Composite P which is canonically equivalent to <L, C>,\n//then replace L by P in the sequence and delete C from the sequence.\nif(is_blocked(code_points.span(), j, i))\ncontinue;\nif(last_starter == -1) {\nif(is_starter(current_character))\nlast_starter = i;\ncontinue;\n}\n//R2. If there is such an L, and C is not blocked from L,\n//and there exists a Primary Composite P which is canonically equivalent to <L, C>,\n//then replace L by P in the sequence and delete C from the sequence.\nif(is_blocked(code_points.span(), last_starter, i)) {\nif(is_starter(current_character))\nlast_starter = i;\ncontinue;\n}\n\nautocomposite =combine_hangul_code_points(code_points[j], current_character);\nautocomposite =combine_hangul_code_points(code_points[last_starter], current_character);\n\nif(composite ==0)\ncomposite =combine_code_points(code_points[j], current_character);\nif(composite ==0)\ncomposite =combine_code_points(code_points[last_starter], current_character);\n\nif(composite !=0) {\ncode_points[j] = composite;\ncode_points.remove(i);\n--i;\nbreak;\n}\nif(composite ==0) {\nif(is_starter(current_character))\nlast_starter = i;\ncontinue;\n}\n\ncode_points[last_starter] = composite;\ncode_points.remove(i);\n--i;\n}\n}\n\n"}
{"Commit title": "Bump org.mockito:mockito-core from 4.11.0 to 5.12.0", "Commit body": "@@ -16,7 +16,7 @@ ext {\nreactiveStreamsVersion=\"1.0.4\"\njunitVersion=\"4.13.2\"\ntestNgVersion=\"7.5\"\nmockitoVersion=\"4.11.0\"\nmockitoVersion=\"5.12.0\"\njmhLibVersion=\"1.21\"\nguavaVersion=\"33.2.0-jre\"\n}\n"}
{"Commit title": "#22244fix typo", "Commit body": "@@ -70,7 +70,7 @@ tree.sequences.node.tip = MariaDB Sequences\n\nmanager.catalog.name=Catalog manager\n\nparameters.all.caches= Cachemeta data\nparameters.all.caches= Cachemetadata\nparameters.all.caches.tip= Read tables constraints and columns at the stage of reading tables.\\nThis setting may reduce metadata loading performance for small databases and increase for large databases.\n\nmeta.org.jkiss.dbeaver.ext.mysql.model.MySQLCatalog.name.name=Schema Name\n"}
{"Commit title": "AK: Use correct wide integer type for qhat check in UFixedBigIntDivision", "Commit body": "@@ -95,7 +95,7 @@ constexpr void div_mod_internal(\nqhat =div_mod_words(dividend[i -1], dividend[i], divisor_approx, rhat);\n\nautois_qhat_too_large = [&] {\nreturnUFixedBigInt<native_word_size> { qhat }.wide_multiply(divisor[divisor_len -2]) >u128{ dividend[i -2], rhat };\nreturnUFixedBigInt<native_word_size> { qhat }.wide_multiply(divisor[divisor_len -2]) >UFixedBigInt<native_word_size *2>{ dividend[i -2], rhat };\n};\nif(is_qhat_too_large()) {\n--qhat;\n"}
{"Commit title": "#20060Revert JSQLParser to 4.5", "Commit body": "@@ -9,7 +9,7 @@\n<name>DBeaver - 3rd party dependencies</name>\n\n<properties>\n<tycho-version>3.0.0</tycho-version>\n<tycho-version>3.0.1</tycho-version>\n<reficio-p2-version>1.4.1</reficio-p2-version>\n<repo-name>DBeaver CE Update</repo-name>\n</properties>\n@@ -34,7 +34,7 @@\n<id>default-cli</id>\n<configuration>\n<artifacts>\n<artifact><id>com.github.jsqlparser:jsqlparser:4.6</id></artifact>\n<artifact><id>com.github.jsqlparser:jsqlparser:4.5</id></artifact>\n<!--<artifact><id>com.manticore-projects.jsqlformatter:jsqlformatter:0.1.7</id></artifact>-->\n\n<artifact><id>org.apache.commons:commons-jexl3:3.1</id></artifact>\n"}
{"Commit title": "LibPDF: Usedraw_rect()to show debug clipping rects", "Commit body": "@@ -305,12 +305,12 @@ RENDERER_HANDLER(path_append_rect)\n\nvoidRenderer::activate_clip()\n{\nautobounding_box =state().clipping_paths.current.bounding_box();\nautobounding_box =state().clipping_paths.current.bounding_box().to_type<int>();\nm_painter.clear_clip_rect();\nif(m_rendering_preferences.show_clipping_paths) {\nm_painter.stroke_path(rect_path(bounding_box), Color::Black,1);\nm_painter.draw_rect(bounding_box, Color::Black);\n}\nm_painter.add_clip_rect(bounding_box.to_type<int>());\nm_painter.add_clip_rect(bounding_box);\n}\n\nvoidRenderer::deactivate_clip()\n"}
{"Commit title": "#18798escape wild cards during primary and imported keys loading", "Commit body": "@@ -732,8 +732,9 @@ public JDBCStatement prepareUniqueConstraintsLoadStatement(@NotNull JDBCSession\nthrowsSQLException,DBException{\nreturnsession.getMetaData().getPrimaryKeys(\nowner.getCatalog() ==null?null:owner.getCatalog().getName(),\nowner.getSchema() ==null||DBUtils.isVirtualObject(owner.getSchema()) ?null:owner.getSchema().getName(),\nforParent==null?owner.getDataSource().getAllObjectsPattern() :forParent.getName())\nowner.getSchema() ==null||DBUtils.isVirtualObject(owner.getSchema()) ?\nnull:JDBCUtils.escapeWildCards(session,owner.getSchema().getName()),\nforParent==null?owner.getDataSource().getAllObjectsPattern() :JDBCUtils.escapeWildCards(session,forParent.getName()))\n.getSourceStatement();\n}\n\n@@ -749,10 +750,11 @@ public GenericTableForeignKey createTableForeignKeyImpl(GenericTableBase table,\npublicJDBCStatementprepareForeignKeysLoadStatement(@NotNullJDBCSessionsession,@NotNullGenericStructContainerowner,@NullableGenericTableBaseforParent)throwsSQLException{\nreturnsession.getMetaData().getImportedKeys(\nowner.getCatalog() ==null?null:owner.getCatalog().getName(),\nowner.getSchema() ==null||DBUtils.isVirtualObject(owner.getSchema()) ?null:owner.getSchema().getName(),\nowner.getSchema() ==null||DBUtils.isVirtualObject(owner.getSchema()) ?\nnull:JDBCUtils.escapeWildCards(session,owner.getSchema().getName()),\nforParent==null?\nowner.getDataSource().getAllObjectsPattern() :\nforParent.getName())\nJDBCUtils.escapeWildCards(session,forParent.getName()))\n.getSourceStatement();\n}\n\n"}
{"Commit title": "LibIPC: Use a simpler encoding for arithmetic values", "Commit body": "@@ -40,11 +40,6 @@ class Encoder {\nreturnm_buffer.data.try_ensure_capacity(m_buffer.data.size() + capacity);\n}\n\nvoidappend(u8 value)\n{\nm_buffer.data.unchecked_append(value);\n}\n\nErrorOr<void>append(u8const* values,size_tcount)\n{\nTRY(extend_capacity(count));\n@@ -67,31 +62,7 @@ class Encoder {\ntemplate<Arithmetic T>\nErrorOr<void>encode(Encoder& encoder, Tconst& value)\n{\nTRY(encoder.extend_capacity(sizeof(T)));\n\nifconstexpr(sizeof(T) ==1) {\nencoder.append(static_cast<u8>(value));\n}elseifconstexpr(sizeof(T) ==2) {\nencoder.append(static_cast<u8>(value));\nencoder.append(static_cast<u8>(value >>8));\n}elseifconstexpr(sizeof(T) ==4) {\nencoder.append(static_cast<u8>(value));\nencoder.append(static_cast<u8>(value >>8));\nencoder.append(static_cast<u8>(value >>16));\nencoder.append(static_cast<u8>(value >>24));\n}elseifconstexpr(sizeof(T) ==8) {\nencoder.append(static_cast<u8>(value));\nencoder.append(static_cast<u8>(value >>8));\nencoder.append(static_cast<u8>(value >>16));\nencoder.append(static_cast<u8>(value >>24));\nencoder.append(static_cast<u8>(value >>32));\nencoder.append(static_cast<u8>(value >>40));\nencoder.append(static_cast<u8>(value >>48));\nencoder.append(static_cast<u8>(value >>56));\n}else{\nstatic_assert(DependentFalse<T>);\n}\n\nTRY(encoder.append(reinterpret_cast<u8const*>(&value),sizeof(value)));\nreturn{};\n}\n\n"}
{"Commit title": "HackStudio: Remove adjustment of text range for documentation tooltip", "Commit body": "@@ -263,11 +263,8 @@ void Editor::mousemove_event(GUI::MouseEvent& event)\n}\n\nif(span.range.contains(text_position)) {\nautoadjusted_range = span.range;\nautoend_line_length =document().line(span.range.end().line()).length();\nadjusted_range.end().set_column(min(end_line_length, adjusted_range.end().column() +1));\nautohovered_span_text =document().text_in_range(adjusted_range);\ndbgln_if(EDITOR_DEBUG,\"Hovering: {}\\\"{}\\\"\", adjusted_range, hovered_span_text);\nautohovered_span_text =document().text_in_range(span.range);\ndbgln_if(EDITOR_DEBUG,\"Hovering: {}\\\"{}\\\"\", span.range, hovered_span_text);\n\nif(is_clickable) {\nis_over_clickable =true;\n"}
{"Commit title": "#17675read enum values specifically for new added types", "Commit body": "@@ -314,6 +314,27 @@ private void readEnumValues(@NotNull DBRProgressMonitor monitor) throws DBExcept\n.toArray();\n}\n\nprivatevoidreadNewEnumValues(DBRProgressMonitormonitor)throwsDBException{\ntry(JDBCSessionsession=DBUtils.openMetaSession(monitor,this,\"Refresh enum values\")) {\ntry(JDBCPreparedStatementdbStat=session.prepareStatement(\n\"SELECT e.enumlabel\\n\"+\n\"FROM pg_catalog.pg_enum e\\n\"+\n\"WHERE e.enumtypid=?\\n\"+\n\"ORDER BY e.enumsortorder\")) {\ndbStat.setLong(1,getObjectId());\ntry(JDBCResultSetrs=dbStat.executeQuery()) {\nList<String>values=newArrayList<>();\nwhile(rs.nextRow()) {\nvalues.add(JDBCUtils.safeGetString(rs,1));\n}\nenumValues=values.toArray();\n}\n}catch(SQLExceptione) {\nthrownewDBException(\"Error reading enum values\",e,getDataSource());\n}\n}\n}\n\npublicstaticString[]getOidTypes() {\nreturnOID_TYPES;\n}\n@@ -627,9 +648,13 @@ public DBSObject refreshObject(@NotNull DBRProgressMonitor monitor) throws DBExc\n\n@Property(viewable=true,order=16,visibleIf=EnumTypeValidator.class)\npublicObject[]getEnumValues(DBRProgressMonitormonitor) {\nif(typeCategory==PostgreTypeCategory.E&&enumValues==null) {\nif(typeCategory==PostgreTypeCategory.E&&ArrayUtils.isEmpty(enumValues)) {\ntry{\nreadEnumValues(monitor);\nif(ArrayUtils.isEmpty(enumValues)) {\n// Probably new objects not cached yet. Let's read them.\nreadNewEnumValues(monitor);\n}\n}catch(DBExceptione) {\nlog.error(\"Can't read enum values of type \"+getFullTypeName());\nenumValues=newObject[]{0};\n"}
{"Commit title": "LibUnicode: Make unicode data generation logic more relocatable", "Commit body": "@@ -30,31 +30,25 @@ if (ENABLE_UNICODE_DATABASE_DOWNLOAD)\nfile(DOWNLOAD${WORD_BREAK_URL}${WORD_BREAK_PATH}INACTIVITY_TIMEOUT 10)\nendif()\n\nset(UNICODE_GENERATOR CodeGenerators/GenerateUnicodeData)\nset(UNICODE_DATA_HEADER UnicodeData.h)\nset(UNICODE_DATA_IMPLEMENTATION UnicodeData.cpp)\nset(UNICODE_DATA_HEADER LibUnicode/UnicodeData.h)\nset(UNICODE_DATA_IMPLEMENTATION LibUnicode/UnicodeData.cpp)\n\nif(CMAKE_SOURCE_DIRMATCHES\".*/Lagom\")#Lagom-only build.\nset(UNICODE_GENERATOR LibUnicode/CodeGenerators/GenerateUnicodeData)\nset(UNICODE_DATA_HEADER LibUnicode/UnicodeData.h)\nset(UNICODE_DATA_IMPLEMENTATION LibUnicode/UnicodeData.cpp)\nelseif(CMAKE_CURRENT_BINARY_DIRMATCHES\".*/Lagom\")#Lagom build within the main SerenityOS build.\nset(UNICODE_GENERATOR ../../Userland/Libraries/LibUnicode/CodeGenerators/GenerateUnicodeData)\nset(UNICODE_DATA_HEADER LibUnicode/UnicodeData.h)\nset(UNICODE_DATA_IMPLEMENTATION LibUnicode/UnicodeData.cpp)\nif(CMAKE_CURRENT_BINARY_DIRMATCHES\".*/LibUnicode\")#Serenity build.\nset(UNICODE_DATA_HEADER UnicodeData.h)\nset(UNICODE_DATA_IMPLEMENTATION UnicodeData.cpp)\nendif()\n\nadd_custom_command(\nOUTPUT${UNICODE_DATA_HEADER}\nCOMMAND${write_if_different}${UNICODE_DATA_HEADER}${UNICODE_GENERATOR}-h -u${UNICODE_DATA_PATH}-s${SPECIAL_CASING_PATH}-p${PROP_LIST_PATH}-w${WORD_BREAK_PATH}\nCOMMAND${write_if_different}${UNICODE_DATA_HEADER}$<TARGET_FILE:GenerateUnicodeData>-h -u${UNICODE_DATA_PATH}-s${SPECIAL_CASING_PATH}-p${PROP_LIST_PATH}-w${WORD_BREAK_PATH}\nVERBATIM\nDEPENDSGenerateUnicodeData\nMAIN_DEPENDENCY${UNICODE_DATA_PATH}${SPECIAL_CASING_PATH}\n)\n\nadd_custom_command(\nOUTPUT${UNICODE_DATA_IMPLEMENTATION}\nCOMMAND${write_if_different}${UNICODE_DATA_IMPLEMENTATION}${UNICODE_GENERATOR}-c -u${UNICODE_DATA_PATH}-s${SPECIAL_CASING_PATH}-p${PROP_LIST_PATH}-w${WORD_BREAK_PATH}\nCOMMAND${write_if_different}${UNICODE_DATA_IMPLEMENTATION}$<TARGET_FILE:GenerateUnicodeData>-c -u${UNICODE_DATA_PATH}-s${SPECIAL_CASING_PATH}-p${PROP_LIST_PATH}-w${WORD_BREAK_PATH}\nVERBATIM\nDEPENDSGenerateUnicodeData\nMAIN_DEPENDENCY${UNICODE_DATA_PATH}${SPECIAL_CASING_PATH}\n"}
{"Commit title": "LibGfx: Remove unused headers from BitmapFont.{cpp,h}", "Commit body": "@@ -5,20 +5,11 @@\n*/\n\n#include\"BitmapFont.h\"\n#include\"Bitmap.h\"\n#include\"Emoji.h\"\n#include<AK/StdLibExtras.h>\n#include<AK/StringBuilder.h>\n#include<AK/Utf32View.h>\n#include<AK/Utf8View.h>\n#include<AK/Vector.h>\n#include<LibCore/FileStream.h>\n#include<LibGfx/FontDatabase.h>\n#include<stdio.h>\n#include<stdlib.h>\n#include<string.h>\n#include<sys/mman.h>\n#include<unistd.h>\n\nnamespaceGfx{\n\n"}
{"Commit title": "LibGUI: Account for the row and column headers when painting a TableView", "Commit body": "@@ -73,8 +73,8 @@ void TableView::paint_event(PaintEvent& event)\ninty_offset =column_header().is_visible() ?column_header().height() :0;\n\nbooldummy;\nintfirst_visible_row =index_at_event_position(frame_inner_rect().top_left(), dummy).row();\nintlast_visible_row =index_at_event_position(frame_inner_rect().bottom_right(), dummy).row();\nintfirst_visible_row =index_at_event_position(frame_inner_rect().top_left().translated(x_offset, y_offset), dummy).row();\nintlast_visible_row =index_at_event_position(frame_inner_rect().bottom_right().translated(x_offset, y_offset), dummy).row();\n\nif(first_visible_row == -1)\nfirst_visible_row =0;\n"}
{"Commit title": "Everywhere: Force linker hash style to be gnu", "Commit body": "@@ -116,7 +116,8 @@ endforeach()\n\nset(CMAKE_INSTALL_NAME_TOOL\"\")\nset(CMAKE_SHARED_LIBRARY_SUFFIX\".so\")\nset(CMAKE_SHARED_LIBRARY_CREATE_CXX_FLAGS\"-shared\")\nset(CMAKE_SHARED_LIBRARY_CREATE_CXX_FLAGS\"-shared -Wl,--hash-style=gnu\")\nset(CMAKE_CXX_LINK_FLAGS\"-Wl,--hash-style=gnu\")\n\n#Note: MacOS has different rpath rules from linux.\n#We disable it completely for MacOS hosts to avoid having to track down all the individual flags to unset\n"}
{"Commit title": "xxhash.h as of 2c611a76f914828bed675f0f342d6c4199ffee1e:xxhash.h", "Commit body": ""}
{"Commit title": "Add --slow_usecs option to determine when the \"long op\" message is pr…", "Commit body": "@@ -1235,6 +1235,10 @@ DEFINE_int64(stats_interval_seconds, 0, \"Report stats every N seconds. This \"\nDEFINE_int32(stats_per_interval,0,\"Reports additional stats per interval when\"\n\"this is greater than 0.\");\n\nDEFINE_uint64(slow_usecs,1000000,\n\"A message is printed for operations that\"\n\"take at least this many microseconds.\");\n\nDEFINE_int64(report_interval_seconds,0,\n\"If greater than zero, it will write simple stats in CSV format\"\n\"to --report_file every N seconds\");\n@@ -2140,7 +2144,7 @@ class Stats {\n}\nhist_[op_type]->Add(micros);\n\nif(micros >20000&& !FLAGS_stats_interval) {\nif(micros >= FLAGS_slow_usecs&& !FLAGS_stats_interval) {\nfprintf(stderr,\"long op: %\"PRIu64\"micros%30s\\r\", micros,\"\");\nfflush(stderr);\n}\n"}
{"Commit title": "Reword", "Commit body": "@@ -1369,8 +1369,7 @@ struct Options : public DBOptions, public ColumnFamilyOptions {\n//NOT MAINTAINED: This function has not been and is not maintained.\n//DEPRECATED: This function might be removed in a future release.\n//In general, defaults are changed to suit broad interests. Opting\n//out of a change on upgrade should be deliberate and considered\n//rather than automatic and thoughtless.\n//out of a change on upgrade should be deliberate and considered.\nOptions*OldDefaults(introcksdb_major_version =4,\nintrocksdb_minor_version =6);\n\n"}
{"Commit title": "Add assertion", "Commit body": "@@ -338,6 +338,7 @@ class VersionStorageInfo {\n\nconstuint64_tfile_number = meta->fd.GetNumber();\n\nassert(file_locations_.find(file_number) == file_locations_.end());\nfile_locations_.emplace(file_number,FileLocation(level, pos));\n}\n}\n"}
{"Commit title": "typo.", "Commit body": "@@ -2096,7 +2096,7 @@ def array_interface(array: NumpyOrCupy) -> bytes:\nifarray.shape[0]!=n_samplesandis_flatten(array):\nwarnings.warn(\n\"Since 2.1.0, the shape of the gradient and hessian is required to\"\n\" be (n_samples, n_targets) or (n_samples,n_targets).\",\n\" be (n_samples, n_targets) or (n_samples,n_classes).\",\nFutureWarning,\n)\narray=array.reshape(n_samples,array.size//n_samples)\n"}
{"Commit title": "Cleanup.", "Commit body": "@@ -166,43 +166,7 @@ float GHistIndexMatrix::GetFvalue(size_t ridx, size_t fidx, bool is_cat) const {\nautoconst&values = cut.Values();\nautoconst&mins = cut.MinValues();\nautoconst&ptrs = cut.Ptrs();\nif(is_cat) {\nautogidx =GetGindex(ridx, fidx);\nif(gidx == -1) {\nreturnstd::numeric_limits<float>::quiet_NaN();\n}\nreturnvalues[gidx];\n}\n\nautoget_bin_val = [&](auto&column) {\nautobin_idx = column[ridx];\nif(bin_idx == common::DenseColumnIter<uint8_t,true>::kMissingId) {\nreturnstd::numeric_limits<float>::quiet_NaN();\n}\nreturncommon::HistogramCuts::NumericBinValue(ptrs, values, mins, fidx, bin_idx);\n};\n\nif(columns_->GetColumnType(fidx) == common::kDenseColumn) {\nif(columns_->AnyMissing()) {\nreturncommon::DispatchBinType(columns_->GetTypeSize(), [&](autodtype) {\nautocolumn = columns_->DenseColumn<decltype(dtype),true>(fidx);\nreturnget_bin_val(column);\n});\n}else{\nreturncommon::DispatchBinType(columns_->GetTypeSize(), [&](autodtype) {\nautocolumn = columns_->DenseColumn<decltype(dtype),false>(fidx);\nreturnget_bin_val(column);\n});\n}\n}else{\nreturncommon::DispatchBinType(columns_->GetTypeSize(), [&](autodtype) {\nautocolumn = columns_->SparseColumn<decltype(dtype)>(fidx,0);\nreturnget_bin_val(column);\n});\n}\n\nSPAN_CHECK(false);\nreturnstd::numeric_limits<float>::quiet_NaN();\nreturnthis->GetFvalue(ptrs, values, mins, ridx, fidx, is_cat);\n}\n\nfloatGHistIndexMatrix::GetFvalue(std::vector<std::uint32_t>const&ptrs,\n"}
{"Commit title": "flake", "Commit body": "@@ -8627,7 +8627,6 @@ def test_c_preprocessor(self):\nnormal=self.run_js('a.out.js')\nprint(str(normal))\n\n\n# Test that legacy settings that have been fixed to a specific value and their value can no longer be changed,\ndeftest_legacy_settings_forbidden_to_change(self):\nstderr=self.expect_fail([EMCC,'-s','MEMFS_APPEND_TO_TYPED_ARRAYS=0',test_file('hello_world.c')])\n"}
{"Commit title": "Address review comments for TextDecoder", "Commit body": "@@ -515,15 +515,15 @@ function stringToAscii(str, outPtr) {\n// Given a pointer 'ptr' to a null-terminated UTF8-encoded string in the given array that contains uint8 values, returns\n// a copy of that string as a Javascript String object.\n\nvarutf8_decoder=newTextDecoder('utf8');\nvarUTF8Decoder=newTextDecoder('utf8');\nfunctionUTF8ArrayToString(u8Array,idx){\nvarendPtr=idx;\n// TextDecoder needs to know the byte length in advance, it doesn't stop on null terminator by itself.\n// Also, use the length info to avoid running tiny strings through TextDecoder, since .subarray() allocates garbage.\nwhile(u8Array[endPtr])++endPtr;\nwhile(u8Array[endPtr])++endPtr;\n\nif(endPtr-idx>16&&u8Array.subarray){\nreturnutf8_decoder.decode(u8Array.subarray(idx,endPtr));\nreturnUTF8Decoder.decode(u8Array.subarray(idx,endPtr));\n}else{\nvaru0,u1,u2,u3,u4,u5;\n\n@@ -679,23 +679,22 @@ function lengthBytesUTF8(str) {\n// Given a pointer 'ptr' to a null-terminated UTF16LE-encoded string in the emscripten HEAP, returns\n// a copy of that string as a Javascript String object.\n\nvarutf16_decoder=newTextDecoder('utf-16le');\nvarUTF16Decoder=newTextDecoder('utf-16le');\nfunctionUTF16ToString(ptr){\nvarendPtr=ptr;\n// TextDecoder needs to know the byte length in advance, it doesn't stop on null terminator by itself.\n// Also, use the length info to avoid running tiny strings through TextDecoder, since .subarray() allocates garbage.\nwhile({{{makeGetValue('endPtr',0,'i16')}}})endPtr+=2;\nwhile({{{makeGetValue('endPtr',0,'i16')}}})endPtr+=2;\n\nif(endPtr-ptr>32){\nreturnutf16_decoder.decode(HEAPU8.subarray(ptr,endPtr));\nreturnUTF16Decoder.decode(HEAPU8.subarray(ptr,endPtr));\n}else{\nvari=0;\n\nvarstr='';\nwhile(1){\nvarcodeUnit={{{makeGetValue('ptr','i*2','i16')}}};\nif(codeUnit==0)\nreturnstr;\nif(codeUnit==0)returnstr;\n++i;\n// fromCharCode constructs a character from a UTF-16 code unit, so we can pass the UTF16 string right through.\nstr+=String.fromCharCode(codeUnit);\n"}
{"Commit title": "Style tweak - don't traverse if ignoreName and the node is a name", "Commit body": "@@ -1190,7 +1190,11 @@ void eliminate(Ref ast, bool memSafe) {\nRef target = node[2];\nRef value = node[3];\nboolnameTarget = target[0] == NAME;\ntraverseInOrder(target,true,  nameTarget);//evaluate left\n//If this is an assign to a name, handle it below rather than\n//traversing and treating as a read\nif(!nameTarget) {\ntraverseInOrder(target,true,false);//evaluate left\n}\ntraverseInOrder(value,false,false);//evaluate right\n//do the actual assignment\nif(nameTarget) {\n@@ -1230,7 +1234,11 @@ void eliminate(Ref ast, bool memSafe) {\n}\n}\n}elseif(type == SUB) {\ntraverseInOrder(node[1],false, !memSafe);//evaluate inner\n//Only keep track of the global array names in memsafe mode i.e.\n//when they may change underneath us due to resizing\nif(node[1][0] != NAME || memSafe) {\ntraverseInOrder(node[1],false,false);//evaluate inner\n}\ntraverseInOrder(node[2],false,false);//evaluate outer\n//ignoreSub means we are a write (happening later), not a read\nif(!ignoreSub && !isTempDoublePtrAccess(node)) {\n@@ -1290,7 +1298,10 @@ void eliminate(Ref ast, bool memSafe) {\ntraverseInOrder(node[2],false,false);\n}elseif(IGNORABLE_ELIMINATOR_SCAN_NODES.has(type)) {\n}elseif(type == CALL) {\ntraverseInOrder(node[1],false,true);\n//Named functions never change and are therefore safe to not track\nif(node[1][0] != NAME) {\ntraverseInOrder(node[1],false,false);\n}\nRef args = node[2];\nfor(size_ti =0; i < args->size(); i++) {\ntraverseInOrder(args[i],false,false);\n"}
{"Commit title": "Add Tiago Quelhas to AUTHORS", "Commit body": "@@ -181,4 +181,4 @@ a license to everyone to use it as detailed in LICENSE.)\n* Thaddée Tyl <thaddee.tyl@gmail.com>\n* Philipp Wiesemann <philipp.wiesemann@arcor.de>\n* Jan Jongboom <janjongboom@gmail.com> (copyright owned by Telenor Digital AS)\n\n* Tiago Quelhas <tiagoq@gmail.com>"}
{"Commit title": "-s ERROR_ON_UNDEFINED_SYMBOLS=1 should be the default value, so set i…", "Commit body": "@@ -1098,6 +1098,11 @@ def load(self, args=[]):\nsettings=re.sub(r'var ([\\w\\d]+)',r'self.attrs[\"\\1\"]',settings)\nexecsettings\n\n# Apply default values for settings that are configured from environment variables.\nifos.environ.get('EMSCRIPTEN_STRICT')andint(os.environ.get('EMSCRIPTEN_STRICT'))!=0:\n# The default value -s ERROR_ON_UNDEFINED_SYMBOLS=0 is deprecated. Use the default value 1 in strict mode.\nself.attrs['ERROR_ON_UNDEFINED_SYMBOLS']=1\n\n# Apply additional settings. First -O, then -s\nforiinrange(len(args)):\nifargs[i].startswith('-O'):\n"}
{"Commit title": "Ignore undefined symbols in symbol list", "Commit body": "@@ -33,11 +33,14 @@ def calculate(temp_files, in_temp, stdout_, stderr_, forced=[]):\n# Check if we need to include some libraries that we compile. (We implement libc ourselves in js, but\n# compile a malloc implementation and stdlibc++.)\n\ndefread_symbols(path,exclude=None):\nsymbols=map(lambdaline:line.strip().split(' ')[1],open(path).readlines())\nifexclude:\nsymbols=filter(lambdasymbol:symbolnotinexclude,symbols)\nreturnset(symbols)\ndefread_symbols(path,exclude=()):\nsymbols=set()\nwithopen(path,'r')asfile:\nforlineinfile:\nmark,symbol=line.strip().split(' ')\nifmark!='U'andsymbolnotinexclude:\nsymbols.add(symbol)\nreturnsymbols\n\ndefault_opts=[]\n# If we're building tracing, we should build the system libraries that way too.\n"}
{"Commit title": "Bump github/codeql-action from 3.24.10 to 3.25.11", "Commit body": "@@ -52,6 +52,6 @@ jobs:\n\n#Upload the results to GitHub's code scanning dashboard.\n-name:\"Upload to code-scanning\"\nuses:github/codeql-action/upload-sarif@4355270be187e1b672a7a1c7c7bae5afdc1ab94a#v3.24.10\nuses:github/codeql-action/upload-sarif@b611370bb5703a7efb587f9d136a52ea24c5c38c#v3.25.11\nwith:\nsarif_file:results.sarif"}
{"Commit title": "Bump github/codeql-action from 2.21.9 to 2.22.4", "Commit body": "@@ -52,6 +52,6 @@ jobs:\n\n#Upload the results to GitHub's code scanning dashboard.\n-name:\"Upload to code-scanning\"\nuses:github/codeql-action/upload-sarif@ddccb873888234080b77e9bc2d4764d5ccaaccf9#v2.21.9\nuses:github/codeql-action/upload-sarif@49abf0ba24d0b7953cb586944e918a0b92074c80#v2.22.4\nwith:\nsarif_file:results.sarif"}
{"Commit title": "Bump github/codeql-action from 2.22.5 to 2.22.7", "Commit body": "@@ -52,6 +52,6 @@ jobs:\n\n#Upload the results to GitHub's code scanning dashboard.\n-name:\"Upload to code-scanning\"\nuses:github/codeql-action/upload-sarif@74483a38d39275f33fcff5f35b679b5ca4a26a99#v2.22.5\nuses:github/codeql-action/upload-sarif@66b90a5db151a8042fa97405c6cf843bbe433f7b#v2.22.7\nwith:\nsarif_file:results.sarif"}
{"Commit title": "Bump github/codeql-action from 2.21.9 to 2.22.5", "Commit body": "@@ -52,6 +52,6 @@ jobs:\n\n#Upload the results to GitHub's code scanning dashboard.\n-name:\"Upload to code-scanning\"\nuses:github/codeql-action/upload-sarif@ddccb873888234080b77e9bc2d4764d5ccaaccf9#v2.21.9\nuses:github/codeql-action/upload-sarif@74483a38d39275f33fcff5f35b679b5ca4a26a99#v2.22.5\nwith:\nsarif_file:results.sarif"}
{"Commit title": "Bump github/codeql-action from 2.21.9 to 2.22.4", "Commit body": "@@ -52,6 +52,6 @@ jobs:\n\n#Upload the results to GitHub's code scanning dashboard.\n-name:\"Upload to code-scanning\"\nuses:github/codeql-action/upload-sarif@ddccb873888234080b77e9bc2d4764d5ccaaccf9#v2.21.9\nuses:github/codeql-action/upload-sarif@49abf0ba24d0b7953cb586944e918a0b92074c80#v2.22.4\nwith:\nsarif_file:results.sarif"}
{"Commit title": "Bump github/codeql-action from 2.21.9 to 2.22.3", "Commit body": "@@ -52,6 +52,6 @@ jobs:\n\n#Upload the results to GitHub's code scanning dashboard.\n-name:\"Upload to code-scanning\"\nuses:github/codeql-action/upload-sarif@ddccb873888234080b77e9bc2d4764d5ccaaccf9#v2.21.9\nuses:github/codeql-action/upload-sarif@0116bc2df50751f9724a2e35ef1f24d22f90e4e1#v2.22.3\nwith:\nsarif_file:results.sarif"}
{"Commit title": "Bump github/codeql-action from 2.21.9 to 2.22.1", "Commit body": "@@ -52,6 +52,6 @@ jobs:\n\n#Upload the results to GitHub's code scanning dashboard.\n-name:\"Upload to code-scanning\"\nuses:github/codeql-action/upload-sarif@ddccb873888234080b77e9bc2d4764d5ccaaccf9#v2.21.9\nuses:github/codeql-action/upload-sarif@fdcae64e1484d349b3366718cdfef3d404390e85#v2.22.1\nwith:\nsarif_file:results.sarif"}
{"Commit title": "Bump github/codeql-action from 2.21.4 to 2.21.7", "Commit body": "@@ -52,6 +52,6 @@ jobs:\n\n#Upload the results to GitHub's code scanning dashboard.\n-name:\"Upload to code-scanning\"\nuses:github/codeql-action/upload-sarif@a09933a12a80f87b87005513f0abb1494c27a716#v2.21.4\nuses:github/codeql-action/upload-sarif@04daf014b50eaf774287bf3f0f1869d4b4c4b913#v2.21.7\nwith:\nsarif_file:results.sarif"}
{"Commit title": "exception deleting empty path", "Commit body": "@@ -5445,6 +5445,22 @@ def test_fs_llseek(self):\ndeftest_fs_64bit(self):\nself.do_runf(test_file('fs/test_64bit.c'),'success')\n\ndeftest_fs_delete_empty(self):\nself.set_setting('FORCE_FILESYSTEM')\nself.emcc_args+=['-std=c++17']\nsrc=r'''\n#include <iostream>\n#include <filesystem>\nint main()\n{\nstd::string path(\"\");\nstd::filesystem::remove(path);\nstd::cout << \"hello\" << std::endl;\nreturn 0;\n}\n'''\nself.do_run(src,'hello\\n')\n\ndeftest_sigalrm(self):\nself.do_runf(test_file('test_sigalrm.c'),'Received alarm!')\nself.set_setting('EXIT_RUNTIME')\n"}
{"Commit title": "Bump github/codeql-action from 2.20.0 to 2.20.1", "Commit body": "@@ -52,6 +52,6 @@ jobs:\n\n#Upload the results to GitHub's code scanning dashboard.\n-name:\"Upload to code-scanning\"\nuses:github/codeql-action/upload-sarif@6c089f53dd51dc3fc7e599c3cb5356453a52ca9e#v2.20.0\nuses:github/codeql-action/upload-sarif@f6e388ebf0efc915c6c5b165b019ee61a6746a38#v2.20.1\nwith:\nsarif_file:results.sarif"}
{"Commit title": "changed createLazyFile to only issue xhr requests when the file is ac…", "Commit body": ""}
{"Commit title": "changed createLazyFile to only issue xhr requests when the file is ac…", "Commit body": "@@ -294,74 +294,97 @@ LibraryManager.library = {\nif (typeof XMLHttpRequest !== 'undefined') {\nif (!ENVIRONMENT_IS_WORKER) throw 'Cannot do synchronous binary XHRs outside webworkers in modern browsers. Use --embed-file or --preload-file in emcc';\n// Lazy chunked Uint8Array (implements get and length from Uint8Array). Actual getting is abstracted away for eventual reuse.\nvar LazyUint8Array = function(chunkSize, length) {\nthis.length = length;\nthis.chunkSize = chunkSize;\nvar LazyUint8Array = function() {\nthis.lengthKnown = false;\nthis.chunks = []; // Loaded chunks. Index is the chunk number\n}\nLazyUint8Array.prototype.get = function(idx) {\nif (idx > this.length-1 || idx < 0) {\nreturn undefined;\n}\nvar chunkOffset = idx % chunkSize;\nvar chunkNum = Math.floor(idx / chunkSize);\nvar chunkOffset = idx %this.chunkSize;\nvar chunkNum = Math.floor(idx /this.chunkSize);\nreturn this.getter(chunkNum)[chunkOffset];\n}\nLazyUint8Array.prototype.setDataGetter = function(getter) {\nthis.getter = getter;\n}\n\n// Find length\nvar xhr = new XMLHttpRequest();\nxhr.open('HEAD', url, false);\nxhr.send(null);\nif (!(xhr.status >= 200 && xhr.status < 300 || xhr.status === 304)) throw new Error(\"Couldn't load \" + url + \". Status: \" + xhr.status);\nvar datalength = Number(xhr.getResponseHeader(\"Content-length\"));\nvar header;\nvar hasByteServing = (header = xhr.getResponseHeader(\"Accept-Ranges\")) && header === \"bytes\";\n\nLazyUint8Array.prototype.getLength = function() {\n// Find length\nvar xhr = new XMLHttpRequest();\nxhr.open('HEAD', url, false);\nxhr.send(null);\nif (!(xhr.status >= 200 && xhr.status < 300 || xhr.status === 304)) throw new Error(\"Couldn't load \" + url + \". Status: \" + xhr.status);\nvar datalength = Number(xhr.getResponseHeader(\"Content-length\"));\nvar header;\nvar hasByteServing = (header = xhr.getResponseHeader(\"Accept-Ranges\")) && header === \"bytes\";\n#if SMALL_XHR_CHUNKS\nvar chunkSize = 1024; // Chunk size in bytes\nvar chunkSize = 1024; // Chunk size in bytes\n#else\nvar chunkSize = 1024*1024; // Chunk size in bytes\nvar chunkSize = 1024*1024; // Chunk size in bytes\n#endif\nif (!hasByteServing) chunkSize = datalength;\n\n// Function to get a range from the remote URL.\nvar doXHR = (function(from, to) {\nif (from > to) throw new Error(\"invalid range (\" + from + \", \" + to + \") or no bytes requested!\");\nif (to > datalength-1) throw new Error(\"only \" + datalength + \" bytes available! programmer error!\");\n\n// TODO: Use mozResponseArrayBuffer, responseStream, etc. if available.\nvar xhr = new XMLHttpRequest();\nxhr.open('GET', url, false);\nif (datalength !== chunkSize) xhr.setRequestHeader(\"Range\", \"bytes=\" + from + \"-\" + to);\n\n// Some hints to the browser that we want binary data.\nif (typeof Uint8Array != 'undefined') xhr.responseType = 'arraybuffer';\nif (xhr.overrideMimeType) {\nxhr.overrideMimeType('text/plain; charset=x-user-defined');\n}\nif (!hasByteServing) chunkSize = datalength;\n\n// Function to get a range from the remote URL.\nvar doXHR = (function(from, to) {\nif (from > to) throw new Error(\"invalid range (\" + from + \", \" + to + \") or no bytes requested!\");\nif (to > datalength-1) throw new Error(\"only \" + datalength + \" bytes available! programmer error!\");\n\n// TODO: Use mozResponseArrayBuffer, responseStream, etc. if available.\nvar xhr = new XMLHttpRequest();\nxhr.open('GET', url, false);\nif (datalength !== chunkSize) xhr.setRequestHeader(\"Range\", \"bytes=\" + from + \"-\" + to);\n\n// Some hints to the browser that we want binary data.\nif (typeof Uint8Array != 'undefined') xhr.responseType = 'arraybuffer';\nif (xhr.overrideMimeType) {\nxhr.overrideMimeType('text/plain; charset=x-user-defined');\n}\n\nxhr.send(null);\nif (!(xhr.status >= 200 && xhr.status < 300 || xhr.status === 304)) throw new Error(\"Couldn't load \" + url + \". Status: \" + xhr.status);\nif (xhr.response !== undefined) {\nreturn new Uint8Array(xhr.response || []);\n} else {\nreturn intArrayFromString(xhr.responseText || '', true);\n}\n});\nvar lazyArray = this;\nlazyArray.setDataGetter(function(chunkNum) {\nvar start = chunkNum * chunkSize;\nvar end = (chunkNum+1) * chunkSize - 1; // including this byte\nend = Math.min(end, datalength-1); // if datalength-1 is selected, this is the last block\nif (typeof(lazyArray.chunks[chunkNum]) === \"undefined\") {\nlazyArray.chunks[chunkNum] = doXHR(start, end);\n}\nif (typeof(lazyArray.chunks[chunkNum]) === \"undefined\") throw new Error(\"doXHR failed!\");\nreturn lazyArray.chunks[chunkNum];\n});\n\nthis._length = datalength;\nthis._chunkSize = chunkSize;\nthis.lengthKnown = true;\n}\n\nxhr.send(null);\nif (!(xhr.status >= 200 && xhr.status < 300 || xhr.status === 304)) throw new Error(\"Couldn't load \" + url + \". Status: \" + xhr.status);\nif (xhr.response !== undefined) {\nreturn new Uint8Array(xhr.response || []);\n} else {\nreturn intArrayFromString(xhr.responseText || '', true);\n}\nvar lazyArray = new LazyUint8Array();\nObject.defineProperty(lazyArray, \"length\", {\nget: function() {\nif(!this.lengthKnown) {\nthis.getLength();\n}\nreturn this._length;\n}\n});\n\nvar lazyArray = new LazyUint8Array(chunkSize, datalength);\nlazyArray.setDataGetter(function(chunkNum) {\nvar start = chunkNum * lazyArray.chunkSize;\nvar end = (chunkNum+1) * lazyArray.chunkSize - 1; // including this byte\nend = Math.min(end, datalength-1); // if datalength-1 is selected, this is the last block\nif (typeof(lazyArray.chunks[chunkNum]) === \"undefined\") {\nlazyArray.chunks[chunkNum] = doXHR(start, end);\n}\nif (typeof(lazyArray.chunks[chunkNum]) === \"undefined\") throw new Error(\"doXHR failed!\");\nreturn lazyArray.chunks[chunkNum];\nObject.defineProperty(lazyArray, \"chunkSize\", {\nget: function() {\nif(!this.lengthKnown) {\nthis.getLength();\n}\nreturn this._chunkSize;\n}\n});\n\nvar properties = { isDevice: false, contents: lazyArray };\n} else {\nvar properties = { isDevice: false, url: url };\n"}
{"Commit title": "changed createLazyFile to only issue xhr requests when the file is ac…", "Commit body": "@@ -294,74 +294,97 @@ LibraryManager.library = {\nif (typeof XMLHttpRequest !== 'undefined') {\nif (!ENVIRONMENT_IS_WORKER) throw 'Cannot do synchronous binary XHRs outside webworkers in modern browsers. Use --embed-file or --preload-file in emcc';\n// Lazy chunked Uint8Array (implements get and length from Uint8Array). Actual getting is abstracted away for eventual reuse.\nvar LazyUint8Array = function(chunkSize, length) {\nthis.length = length;\nthis.chunkSize = chunkSize;\nvar LazyUint8Array = function() {\nthis.lengthKnown = false;\nthis.chunks = []; // Loaded chunks. Index is the chunk number\n}\nLazyUint8Array.prototype.get = function(idx) {\nif (idx > this.length-1 || idx < 0) {\nreturn undefined;\n}\nvar chunkOffset = idx % chunkSize;\nvar chunkNum = Math.floor(idx / chunkSize);\nvar chunkOffset = idx %this.chunkSize;\nvar chunkNum = Math.floor(idx /this.chunkSize);\nreturn this.getter(chunkNum)[chunkOffset];\n}\nLazyUint8Array.prototype.setDataGetter = function(getter) {\nthis.getter = getter;\n}\n\n// Find length\nvar xhr = new XMLHttpRequest();\nxhr.open('HEAD', url, false);\nxhr.send(null);\nif (!(xhr.status >= 200 && xhr.status < 300 || xhr.status === 304)) throw new Error(\"Couldn't load \" + url + \". Status: \" + xhr.status);\nvar datalength = Number(xhr.getResponseHeader(\"Content-length\"));\nvar header;\nvar hasByteServing = (header = xhr.getResponseHeader(\"Accept-Ranges\")) && header === \"bytes\";\n\nLazyUint8Array.prototype.getLength = function() {\n// Find length\nvar xhr = new XMLHttpRequest();\nxhr.open('HEAD', url, false);\nxhr.send(null);\nif (!(xhr.status >= 200 && xhr.status < 300 || xhr.status === 304)) throw new Error(\"Couldn't load \" + url + \". Status: \" + xhr.status);\nvar datalength = Number(xhr.getResponseHeader(\"Content-length\"));\nvar header;\nvar hasByteServing = (header = xhr.getResponseHeader(\"Accept-Ranges\")) && header === \"bytes\";\n#if SMALL_XHR_CHUNKS\nvar chunkSize = 1024; // Chunk size in bytes\nvar chunkSize = 1024; // Chunk size in bytes\n#else\nvar chunkSize = 1024*1024; // Chunk size in bytes\nvar chunkSize = 1024*1024; // Chunk size in bytes\n#endif\nif (!hasByteServing) chunkSize = datalength;\n\n// Function to get a range from the remote URL.\nvar doXHR = (function(from, to) {\nif (from > to) throw new Error(\"invalid range (\" + from + \", \" + to + \") or no bytes requested!\");\nif (to > datalength-1) throw new Error(\"only \" + datalength + \" bytes available! programmer error!\");\n\n// TODO: Use mozResponseArrayBuffer, responseStream, etc. if available.\nvar xhr = new XMLHttpRequest();\nxhr.open('GET', url, false);\nif (datalength !== chunkSize) xhr.setRequestHeader(\"Range\", \"bytes=\" + from + \"-\" + to);\n\n// Some hints to the browser that we want binary data.\nif (typeof Uint8Array != 'undefined') xhr.responseType = 'arraybuffer';\nif (xhr.overrideMimeType) {\nxhr.overrideMimeType('text/plain; charset=x-user-defined');\n}\nif (!hasByteServing) chunkSize = datalength;\n\n// Function to get a range from the remote URL.\nvar doXHR = (function(from, to) {\nif (from > to) throw new Error(\"invalid range (\" + from + \", \" + to + \") or no bytes requested!\");\nif (to > datalength-1) throw new Error(\"only \" + datalength + \" bytes available! programmer error!\");\n\n// TODO: Use mozResponseArrayBuffer, responseStream, etc. if available.\nvar xhr = new XMLHttpRequest();\nxhr.open('GET', url, false);\nif (datalength !== chunkSize) xhr.setRequestHeader(\"Range\", \"bytes=\" + from + \"-\" + to);\n\n// Some hints to the browser that we want binary data.\nif (typeof Uint8Array != 'undefined') xhr.responseType = 'arraybuffer';\nif (xhr.overrideMimeType) {\nxhr.overrideMimeType('text/plain; charset=x-user-defined');\n}\n\nxhr.send(null);\nif (!(xhr.status >= 200 && xhr.status < 300 || xhr.status === 304)) throw new Error(\"Couldn't load \" + url + \". Status: \" + xhr.status);\nif (xhr.response !== undefined) {\nreturn new Uint8Array(xhr.response || []);\n} else {\nreturn intArrayFromString(xhr.responseText || '', true);\n}\n});\nvar lazyArray = this;\nlazyArray.setDataGetter(function(chunkNum) {\nvar start = chunkNum * chunkSize;\nvar end = (chunkNum+1) * chunkSize - 1; // including this byte\nend = Math.min(end, datalength-1); // if datalength-1 is selected, this is the last block\nif (typeof(lazyArray.chunks[chunkNum]) === \"undefined\") {\nlazyArray.chunks[chunkNum] = doXHR(start, end);\n}\nif (typeof(lazyArray.chunks[chunkNum]) === \"undefined\") throw new Error(\"doXHR failed!\");\nreturn lazyArray.chunks[chunkNum];\n});\n\nthis._length = datalength;\nthis._chunkSize = chunkSize;\nthis.lengthKnown = true;\n}\n\nxhr.send(null);\nif (!(xhr.status >= 200 && xhr.status < 300 || xhr.status === 304)) throw new Error(\"Couldn't load \" + url + \". Status: \" + xhr.status);\nif (xhr.response !== undefined) {\nreturn new Uint8Array(xhr.response || []);\n} else {\nreturn intArrayFromString(xhr.responseText || '', true);\n}\nvar lazyArray = new LazyUint8Array();\nObject.defineProperty(lazyArray, \"length\", {\nget: function() {\nif(!this.lengthKnown) {\nthis.getLength();\n}\nreturn this._length;\n}\n});\n\nvar lazyArray = new LazyUint8Array(chunkSize, datalength);\nlazyArray.setDataGetter(function(chunkNum) {\nvar start = chunkNum * lazyArray.chunkSize;\nvar end = (chunkNum+1) * lazyArray.chunkSize - 1; // including this byte\nend = Math.min(end, datalength-1); // if datalength-1 is selected, this is the last block\nif (typeof(lazyArray.chunks[chunkNum]) === \"undefined\") {\nlazyArray.chunks[chunkNum] = doXHR(start, end);\n}\nif (typeof(lazyArray.chunks[chunkNum]) === \"undefined\") throw new Error(\"doXHR failed!\");\nreturn lazyArray.chunks[chunkNum];\nObject.defineProperty(lazyArray, \"chunkSize\", {\nget: function() {\nif(!this.lengthKnown) {\nthis.getLength();\n}\nreturn this._chunkSize;\n}\n});\n\nvar properties = { isDevice: false, contents: lazyArray };\n} else {\nvar properties = { isDevice: false, url: url };\n"}
{"Commit title": "fix", "Commit body": "@@ -661,6 +661,7 @@ def setUp(self):\n# expect this.  --no-clean can be used to keep the old contents for the new test\n# run. This can be useful when iterating on a given test with extra files you want to keep\n# around in the output directory.\nmake_dir_writeable(self.working_dir)\nutils.delete_contents(self.working_dir)\nelse:\nprint('Creating new test output directory')\n"}
{"Commit title": "Update convertJSArrayToNumberVector comment to describe typed array c…", "Commit body": "@@ -596,8 +596,7 @@ namespace emscripten {\nrv.resize(l);\n\n//Copy the array into our vector through the use of typed arrays.\n//The main issue however, is that if it contains something else than numbers (for example a string),\n//it will try to convert it as if using Number().\n//It will try to convert each element through Number().\n//See https://www.ecma-international.org/ecma-262/6.0/#sec-%typedarray%.prototype.set-array-offset\n//and https://www.ecma-international.org/ecma-262/6.0/#sec-tonumber\nval memoryView{typed_memory_view(l, rv.data()) };\n"}
{"Commit title": "update test", "Commit body": "@@ -7780,9 +7780,9 @@ def test(filename, expectations):\n([],25, ['abort','tempDoublePtr'], ['waka'],48213,26,19),\n(['-O1'],20, ['abort','tempDoublePtr'], ['waka'],13460,17,17),\n(['-O2'],20, ['abort','tempDoublePtr'], ['waka'],13381,17,17),\n(['-O3'],13, ['abort'],                  ['tempDoublePtr','waka'],10165,16,3),# in -O3, -Os and -Oz we metadce\n(['-Os'],13, ['abort'],                  ['tempDoublePtr','waka'],10091,16,3),\n(['-Oz'],13, ['abort'],                  ['tempDoublePtr','waka'],10081,16,3),\n(['-O3'],7, ['abort'],                  ['tempDoublePtr','waka'],2678,10,2),# in -O3, -Os and -Oz we metadce\n(['-Os'],7, ['abort'],                  ['tempDoublePtr','waka'],2771,10,2),\n(['-Oz'],7, ['abort'],                  ['tempDoublePtr','waka'],2765,10,2),\n# finally, check what happens when we export nothing. wasm should be almost empty\n(['-Os','-s','EXPORTED_FUNCTIONS=[]'],\n0, [],                         ['tempDoublePtr','waka'],8,0,0),# totally empty!\n"}
{"Commit title": "Utilize parent pointer appropriately", "Commit body": "@@ -40,8 +40,8 @@\nExecutionLogWidget::ExecutionLogWidget(QWidget *parent,constLog::MsgTypes &types)\n: QWidget(parent)\n, m_ui(newUi::ExecutionLogWidget)\n, m_msgList(newLogListWidget(MAX_LOG_MESSAGES, types))\n, m_peerList(newLogListWidget(MAX_LOG_MESSAGES))\n, m_msgList(newLogListWidget(MAX_LOG_MESSAGES, types,this))\n, m_peerList(newLogListWidget(MAX_LOG_MESSAGES, Log::ALL,this))\n{\nm_ui->setupUi(this);\n\n@@ -63,8 +63,6 @@ ExecutionLogWidget::ExecutionLogWidget(QWidget *parent, const Log::MsgTypes &typ\n\nExecutionLogWidget::~ExecutionLogWidget()\n{\ndeletem_msgList;\ndeletem_peerList;\ndeletem_ui;\n}\n\n"}
{"Commit title": "Use a more detailed alert mask where possible", "Commit body": "@@ -392,7 +392,11 @@ Session::Session(QObject *parent)\n| libt::alert::tracker_notification\n| libt::alert::status_notification\n| libt::alert::ip_block_notification\n#ifLIBTORRENT_VERSION_NUM < 10110\n| libt::alert::progress_notification\n#else\n| libt::alert::file_progress_notification\n#endif\n| libt::alert::stats_notification;\n\n#ifLIBTORRENT_VERSION_NUM < 10100\n"}
{"Commit title": "Patch torrent resume parameters", "Commit body": "@@ -392,7 +392,7 @@ Session::Session(QObject *parent)\n| libt::alert::tracker_notification\n| libt::alert::status_notification\n| libt::alert::ip_block_notification\n| libt::alert::progress_notification\n| libt::alert::file_progress_notification\n| libt::alert::stats_notification;\n\n#ifLIBTORRENT_VERSION_NUM < 10100\n"}
{"Commit title": "Add missing units", "Commit body": "@@ -388,10 +388,10 @@ window.addEvent('load', function () {\n$('TotalPeerConnections').set('html',serverState.total_peer_connections);\n$('ReadCacheHits').set('html',serverState.read_cache_hits);\n$('TotalBuffersSize').set('html',friendlyUnit(serverState.total_buffers_size,false));\n$('WriteCacheOverload').set('html',serverState.write_cache_overload);\n$('ReadCacheOverload').set('html',serverState.read_cache_overload);\n$('WriteCacheOverload').set('html',serverState.write_cache_overload+\"%\");\n$('ReadCacheOverload').set('html',serverState.read_cache_overload+\"%\");\n$('QueuedIOJobs').set('html',serverState.queued_io_jobs);\n$('AverageTimeInQueue').set('html',serverState.average_time_queue);\n$('AverageTimeInQueue').set('html',serverState.average_time_queue+\" ms\");\n$('TotalQueuedSize').set('html',friendlyUnit(serverState.total_queued_size,false));\n}\n\n"}
{"Commit title": "Change MixedModeAlgorithm default to TCP.Closes#7779.", "Commit body": "@@ -304,7 +304,7 @@ Session::Session(QObject *parent)\n, m_btProtocol(BITTORRENT_SESSION_KEY(\"BTProtocol\"), BTProtocol::Both\n, clampValue(BTProtocol::Both, BTProtocol::UTP))\n, m_isUTPRateLimited(BITTORRENT_SESSION_KEY(\"uTPRateLimited\"),true)\n, m_utpMixedMode(BITTORRENT_SESSION_KEY(\"uTPMixedMode\"), MixedModeAlgorithm::Proportional\n, m_utpMixedMode(BITTORRENT_SESSION_KEY(\"uTPMixedMode\"), MixedModeAlgorithm::TCP\n, clampValue(MixedModeAlgorithm::TCP, MixedModeAlgorithm::Proportional))\n, m_multiConnectionsPerIpEnabled(BITTORRENT_SESSION_KEY(\"MultiConnectionsPerIp\"),false)\n, m_isAddTrackersEnabled(BITTORRENT_SESSION_KEY(\"AddTrackersEnabled\"),false)\n"}
{"Commit title": "Fix RSS Parser", "Commit body": "@@ -227,10 +227,9 @@ void Parser::parse(const QByteArray &feedData)\n//read and create items from a rss document\nvoidParser::parse_impl(constQByteArray &feedData)\n{\nqDebug() << Q_FUNC_INFO;\n\nQXmlStreamReaderxml(feedData);\nboolfoundChannel =false;\n\nwhile(xml.readNextStartElement()) {\nif(xml.name() ==\"rss\") {\n//Find channels\n@@ -258,11 +257,15 @@ void Parser::parse_impl(const QByteArray &feedData)\n}\n}\n\nif(xml.hasError())\nm_result.error= xml.errorString();\nelseif(!foundChannel)\nif(!foundChannel) {\nm_result.error=tr(\"Invalid RSS feed.\");\nelse\n}\nelseif(xml.hasError()) {\nm_result.error=tr(\"%1 (line: %2, column: %3, offset: %4).\")\n.arg(xml.errorString()).arg(xml.lineNumber())\n.arg(xml.columnNumber()).arg(xml.characterOffset());\n}\nelse{\n//Sort article list chronologically\n//NOTE: We don't need to sort it here if articles are always\n//sorted in fetched XML in reverse chronological order\n@@ -271,6 +274,7 @@ void Parser::parse_impl(const QByteArray &feedData)\n{\nreturna1[\"date\"].toDateTime() < a2[\"date\"].toDateTime();\n});\n}\n\nemitfinished(m_result);\nm_result.articles.clear();//clear articles only\n@@ -288,35 +292,34 @@ void Parser::parseRssArticle(QXmlStreamReader &xml)\nbreak;\n\nif(xml.isStartElement()) {\nconstQStringtext(xml.readElementText().trimmed());\n\nif(name ==QLatin1String(\"title\")) {\narticle[Article::KeyTitle] =text;\narticle[Article::KeyTitle] =xml.readElementText().trimmed();\n}\nelseif(name ==QLatin1String(\"enclosure\")) {\nif(xml.attributes().value(\"type\") ==QLatin1String(\"application/x-bittorrent\"))\narticle[Article::KeyTorrentURL] = xml.attributes().value(QLatin1String(\"url\")).toString();\n}\nelseif(name ==QLatin1String(\"link\")) {\nconstQString text {xml.readElementText().trimmed()};\nif(text.startsWith(QLatin1String(\"magnet:\"), Qt::CaseInsensitive))\narticle[Article::KeyTorrentURL] = text;//magnet link instead of a news URL\nelse\narticle[Article::KeyLink] = text;\n}\nelseif(name ==QLatin1String(\"description\")) {\narticle[Article::KeyDescription] =text;\narticle[Article::KeyDescription] =xml.readElementText(QXmlStreamReader::IncludeChildElements);\n}\nelseif(name ==QLatin1String(\"pubDate\")) {\narticle[Article::KeyDate] =parseDate(text);\narticle[Article::KeyDate] =parseDate(xml.readElementText().trimmed());\n}\nelseif(name ==QLatin1String(\"author\")) {\narticle[Article::KeyAuthor] =text;\narticle[Article::KeyAuthor] =xml.readElementText().trimmed();\n}\nelseif(name ==QLatin1String(\"guid\")) {\narticle[Article::KeyId] =text;\narticle[Article::KeyId] =xml.readElementText().trimmed();\n}\nelse{\narticle[name] =text;\narticle[name] =xml.readElementText(QXmlStreamReader::IncludeChildElements);\n}\n}\n}\n@@ -326,17 +329,14 @@ void Parser::parseRssArticle(QXmlStreamReader &xml)\n\nvoidParser::parseRSSChannel(QXmlStreamReader &xml)\n{\nqDebug() << Q_FUNC_INFO;\nQ_ASSERT(xml.isStartElement() && xml.name() ==\"channel\");\n\nwhile(!xml.atEnd()) {\nxml.readNext();\n\nif(xml.isStartElement()) {\nif(xml.name() ==\"title\") {\nif(xml.name() ==QLatin1String(\"title\")) {\nm_result.title= xml.readElementText();\n}\nelseif(xml.name() ==\"lastBuildDate\") {\nelseif(xml.name() ==QLatin1String(\"lastBuildDate\")) {\nQString lastBuildDate = xml.readElementText();\nif(!lastBuildDate.isEmpty()) {\nif(m_result.lastBuildDate== lastBuildDate) {\n@@ -346,7 +346,7 @@ void Parser::parseRSSChannel(QXmlStreamReader &xml)\nm_result.lastBuildDate= lastBuildDate;\n}\n}\nelseif(xml.name() ==\"item\") {\nelseif(xml.name() ==QLatin1String(\"item\")) {\nparseRssArticle(xml);\n}\n}\n@@ -366,14 +366,12 @@ void Parser::parseAtomArticle(QXmlStreamReader &xml)\nbreak;\n\nif(xml.isStartElement()) {\nconstQStringtext(xml.readElementText().trimmed());\n\nif(name ==QLatin1String(\"title\")) {\narticle[Article::KeyTitle] =text;\narticle[Article::KeyTitle] =xml.readElementText().trimmed();\n}\nelseif(name ==QLatin1String(\"link\")) {\nQStringlink= (xml.attributes().isEmpty()\n?text\n?xml.readElementText().trimmed()\n: xml.attributes().value(QLatin1String(\"href\")).toString());\n\nif(link.startsWith(QLatin1String(\"magnet:\"), Qt::CaseInsensitive))\n@@ -385,42 +383,38 @@ void Parser::parseAtomArticle(QXmlStreamReader &xml)\narticle[Article::KeyLink] = (m_baseUrl.isEmpty() ?link: m_baseUrl +link);\n\n}\nelseif((name ==QLatin1String(\"summary\")) || (name ==QLatin1String(\"content\"))){\nelseif((name ==QLatin1String(\"summary\")) || (name ==QLatin1String(\"content\"))){\nif(doubleContent) {//Duplicate content -> ignore\nxml.readNext();\n\nwhile((xml.name() !=QLatin1String(\"summary\")) && (xml.name() !=QLatin1String(\"content\")))\nxml.readNext();\n\nxml.skipCurrentElement();\ncontinue;\n}\n\n//Try to also parse broken articles, which don't use html '&' escapes\n//Actually works great for non-broken content too\nQString feedText = xml.readElementText(QXmlStreamReader::IncludeChildElements);\nif(!feedText.isEmpty())\narticle[Article::KeyDescription] = feedText.trimmed();\n\ndoubleContent =true;\nQString feedText = xml.readElementText(QXmlStreamReader::IncludeChildElements).trimmed();\nif(!feedText.isEmpty()){\narticle[Article::KeyDescription] = feedText;\ndoubleContent =true;\n}\n}\nelseif(name ==QLatin1String(\"updated\")) {\n//ATOM uses standard compliant date, don't do fancy stuff\nQDateTime articleDate =QDateTime::fromString(text, Qt::ISODate);\nQDateTime articleDate =QDateTime::fromString(xml.readElementText().trimmed(), Qt::ISODate);\narticle[Article::KeyDate] = (articleDate.isValid() ? articleDate :QDateTime::currentDateTime());\n}\nelseif(name ==QLatin1String(\"author\")) {\nxml.readNext();\nwhile(xml.name() !=QLatin1String(\"author\")) {\nwhile(xml.readNextStartElement()) {\nif(xml.name() ==QLatin1String(\"name\"))\narticle[Article::KeyAuthor] = xml.readElementText().trimmed();\nxml.readNext();\nelse\nxml.skipCurrentElement();\n}\n}\nelseif(name ==QLatin1String(\"id\")) {\narticle[Article::KeyId] =text;\narticle[Article::KeyId] =xml.readElementText().trimmed();\n}\nelse{\narticle[name] =text;\narticle[name] =xml.readElementText(QXmlStreamReader::IncludeChildElements);\n}\n}\n}\n@@ -430,19 +424,16 @@ void Parser::parseAtomArticle(QXmlStreamReader &xml)\n\nvoidParser::parseAtomChannel(QXmlStreamReader &xml)\n{\nqDebug() << Q_FUNC_INFO;\nQ_ASSERT(xml.isStartElement() && xml.name() ==\"feed\");\n\nm_baseUrl = xml.attributes().value(\"xml:base\").toString();\n\nwhile(!xml.atEnd()) {\nxml.readNext();\n\nif(xml.isStartElement()) {\nif(xml.name() ==\"title\") {\nif(xml.name() ==QLatin1String(\"title\")) {\nm_result.title= xml.readElementText();\n}\nelseif(xml.name() ==\"updated\") {\nelseif(xml.name() ==QLatin1String(\"updated\")) {\nQString lastBuildDate = xml.readElementText();\nif(!lastBuildDate.isEmpty()) {\nif(m_result.lastBuildDate== lastBuildDate) {\n@@ -452,7 +443,7 @@ void Parser::parseAtomChannel(QXmlStreamReader &xml)\nm_result.lastBuildDate= lastBuildDate;\n}\n}\nelseif(xml.name() ==\"entry\") {\nelseif(xml.name() ==QLatin1String(\"entry\")) {\nparseAtomArticle(xml);\n}\n}\n"}
{"Commit title": "Optimize Session::startupTorrents()", "Commit body": "@@ -1898,7 +1898,6 @@ void Session::startUpTorrents()\nQStringList fastresumes = resumeDataDir.entryList(\nQStringList(QLatin1String(\"*.fastresume\")), QDir::Files, QDir::Unsorted);\n\nQString filePath;\nLogger *constlogger =Logger::instance();\n\ntypedefstruct\n@@ -1909,28 +1908,41 @@ void Session::startUpTorrents()\nQByteArray data;\n} TorrentResumeData;\n\nautostartupTorrent = [this, logger, resumeDataDir](constTorrentResumeData &params)\n{\nQString filePath = resumeDataDir.filePath(QString(\"%1.torrent\").arg(params.hash));\nqDebug() <<\"Starting up torrent\"<< params.hash<<\"...\";\nif(!addTorrent_impl(params.addTorrentData, params.magnetUri,TorrentInfo::loadFromFile(filePath), params.data))\nlogger->addMessage(tr(\"Unable to resume torrent '%1'.\",\"e.g: Unable to resume torrent 'hash'.\")\n.arg(params.hash), Log::CRITICAL);\n};\n\nqDebug(\"Starting up torrents\");\nqDebug(\"Queue size: %d\", fastresumes.size());\n//Resume downloads\nQMap<int, TorrentResumeData> queuedResumeData;\nintnextQueuePosition =1;\nQRegExprx(QLatin1String(\"^([A-Fa-f0-9]{40})\\\\.fastresume$\"));\nforeach(constQString &fastresumeName, fastresumes) {\nif(rx.indexIn(fastresumeName) == -1)continue;\n\nQString hash = rx.cap(1);\nQString fastresumePath =\nresumeDataDir.absoluteFilePath(fastresumeName);\nQString fastresumePath = resumeDataDir.absoluteFilePath(fastresumeName);\nQByteArray data;\nAddTorrentData resumeData;\nMagnetUri magnetUri;\nintqueuePosition;\nif(readFile(fastresumePath, data) &&loadTorrentResumeData(data, resumeData, queuePosition, magnetUri)) {\nif(queuePosition ==0) {\nfilePath = resumeDataDir.filePath(QString(\"%1.torrent\").arg(hash));\nqDebug(\"Starting up torrent %s ...\",qPrintable(hash));\nif(!addTorrent_impl(resumeData, magnetUri,TorrentInfo::loadFromFile(filePath), data))\nlogger->addMessage(tr(\"Unable to resume torrent '%1'.\",\"e.g: Unable to resume torrent 'hash'.\")\n.arg(hash), Log::CRITICAL);\nif(queuePosition <= nextQueuePosition) {\nstartupTorrent({ hash, magnetUri, resumeData, data });\n\nif(queuePosition == nextQueuePosition) {\n++nextQueuePosition;\nwhile(queuedResumeData.contains(nextQueuePosition)) {\nstartupTorrent(queuedResumeData.take(nextQueuePosition));\n++nextQueuePosition;\n}\n}\n}\nelse{\nqueuedResumeData[queuePosition] = { hash, magnetUri, resumeData, data };\n@@ -1939,13 +1951,8 @@ void Session::startUpTorrents()\n}\n\n//starting up downloading torrents (queue position > 0)\nforeach(constTorrentResumeData &torrentResumeData, queuedResumeData) {\nfilePath = resumeDataDir.filePath(QString(\"%1.torrent\").arg(torrentResumeData.hash));\nqDebug(\"Starting up torrent %s ...\",qPrintable(torrentResumeData.hash));\nif(!addTorrent_impl(torrentResumeData.addTorrentData, torrentResumeData.magnetUri,TorrentInfo::loadFromFile(filePath), torrentResumeData.data))\nlogger->addMessage(tr(\"Unable to resume torrent '%1'.\",\"e.g: Unable to resume torrent 'hash'.\")\n.arg(torrentResumeData.hash), Log::CRITICAL);\n}\nforeach(constTorrentResumeData &torrentResumeData, queuedResumeData)\nstartupTorrent(torrentResumeData);\n}\n\nquint64Session::getAlltimeDL()const\n"}
{"Commit title": "Fix copy tracker build failure", "Commit body": "@@ -34,6 +34,7 @@\n#include<QHash>\n#include<QAction>\n#include<QColor>\n#include<QDebug>\n#include<libtorrent/version.hpp>\n#include<libtorrent/peer_info.hpp>\n#include\"trackerlist.h\"\n"}
{"Commit title": "Optimize the addittion of huge torrents. Fixes issue#288.", "Commit body": "@@ -1270,20 +1270,11 @@ void QBtSession::loadTorrentTempData(QTorrentHandle &h, QString savePath, bool m\n//Update file names\nconstQStringList files_path =TorrentTempData::getFilesPath(hash);\nboolforce_recheck =false;\nQDirbase_dir(h.save_path());\nif(files_path.size() == h.num_files()) {\nfor(inti=0; i<h.num_files(); ++i) {\nQString old_path = h.absolute_files_path().at(i);\nold_path.replace(\"\\\\\",\"/\");\nif(!QFile::exists(old_path)) {\n//Remove old parent folder manually since we will\n//not get a file_renamed alert\nQStringList parts = old_path.split(\"/\", QString::SkipEmptyParts);\nparts.removeLast();\nif(!parts.empty())\nQDir().rmpath(parts.join(\"/\"));\n}\nconstQString &path = files_path.at(i);\nif(!force_recheck &&QDir(h.save_path()).exists(path))\nif(!force_recheck &&base_dir.exists(path))\nforce_recheck =true;\nqDebug(\"Renaming file to %s\",qPrintable(path));\nh.rename_file(i, path);\n"}
{"Commit title": "Drop extra semicolon", "Commit body": "@@ -42,5 +42,5 @@ inline namespace WindowStateNS\nHidden\n#endif\n};\nQ_ENUM_NS(WindowState);\nQ_ENUM_NS(WindowState)\n}"}
{"Commit title": "NSIS: Update Indonesian translation", "Commit body": "@@ -31,7 +31,7 @@ LangString inst_requires_64bit ${LANG_INDONESIAN} \"Aplikasi ini hanya berjalan p\n;LangString inst_requires_win7 ${LANG_ENGLISH} \"This qBittorrent version requires at least Windows 7.\"\nLangStringinst_requires_win7${LANG_INDONESIAN}\"Versi qBittorrent ini membutuhkan setidaknya Windows 7.\"\n;LangString inst_requires_win10 ${LANG_ENGLISH} \"This installer requires at least Windows 10 1809.\"\nLangStringinst_requires_win10${LANG_INDONESIAN}\"This installer requires at leastWindows 10 1809.\"\nLangStringinst_requires_win10${LANG_INDONESIAN}\"Penginstal ini membutuhkan setidaknyaWindows 10 1809.\"\n;LangString inst_uninstall_link_description ${LANG_ENGLISH} \"Uninstall qBittorrent\"\nLangStringinst_uninstall_link_description${LANG_INDONESIAN}\"Copot qBittorrent\"\n\n"}
{"Commit title": "Update german.nsi", "Commit body": "@@ -7,7 +7,7 @@ LangString inst_dekstop ${LANG_GERMAN} \"Verknüpfung auf dem Desktop erstellen\"\n;LangString inst_startmenu ${LANG_ENGLISH} \"Create Start Menu Shortcut\"\nLangStringinst_startmenu${LANG_GERMAN}\"Eintrag im Startmenü erstellen\"\n;LangString inst_startup ${LANG_ENGLISH} \"Start qBittorrent on Windows start up\"\nLangStringinst_startup${LANG_GERMAN}\"qBittorrent mit Windowsmitstarten\"\nLangStringinst_startup${LANG_GERMAN}\"qBittorrent mit Windowsstarten\"\n;LangString inst_torrent ${LANG_ENGLISH} \"Open .torrent files with qBittorrent\"\nLangStringinst_torrent${LANG_GERMAN}\"Öffne .torrent-Dateien mit qBittorrent\"\n;LangString inst_magnet ${LANG_ENGLISH} \"Open magnet links with qBittorrent\"\n@@ -29,9 +29,9 @@ LangString launch_qbt ${LANG_GERMAN} \"Starte qBittorrent.\"\n;LangString inst_requires_64bit ${LANG_ENGLISH} \"This installer works only in 64-bit Windows versions.\"\nLangStringinst_requires_64bit${LANG_GERMAN}\"Diese Installation funktioniert nur mit einer 64-bit Version von Windows.\"\n;LangString inst_requires_win7 ${LANG_ENGLISH} \"This qBittorrent version requires at least Windows 7.\"\nLangStringinst_requires_win7${LANG_GERMAN}\"Diese Version von qBittorrent erfordertzumindestWindows 7.\"\nLangStringinst_requires_win7${LANG_GERMAN}\"Diese Version von qBittorrent erfordertmindestensWindows 7.\"\n;LangString inst_uninstall_link_description ${LANG_ENGLISH} \"Uninstall qBittorrent\"\nLangStringinst_uninstall_link_description${LANG_GERMAN}\"UninstallqBittorrent\"\nLangStringinst_uninstall_link_description${LANG_GERMAN}\"qBittorrentdeinstallieren\"\n\n;------------------------------------\n;Uninstaller strings\n"}
{"Commit title": "Add missing units", "Commit body": "@@ -388,10 +388,10 @@ window.addEvent('load', function () {\n$('TotalPeerConnections').set('html',serverState.total_peer_connections);\n$('ReadCacheHits').set('html',serverState.read_cache_hits);\n$('TotalBuffersSize').set('html',friendlyUnit(serverState.total_buffers_size,false));\n$('WriteCacheOverload').set('html',serverState.write_cache_overload);\n$('ReadCacheOverload').set('html',serverState.read_cache_overload);\n$('WriteCacheOverload').set('html',serverState.write_cache_overload+\"%\");\n$('ReadCacheOverload').set('html',serverState.read_cache_overload+\"%\");\n$('QueuedIOJobs').set('html',serverState.queued_io_jobs);\n$('AverageTimeInQueue').set('html',serverState.average_time_queue);\n$('AverageTimeInQueue').set('html',serverState.average_time_queue+\" ms\");\n$('TotalQueuedSize').set('html',friendlyUnit(serverState.total_queued_size,false));\n}\n\n"}
{"Commit title": "Add some padding to macOS app icon", "Commit body": ""}
{"Commit title": "Make file icon look like other macOS icons", "Commit body": ""}
{"Commit title": "Fix crash when delete RSS feed", "Commit body": "@@ -137,6 +137,7 @@ void FeedListWidget::handleItemPathChanged(RSS::Item *rssItem)\n\nvoidFeedListWidget::handleItemAboutToBeRemoved(RSS::Item *rssItem)\n{\nrssItem->disconnect(this);\ndeletem_rssToTreeItemMapping.take(rssItem);\n\n//RSS Item is still valid in this slot so if it is the last\n"}
{"Commit title": "Raise the max log entries limit.", "Commit body": "@@ -6,7 +6,7 @@\n#include<QReadWriteLock>\n#include<QObject>\n\nconstintMAX_LOG_MESSAGES =1000;\nconstintMAX_LOG_MESSAGES =20000;\n\nnamespaceLog\n{\n"}
{"Commit title": "Standardize web seed naming in GUI", "Commit body": "@@ -531,11 +531,11 @@ void PropertiesWidget::loadUrlSeeds()\nreturn;\n\nm_ui->listWebSeeds->clear();\nqDebug(\"LoadingURLseeds\");\nqDebug(\"Loadingwebseeds\");\n//Add url seeds\nfor(constQUrl &urlSeed : urlSeeds)\n{\nqDebug(\"LoadingURLseed: %s\",qUtf8Printable(urlSeed.toString()));\nqDebug(\"Loadingwebseed: %s\",qUtf8Printable(urlSeed.toString()));\nnewQListWidgetItem(urlSeed.toString(), m_ui->listWebSeeds);\n}\n});\n@@ -550,16 +550,16 @@ void PropertiesWidget::displayWebSeedListMenu()\nQMenu *menu =newQMenu(this);\nmenu->setAttribute(Qt::WA_DeleteOnClose);\n\nmenu->addAction(UIThemeManager::instance()->getIcon(u\"list-add\"_s),tr(\"New Webseed\"),this, &PropertiesWidget::askWebSeed);\nmenu->addAction(UIThemeManager::instance()->getIcon(u\"list-add\"_s),tr(\"Add webseed...\"),this, &PropertiesWidget::askWebSeed);\n\nif(!rows.isEmpty())\n{\nmenu->addAction(UIThemeManager::instance()->getIcon(u\"edit-clear\"_s,u\"list-remove\"_s),tr(\"RemoveWebseed\")\nmenu->addAction(UIThemeManager::instance()->getIcon(u\"edit-clear\"_s,u\"list-remove\"_s),tr(\"Removewebseed\")\n,this, &PropertiesWidget::deleteSelectedUrlSeeds);\nmenu->addSeparator();\nmenu->addAction(UIThemeManager::instance()->getIcon(u\"edit-copy\"_s),tr(\"CopyWebseed URL\")\nmenu->addAction(UIThemeManager::instance()->getIcon(u\"edit-copy\"_s),tr(\"Copywebseed URL\")\n,this, &PropertiesWidget::copySelectedWebSeedsToClipboard);\nmenu->addAction(UIThemeManager::instance()->getIcon(u\"edit-rename\"_s),tr(\"EditWebseed URL\")\nmenu->addAction(UIThemeManager::instance()->getIcon(u\"edit-rename\"_s),tr(\"Editwebseed URL...\")\n,this, &PropertiesWidget::editWebSeed);\n}\n\n@@ -607,14 +607,14 @@ void PropertiesWidget::askWebSeed()\n{\nboolok =false;\n//Ask user for a new url seed\nconstQString urlSeed =AutoExpandableDialog::getText(this,tr(\"New URLseed\",\"NewHTTP source\"),\ntr(\"New URLseed:\"), QLineEdit::Normal,\nconstQString urlSeed =AutoExpandableDialog::getText(this,tr(\"Add webseed\",\"AddHTTP source\"),\ntr(\"Add webseed:\"), QLineEdit::Normal,\nu\"http://www.\"_s, &ok);\nif(!ok)return;\nqDebug(\"Adding %s web seed\",qUtf8Printable(urlSeed));\nif(!m_ui->listWebSeeds->findItems(urlSeed, Qt::MatchFixedString).empty())\n{\nQMessageBox::warning(this,u\"qBittorrent\"_s,tr(\"ThisURLseed is already in the list.\"), QMessageBox::Ok);\nQMessageBox::warning(this,u\"qBittorrent\"_s,tr(\"Thiswebseed is already in the list.\"), QMessageBox::Ok);\nreturn;\n}\nif(m_torrent)\n@@ -667,7 +667,7 @@ void PropertiesWidget::editWebSeed()\nif(!m_ui->listWebSeeds->findItems(newSeed, Qt::MatchFixedString).empty())\n{\nQMessageBox::warning(this,u\"qBittorrent\"_s,\ntr(\"ThisURLseed is already in the list.\"),\ntr(\"Thiswebseed is already in the list.\"),\nQMessageBox::Ok);\nreturn;\n}\n"}
{"Commit title": "Search helpers: Added parse_date helper", "Commit body": "@@ -27,7 +27,6 @@\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\nimportdatetime\nimportgzip\nimporthtml.entities\nimportio\n@@ -40,6 +39,7 @@\nimporturllib.error\nimporturllib.request\nfromcollections.abcimportMapping\nfromdatetimeimportdate,datetime,timedelta\nfromtypingimportAny,Optional\n\n\n@@ -50,10 +50,10 @@ def getBrowserUserAgent() -> str:\n# https://whattrainisitnow.com/calendar/\n# https://wiki.mozilla.org/index.php?title=Release_Management/Calendar&redirect=no\n\nbaseDate=datetime.date(2024,4,16)\nbaseDate=date(2024,4,16)\nbaseVersion=125\n\nnowDate=datetime.date.today()\nnowDate=date.today()\nnowVersion=baseVersion+((nowDate-baseDate).days//30)\n\nreturnf\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:{nowVersion}.0) Gecko/20100101 Firefox/{nowVersion}.0\"\n@@ -140,3 +140,50 @@ def download_file(url: str, referer: Optional[str] = None) -> str:\n\n# return file path\nreturnf\"{path}{url}\"\n\n\ndefparse_date(date_string:str,fallback_format:Optional[str]=None)->int:\n\"\"\" Try to parse a date by detecting unambiguous formats or falling back to provided format \"\"\"\n\ndate_string=re.sub(r\"\\s+\",\" \",date_string).strip().lower()\ntoday=datetime.now().replace(hour=0,minute=0,second=0,microsecond=0)\nmonths=[\"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\"]\ndate_parsers={\nr\"today\":lambdam:today,\nr\"yesterday\":lambdam:today-timedelta(days=1),\nr\"last week\":lambdam:today-timedelta(days=7),\nr\"last month\":lambdam:today-timedelta(days=30),\nr\"last year\":lambdam:today-timedelta(days=365),\nr\"(\\d+) (day)s?\":lambdam:today-timedelta(days=int(m[1])),\nr\"(\\d+) (week)s?\":lambdam:today-timedelta(days=int(m[1])*7),\nr\"(\\d+) (mo|month)s?\":lambdam:today-timedelta(days=int(m[1])*30),\nr\"(\\d+) (yr|year)s?\":lambdam:today-timedelta(days=int(m[1])*365),\nr\"(\\d\\d\\d\\d)[-/.](\\d\\d)[-/.](\\d\\d) (\\d\\d):(\\d\\d)\":lambdam:datetime(\nint(m[1]),int(m[2]),int(m[3]),int(m[4]),int(m[5])\n),\nr\"(\\d\\d\\d\\d)[-/.](\\d\\d)[-/.](\\d\\d)\":lambdam:datetime(\nint(m[1]),int(m[2]),int(m[3])\n),\nrf\"(\\d+) ({'|'.join(months)})\\w*,? (\\d\\d\\d\\d)\":lambdam:datetime(\nint(m[3]),int(months.index(m[2])+1),int(m[1])\n),\nrf\"({'|'.join(months)})\\w* (\\d+),? (\\d\\d\\d\\d)\":lambdam:datetime(\nint(m[3]),int(months.index(m[1])+1),int(m[2])\n),\n}\n\nforpattern,calcindate_parsers.items():\ntry:\nm=re.match(pattern,date_string)\nifm:\nreturnint(calc(m).timestamp())\nexceptException:\npass\n\niffallback_formatisnotNone:\ntry:\nreturnint(datetime.strptime(date_string,fallback_format).timestamp())\nexceptException:\npass\n\nreturn-1"}
{"Commit title": "no fixed length for combobox", "Commit body": "@@ -199,7 +199,6 @@ MainWindow::MainWindow(IGUIApplication *app, WindowState initialState)\nm_filterBy =newQComboBox(this);\nm_filterBy->addItem(tr(\"Torrent name\"), TransferListModel::Column::TR_NAME);\nm_filterBy->addItem(tr(\"Torrent save path\"), TransferListModel::Column::TR_SAVE_PATH);\nm_filterBy->setFixedWidth(200);\nm_ui->toolBar->insertWidget(m_ui->actionLock, m_filterBy);\n\nQWidget *spacer =newQWidget(this);\n"}
{"Commit title": "Update dutch.nsi", "Commit body": "@@ -19,9 +19,9 @@ LangString inst_pathlimit ${LANG_DUTCH} \"De Windows-begrenzing voor padlengte ui\n;LangString inst_firewallinfo ${LANG_ENGLISH} \"Adding Windows Firewall rule\"\nLangStringinst_firewallinfo${LANG_DUTCH}\"Uitzondering aan Windows Firewall toevoegen\"\n;LangString inst_warning ${LANG_ENGLISH} \"qBittorrent is running. Please close the application before installing.\"\nLangStringinst_warning${LANG_DUTCH}\"qBittorrent is actief. Sluitde toepassingvoordat u deze update installeert.\"\nLangStringinst_warning${LANG_DUTCH}\"qBittorrent is actief. Sluithet afvoordat u deze update installeert.\"\n;LangString inst_uninstall_question ${LANG_ENGLISH} \"Current version will be uninstalled. User settings and torrents will remain intact.\"\nLangStringinst_uninstall_question${LANG_DUTCH}\"Er werd een eerdere installatie gedetecteerd. Deze wordt verwijderd zonder de gebruikersinstellingen te verwijderen.\"\nLangStringinst_uninstall_question${LANG_DUTCH}\"De huidige versie zal verwijderd worden. Gebruikersinstellingen en torrents blijven intact.\"\n;LangString inst_unist ${LANG_ENGLISH} \"Uninstalling previous version.\"\nLangStringinst_unist${LANG_DUTCH}\"Vorige versie verwijderen.\"\n;LangString launch_qbt ${LANG_ENGLISH} \"Launch qBittorrent.\"\n@@ -52,7 +52,7 @@ LangString remove_firewallinfo ${LANG_DUTCH} \"Uitzondering uit Windows Firewall\n;LangString remove_cache ${LANG_ENGLISH} \"Remove torrents and cached data\"\nLangStringremove_cache${LANG_DUTCH}\"Torrents en cachegegevens verwijderen\"\n;LangString uninst_warning ${LANG_ENGLISH} \"qBittorrent is running. Please close the application before uninstalling.\"\nLangStringuninst_warning${LANG_DUTCH}\"qBittorrent is actief. Sluitde toepassingvoordat uzeverwijdert.\"\nLangStringuninst_warning${LANG_DUTCH}\"qBittorrent is actief. Sluithet afvoordat uhetverwijdert.\"\n;LangString uninst_tor_warn ${LANG_ENGLISH} \"Not removing .torrent association. It is associated with:\"\nLangStringuninst_tor_warn${LANG_DUTCH}\".torrent-associatie wordt niet verwijderd. Ze is gekoppeld met:\"\n;LangString uninst_mag_warn ${LANG_ENGLISH} \"Not removing magnet association. It is associated with:\"\n"}
{"Commit title": "Ignore extreme speed values in speedplotview", "Commit body": "@@ -28,6 +28,8 @@\n\n#include\"speedplotview.h\"\n\n#include<cmath>\n\n#include<QLocale>\n#include<QPainter>\n#include<QPen>\n@@ -130,15 +132,22 @@ void SpeedPlotView::Averager::push(const PointData &pointData)\n//Using int32 for accumulator we get overflow when transfer speed reaches 2^31/144 ~~ 14.2 MBytes/s.\n//With quint64 this speed limit is 2^64/144 ~~ 114 PBytes/s.\n//This speed is inaccessible to an ordinary user.\nconstquint64 extremeValue =4*std::pow(1024ull,4);/// 4 TebiBytes\nm_accumulator.x+= pointData.x;\nfor(intid = UP; id < NB_GRAPHS; ++id)\nm_accumulator.y[id] += pointData.y[id];\nm_counter = (m_counter +1) % m_divider;\nif(m_counter !=0)\nreturn;//still accumulating\n//it is time final averaging calculations\nfor(intid = UP; id < NB_GRAPHS; ++id)\nfor(intid = UP; id < NB_GRAPHS; ++id){\nm_accumulator.y[id] /= m_divider;\nif(m_accumulator.y[id] >= extremeValue) {\nm_accumulator = {};\nreturn;\n}\n}\n\nm_accumulator.x/= m_divider;\n//now flush out averaged data\nm_sink.push_back(m_accumulator);\n"}
{"Commit title": "Fix IPv6 endianness", "Commit body": "@@ -107,8 +107,8 @@ namespace\ncaseQAbstractSocket::IPv6Protocol: {\nconstQ_IPV6ADDR ipv6 = addr.toIPv6Address();\nQByteArray ret;\nfor(inti = (sizeof(ipv6.c) -1); i >=0; --i)\nret.append(static_cast<char>(ipv6.c[i]));\nfor(constquint8 i :ipv6.c)\nret.append(i);\nreturnret;\n}\n\n"}
{"Commit title": "Increase WebUI window heights", "Commit body": "@@ -153,7 +153,7 @@ const initializeWindows = function() {\npaddingVertical:0,\npaddingHorizontal:0,\nwidth:loadWindowWidth(id,500),\nheight:loadWindowHeight(id,420),\nheight:loadWindowHeight(id,600),\nonResize:function(){\nsaveWindowSize(id);\n}\n@@ -179,7 +179,7 @@ const initializeWindows = function() {\npaddingVertical:0,\npaddingHorizontal:0,\nwidth:loadWindowWidth(id,700),\nheight:loadWindowHeight(id,500),\nheight:loadWindowHeight(id,600),\nonResize:function(){\nsaveWindowSize(id);\n}\n@@ -200,7 +200,7 @@ const initializeWindows = function() {\npaddingVertical:0,\npaddingHorizontal:0,\nwidth:loadWindowWidth(id,500),\nheight:loadWindowHeight(id,260),\nheight:loadWindowHeight(id,460),\nonResize:function(){\nsaveWindowSize(id);\n}\n@@ -928,7 +928,7 @@ const initializeWindows = function() {\ntoolbarURL:'aboutToolbar.html',\npadding:10,\nwidth:loadWindowWidth(id,550),\nheight:loadWindowHeight(id,290),\nheight:loadWindowHeight(id,360),\nonResize:function(){\nsaveWindowSize(id);\n}\n"}
{"Commit title": "Change number of time axis divisions from 5 to 6 for convenience", "Commit body": "@@ -327,11 +327,11 @@ void SpeedPlotView::paintEvent(QPaintEvent *)\npainter.drawLine(fullRect.left(), rect.top() +0.75* rect.height(), rect.right(), rect.top() +0.75* rect.height());\npainter.drawLine(fullRect.left(), rect.bottom(), rect.right(), rect.bottom());\n\npainter.drawLine(rect.left(), fullRect.top(), rect.left(), fullRect.bottom());\npainter.drawLine(rect.left() +0.2* rect.width(), fullRect.top(), rect.left() +0.2* rect.width(), fullRect.bottom());\npainter.drawLine(rect.left() +0.4* rect.width(), fullRect.top(),rect.left() +0.4* rect.width(), fullRect.bottom());\npainter.drawLine(rect.left() +0.6* rect.width(), fullRect.top(),rect.left() +0.6* rect.width(), fullRect.bottom());\npainter.drawLine(rect.left() +0.8* rect.width(), fullRect.top(), rect.left() +0.8* rect.width(), fullRect.bottom());\nconstintTIME_AXIS_DIVISIONS =6;\nfor(inti =0; i < TIME_AXIS_DIVISIONS; ++i) {\nconstintx =rect.left() +(i* rect.width()) / TIME_AXIS_DIVISIONS;\npainter.drawLine(x, fullRect.top(),x, fullRect.bottom());\n}\n\n//Set antialiasing for graphs\npainter.setRenderHints(QPainter::Antialiasing | QPainter::HighQualityAntialiasing);\n"}
{"Commit title": "Fix crash when delete RSS feed", "Commit body": "@@ -137,6 +137,7 @@ void FeedListWidget::handleItemPathChanged(RSS::Item *rssItem)\n\nvoidFeedListWidget::handleItemAboutToBeRemoved(RSS::Item *rssItem)\n{\nrssItem->disconnect(this);\ndeletem_rssToTreeItemMapping.take(rssItem);\n\n//RSS Item is still valid in this slot so if it is the last\n"}
{"Commit title": "Add tooltip to episode filter text edit", "Commit body": "@@ -73,6 +73,16 @@ AutomatedRssDownloader::AutomatedRssDownloader(const QWeakPointer<RssManager>& m\nQt::CaseInsensitive),\nui->lineEFilter);\nui->lineEFilter->setValidator(m_episodeValidator);\nQString tip =\"<p>\"+tr(\"Matches articles based on episode filter.\") +\"</p><p><b>\"+tr(\"Example:\") +\n\"1x2;8-15;5;30-;</b>\"+tr(\"will match 2, 5, 8 through 15, 30 and onward episodes of season one\",\"example X will match\") +\"</p>\";\ntip +=\"<p>\"+tr(\"Episode filter rules:\") +\"</p><ul><li>\"+tr(\"Season number is a mandatory non-zero value\") +\"</li>\"+\n\"<li>\"+tr(\"Episode number is a mandatory non-zero value\") +\"</li>\"+\n\"<li>\"+tr(\"Filter must end with semicolon\") +\"</li>\"+\n\"<li>\"+tr(\"Three range types for episodes are supported:\") +\"</li>\"+\"<li><ul>\"\n\"<li>\"+tr(\"Single number: <b>1x25;</b> matches episode 25 of season one\") +\"</li>\"+\n\"<li>\"+tr(\"Normal range: <b>1x25-40;</b> matches episodes 25 through 40 of season one\") +\"</li>\"+\n\"<li>\"+tr(\"Infinite range: <b>1x25-;</b> matches 40 and onward episodes of season one\") +\"</li>\"+\"</ul></li></ul>\";\nui->lineEFilter->setToolTip(tip);\ninitLabelCombobox();\nloadFeedList();\nloadSettings();\n"}
{"Commit title": "Allow/Fix special characters in json replies in WebUI", "Commit body": "@@ -76,9 +76,7 @@ QString json::toJson(const QVariant& v) {\nresult +=\"\\\\t\";\nbreak;\ncase'\\\"':\ncase'\\'':\ncase'\\\\':\ncase'&':\nresult +='\\\\';\ncase'\\0':\ndefault:\n"}
{"Commit title": "Add integration tests with include_remote flag", "Commit body": "@@ -70,17 +70,53 @@ TEST_F(UsersTest, test_sanity) {\nrow_map.emplace(\"uuid\", NormalType);\n}\n\n//\n//The returned user might be \"root\" or a test user created as\n//part of the Github CI action (see .github/workflows/hosted_runners.yml\n//and .github/workflows/self_hosted_runners.yml).\n//\n\n//select * case\nautoconstrows =execute_query(\"select * from users\");\nASSERT_GE(rows.size(),1ul);\nvalidate_rows(rows, row_map);\n\n//select with a specified uid\nautotest_uid = rows.front().at(\"uid\");\nautotest_username = rows.front().at(\"username\");\n\n//select with a specified uid\nautoconstrows_one =\nexecute_query(std::string(\"select * from users where uid=\") + test_uid);\nASSERT_GE(rows_one.size(),1ul);\nvalidate_rows(rows_one, row_map);\n\n//select with a specified username\nautoconstrows_one =\nexecute_query(std::string(\"select * from users where username='\") +\ntest_username +\"'\");\nASSERT_GE(rows_one.size(),1ul);\nvalidate_rows(rows_one, row_map);\n\n//select with include_remote flag set\nautoconstrows_one =\nexecute_query(std::string(\"select * from users where include_remote=1\"));\nASSERT_GE(rows.size(),1ul);\nvalidate_rows(rows, row_map);\n\n//select with a specified uid and include_remote flag set\nautotest_uid = rows.front().at(\"uid\");\nautoconstrows_one =execute_query(\nstd::string(\"select * from users where include_remote=1 and uid=\") +\ntest_uid);\nASSERT_GE(rows_one.size(),1ul);\nvalidate_rows(rows_one, row_map);\n\n//select with a specified username and include_remote flag set\nautoconstrows_one =execute_query(\nstd::string(\"select * from users where include_remote=1 and username='\") +\ntest_username +\"'\");\nASSERT_GE(rows_one.size(),1ul);\nvalidate_rows(rows_one, row_map);\n}\n\n}//namespace table_tests\n"}
{"Commit title": "Update watcher test", "Commit body": "@@ -8,6 +8,7 @@\n*/\n\n#include<gmock/gmock.h>\n#include<gmock/gmock-matchers.h>\n#include<gtest/gtest.h>\n\n#include<osquery/config/config.h>\n@@ -326,7 +327,7 @@ TEST_F(WatcherTests, test_watcherrunner_watcherhealth) {\n\nautostatus = runner.isWatcherHealthy(*test_process, state);\nEXPECT_FALSE(status.ok());\nEXPECT_EQ(status.getMessage(),\"Memory limits exceeded\");\nEXPECT_THAT(status.getMessage(),::testing::HasSubstr(\"Memory limits exceeded\"));\n\n//Now emulate a rapid increase in CPU requirements.\nr[\"user_time\"] =INTEGER(1024*1024*1024);\n"}
{"Commit title": "Remove unnecessary fields in distributed metrics", "Commit body": "@@ -130,12 +130,6 @@ Status Distributed::serializeResults(std::string& json) {\nobj.AddMember(\"last_executed\",\nstatic_cast<uint64_t>(perf.last_executed),\nobj.GetAllocator());\nobj.AddMember(\"output_size\",\nstatic_cast<uint64_t>(perf.output_size),\nobj.GetAllocator());\nobj.AddMember(\"wall_time\",\nstatic_cast<uint64_t>(perf.wall_time),\nobj.GetAllocator());\nobj.AddMember(\"wall_time_ms\",\nstatic_cast<uint64_t>(perf.wall_time_ms),\nobj.GetAllocator());\n@@ -439,7 +433,6 @@ void Distributed::recordQueryPerformance(const std::string& name,\nperformance_[name] =QueryPerformance();\n}\n\n//Grab access to the non-const schedule item.\nauto& query = performance_.at(name);\nif(!r1.at(\"user_time\").empty() && !r0.at(\"user_time\").empty()) {\nautout1 = tryTo<longlong>(r1.at(\"user_time\"));\n"}
{"Commit title": "Improved connection time for broken servers. (#7424)", "Commit body": "@@ -391,8 +391,14 @@ WmiRequest::WmiRequest(const std::string& query, std::wstring nspace) {\nreturn;\n}\n\nhr = locator_->ConnectServer(\nnspace_str,nullptr,nullptr,nullptr,0,nullptr,nullptr, &services);\nhr = locator_->ConnectServer(nspace_str,\nnullptr,\nnullptr,\nnullptr,\nWBEM_FLAG_CONNECT_USE_MAX_WAIT,\nnullptr,\nnullptr,\n&services);\nSysFreeString(nspace_str);\n\nif(hr != S_OK) {\n"}
{"Commit title": "Improving listDirectoriesInDirectory by using STD fs::recursive_direc…", "Commit body": "@@ -84,11 +84,36 @@ void genPackage(const std::string& path, Row& r, Logger& logger) {\n}\n}\n\nStatusstdListDirectoriesInDirectory(constfs::path& path,\nstd::vector<std::string>& results,\nboolrecursive) {\nif(path.empty() || !pathExists(path)) {\nreturnStatus(1,\"Target directory is invalid\");\n}\n\nif(recursive) {\nfor(constauto& entry :fs::recursive_directory_iterator(path)) {\nif(fs::is_directory(entry)) {\nresults.push_back(entry.path().string());\n}\n}\n}else{\nfor(constauto& entry :fs::directory_iterator(path)) {\nif(fs::is_directory(entry)) {\nresults.push_back(entry.path().string());\n}\n}\n}\n\nreturnStatus::success();\n}\n\nvoidgenSiteDirectories(conststd::string& site,\nQueryData& results,\nLogger& logger) {\nstd::vector<std::string> directories;\nif(!listDirectoriesInDirectory(site, directories,true).ok()) {\n\nif(!stdListDirectoriesInDirectory(site, directories,true).ok()) {\nreturn;\n}\n\n"}
{"Commit title": "Use total_size within watchdog on Windows", "Commit body": "@@ -33,6 +33,7 @@\n#include<osquery/sql/sql.h>\n\n#include<osquery/utils/conversions/tryto.h>\n#include<osquery/utils/info/platform_type.h>\n#include<osquery/utils/info/tool_type.h>\n#include<osquery/utils/system/time.h>\n\n@@ -410,7 +411,11 @@ PerformanceChange getChange(const Row& r, PerformanceState& state) {\nstatic_cast<pid_t>(tryTo<longlong>(r.at(\"parent\")).takeOr(0LL));\nuser_time = tryTo<longlong>(r.at(\"user_time\")).takeOr(0LL);\nsystem_time = tryTo<longlong>(r.at(\"system_time\")).takeOr(0LL);\nchange.footprint= tryTo<longlong>(r.at(\"resident_size\")).takeOr(0LL);\nif(isPlatform(PlatformType::TYPE_WINDOWS)) {\nchange.footprint= tryTo<longlong>(r.at(\"total_size\")).takeOr(0LL);\n}else{\nchange.footprint= tryTo<longlong>(r.at(\"resident_size\")).takeOr(0LL);\n}\n}catch(conststd::exception&/*e*/) {\nstate.sustained_latency=0;\n}\n@@ -501,7 +506,7 @@ QueryData WatcherRunner::getProcessRow(pid_t pid) const {\np = (pid == ULONG_MAX) ? -1: pid;\n#endif\nreturnSQL::selectFrom(\n{\"parent\",\"user_time\",\"system_time\",\"resident_size\"},\n{\"parent\",\"user_time\",\"system_time\",\"resident_size\",\"total_size\"},\n\"processes\",\n\"pid\",\nEQUALS,\n"}
{"Commit title": "Users table:implement index on the UUID column", "Commit body": "@@ -65,6 +65,46 @@ std::string getUserHomeDir(const std::string& sid) {\nreturn\"\";\n}\n\n//Given a SID, retrieve information about the matching user\nvoidgenUser(conststd::string& sidString, QueryData& results) {\nRow r;\n\nr[\"uuid\"] = sidString;\nr[\"directory\"] =getUserHomeDir(sidString);\n\nPSID sid;\nautoret =ConvertStringSidToSidA(sidString.c_str(), &sid);\nif(ret ==0) {\nVLOG(1) <<\"Convert SID to string failed with\"<<GetLastError();\n}\nautouid =getUidFromSid(sid);\nautogid =getGidFromSid(sid);\nr[\"uid\"] =BIGINT(uid);\nr[\"gid\"] =BIGINT(gid);\nr[\"uid_signed\"] =INTEGER(uid);\nr[\"gid_signed\"] =INTEGER(gid);\nr[\"type\"] =kWellKnownSids.find(sidString) ==kWellKnownSids.end()\n?\"roaming\"\n:\"special\";\n\n//TODO\nr[\"shell\"] =\"C:\\\\Windows\\\\system32\\\\cmd.exe\";\nr[\"description\"] =\"\";\n\nwchar_taccntName[UNLEN] = {0};\nwchar_tdomName[DNLEN] = {0};\nunsignedlongaccntNameLen = UNLEN;\nunsignedlongdomNameLen = DNLEN;\nSID_NAME_USE eUse;\nret =LookupAccountSidW(\nnullptr, sid, accntName, &accntNameLen, domName, &domNameLen, &eUse);\nr[\"username\"] = ret !=0?wstringToString(accntName) :\"\";\n\nresults.push_back(r);\n}\n\n//Enumerate the users from the profiles key in the Registry, skipping any\n//that are given in processedSids (i.e., that have already been processed)\nvoidprocessRoamingProfiles(conststd::set<std::string>& processedSids,\nQueryData& results) {\nQueryData regResults;\n@@ -77,45 +117,15 @@ void processRoamingProfiles(const std::set<std::string>& processedSids,\n}\n\nautosidString = profile.at(\"name\");\nif(processedSids.find(sidString) != processedSids.end()) {\ncontinue;\n}\nr[\"uuid\"] = sidString;\nr[\"directory\"] =getUserHomeDir(sidString);\n\nPSID sid;\nautoret =ConvertStringSidToSidA(sidString.c_str(), &sid);\nif(ret ==0) {\nVLOG(1) <<\"Convert SID to string failed with\"<<GetLastError();\n//Skip this user if already processed\nif(processedSids.find(sidString) == processedSids.end()) {\ngenUser(sidString, results);\n}\nautouid =getUidFromSid(sid);\nautogid =getGidFromSid(sid);\nr[\"uid\"] =BIGINT(uid);\nr[\"gid\"] =BIGINT(gid);\nr[\"uid_signed\"] =INTEGER(uid);\nr[\"gid_signed\"] =INTEGER(gid);\nr[\"type\"] =kWellKnownSids.find(sidString) ==kWellKnownSids.end()\n?\"roaming\"\n:\"special\";\n\n//TODO\nr[\"shell\"] =\"C:\\\\Windows\\\\system32\\\\cmd.exe\";\nr[\"description\"] =\"\";\n\nwchar_taccntName[UNLEN] = {0};\nwchar_tdomName[DNLEN] = {0};\nunsignedlongaccntNameLen = UNLEN;\nunsignedlongdomNameLen = DNLEN;\nSID_NAME_USE eUse;\nret =LookupAccountSidW(\nnullptr, sid, accntName, &accntNameLen, domName, &domNameLen, &eUse);\nr[\"username\"] = ret !=0?wstringToString(accntName) :\"\";\nLocalFree(sid);\n\nresults.push_back(r);\n}\n}\n\n//Enumerate all local users\nvoidprocessLocalAccounts(std::set<std::string>& processedSids,\nQueryData& results) {\nunsignedlongdwUserInfoLevel =3;\n@@ -196,10 +206,19 @@ void processLocalAccounts(std::set<std::string>& processedSids,\n\nQueryDatagenUsers(QueryContext& context) {\nQueryData results;\nstd::set<std::string> processedSids;\n\nprocessLocalAccounts(processedSids, results);\nprocessRoamingProfiles(processedSids, results);\n//implement index on UUID (SID) column by\n//returning only the users in the constraint:\nif(context.constraints[\"uuid\"].exists(EQUALS)) {\nautosidStrings = context.constraints[\"uuid\"].getAll(EQUALS);\nfor(constauto& sidString : sidStrings) {\ngenUser(sidString, results);\n}\n}else{//return all users\nstd::set<std::string> processedSids;\nprocessLocalAccounts(processedSids, results);\nprocessRoamingProfiles(processedSids, results);\n}\n\nreturnresults;\n}\n"}
{"Commit title": "WIP: HACKY way to test ev2 based udev table", "Commit body": "@@ -52,6 +52,11 @@\n#include\"osquery/core/process.h\"\n#include\"osquery/core/watcher.h\"\n\n#include<osquery/ev2/manager.h>\n#include<osquery/evgen/linux/udev.h>\n#include<osquery/tables/system/linux/udev.h>\n\n\n#ifdef__linux__\n#include<sys/syscall.h>\n\n@@ -696,6 +701,16 @@ void Initializer::start() const {\nFLAGS_numeric_monitoring_plugins);\n}\n\n//Hack\nautoem = std::make_shared<ev2::EventManager>();\nautoudevpub = std::make_shared<evgen::UdevPublisher>();\nem->register_publisher(udevpub);\n\nautoudevtable = std::make_shared<UdevTable>(em);\n\nDispatcher::addService(udevpub);\nDispatcher::addService(udevtable);\n\n//Start event threads.\nosquery::attachEvents();\nEventFactory::delay();\n"}
{"Commit title": "Update the handling of invalid array bound error on event query", "Commit body": "@@ -33,8 +33,6 @@ namespace tables {\nconststd::stringkEventLogXmlPrefix=\"<QueryList><Query Id=\\\"0\\\">\";\nconststd::stringkEventLogXmlSuffix=\"</Query></QueryList>\";\n\nconstintkNumEventsBlock=1024;\n\nStatusparseWelXml(QueryContext& context, std::wstring& xml_event, Row& row) {\npt::ptree propTree;\nWELEvent windows_event;\n@@ -87,61 +85,91 @@ Status parseWelXml(QueryContext& context, std::wstring& xml_event, Row& row) {\nvoidrenderQueryResults(QueryContext& context,\nEVT_HANDLE queryResults,\nRowYield& yield) {\nstd::vector<EVT_HANDLE>events(kNumEventsBlock);\nunsignedlongnumEvents =0;\n\n//Retrieve the events one block at a time\nautoret =EvtNext(\nqueryResults,kNumEventsBlock, events.data(), INFINITE,0, &numEvents);\nwhile(ret !=FALSE) {\nfor(unsignedlongi =0; i < numEvents; i++) {\nunsignedlongrenderedBuffSize =0;\nunsignedlongrenderedBuffUsed =0;\nunsignedlongpropCount =0;\nif(!EvtRender(nullptr,\nevents[i],\nEvtRenderEventXml,\nrenderedBuffSize,\nnullptr,\n&renderedBuffUsed,\n&propCount)) {\nif(GetLastError() != ERROR_INSUFFICIENT_BUFFER) {\nLOG(WARNING) <<\"Failed to get the size of rendered event\"\nuint32_tkNumEventsBlock=1024;\nuint32_t_position =0;\n\n//The batch size should be more than 32. It is not documented\n//but `EvtNext` should not fail (RPC_S_INVALID_BOUND error)\n//with low batch size.\nwhile(kNumEventsBlock>32) {\nstd::vector<EVT_HANDLE>events(kNumEventsBlock);\nunsignedlongnumEvents =0;\n//Retrieve the events one block at a time\nautoret =EvtNext(\nqueryResults,kNumEventsBlock, events.data(), INFINITE,0, &numEvents);\nwhile(ret !=FALSE) {\nfor(unsignedlongi =0; i < numEvents; i++) {\nunsignedlongrenderedBuffSize =0;\nunsignedlongrenderedBuffUsed =0;\nunsignedlongpropCount =0;\n_position +=1;\nif(!EvtRender(nullptr,\nevents[i],\nEvtRenderEventXml,\nrenderedBuffSize,\nnullptr,\n&renderedBuffUsed,\n&propCount)) {\nif(GetLastError() != ERROR_INSUFFICIENT_BUFFER) {\nLOG(WARNING) <<\"Failed to get the size of rendered event\"\n<<GetLastError();\nEvtClose(events[i]);\ncontinue;\n}\n}\n\nstd::vector<wchar_t>renderedContent(renderedBuffUsed);\nrenderedBuffSize = renderedBuffUsed;\nif(!EvtRender(nullptr,\nevents[i],\nEvtRenderEventXml,\nrenderedBuffSize,\nrenderedContent.data(),\n&renderedBuffUsed,\n&propCount)) {\nLOG(WARNING) <<\"Failed to render windows event with\"\n<<GetLastError();\nEvtClose(events[i]);\ncontinue;\n}\n}\n\nstd::vector<wchar_t>renderedContent(renderedBuffUsed);\nrenderedBuffSize = renderedBuffUsed;\nif(!EvtRender(nullptr,\nevents[i],\nEvtRenderEventXml,\nrenderedBuffSize,\nrenderedContent.data(),\n&renderedBuffUsed,\n&propCount)) {\nLOG(WARNING) <<\"Failed to render windows event with\"\n<<GetLastError();\nEvtClose(events[i]);\ncontinue;\n\nRow row;\nstd::wstringstream xml_event;\nxml_event << renderedContent.data();\nautostatus =parseWelXml(context, xml_event.str(), row);\nif(status.ok()) {\nyield(TableRowHolder(newDynamicTableRow(std::move(row))));\n}\n}\n\nEvtClose(events[i]);\nret =EvtNext(queryResults,\nkNumEventsBlock,\nevents.data(),\nINFINITE,\n0,\n&numEvents);\n}\n\nRow row;\nstd::wstringstream xml_event;\nxml_event << renderedContent.data();\nautostatus =parseWelXml(context, xml_event.str(), row);\nif(status.ok()) {\nyield(TableRowHolder(newDynamicTableRow(std::move(row))));\n//While reading a batch of large event log reports `EvtNext` may\n//fail with error code 1734 (RPC_S_INVALID_BOUND) and loose the\n//chunk of events. This is an unusual behavior and not documented.\n//The fix reduces the batch size to half and retries `EvtNext`\nif(RPC_S_INVALID_BOUND ==GetLastError()) {\nkNumEventsBlock=kNumEventsBlock/2;\n//`EvtNext` may update the event position in query handler on\n//failure with RPC_S_INVALID_BOUND error. `EvtSeek` reset the\n//position before calling EvtNext with lower batch size.\nif(!EvtSeek(\nqueryResults, _position,nullptr,0, EvtSeekRelativeToFirst)) {\nVLOG(1) <<\"EvtSeek failed with error\"<<GetLastError();\n}\ncontinue;\n}\n\nret =EvtNext(\nqueryResults,kNumEventsBlock, events.data(), INFINITE,0, &numEvents);\nbreak;\n}\n\nif(ERROR_NO_MORE_ITEMS !=GetLastError()) {\n//No need to close the handler after error; The query\n//EvtClose will also close all the event handler\n"}
{"Commit title": "Remove /dev/null monitoring from fsevents", "Commit body": "@@ -69,7 +69,7 @@ void FSEventsEventPublisher::restart() {\n\nif(paths_.empty()) {\n//There are no paths to watch.\npaths_.insert(\"/dev/null\");\nreturn;\n}\n\nstd::vector<CFStringRef> cf_paths;\n"}
{"Commit title": "Change the watcher limits to default=loose", "Commit body": "@@ -50,7 +50,7 @@ const std::string kExtensionExtension = \".ext\";\n\nCLI_FLAG(int32,\nwatchdog_level,\n1,\n0,\n\"Performance limit level (0=loose, 1=normal, 2=restrictive, 3=debug)\");\n\nCLI_FLAG(bool, disable_watchdog,false,\"Disable userland watchdog process\");\n"}
{"Commit title": "Adding verbosity to warnings and fixing regex replaces", "Commit body": "@@ -47,8 +47,10 @@ const std::string kServiceKeyPath =\n\"HKEY_LOCAL_MACHINE\\\\SYSTEM\\\\CurrentControlSet\\\\Services\\\\\";\n\nstaticinlinevoidwin32LogWARNING(conststd::string& msg,\nconstDWORD code = GetLastError()) {\nLOG(WARNING) << msg +\"Error code:\"+std::to_string(code);\nconstDWORD code = GetLastError(),\nconststd::string& deviceName = \"\") {\nLOG(WARNING) << msg <<\"for device\"<< deviceName\n<<\", error code:\"+std::to_string(code);\n}\n\ndevice_infoset_tsetupDevInfoSet(constDWORD flags) {\n@@ -157,10 +159,14 @@ std::string getDriverImagePath(const std::string& service_key) {\nreturn\"\";\n}\n\n//Unify the image path as systemRoot can contain systemroot and/or system32\nstd::transform(path.begin(), path.end(), path.begin(), ::tolower);\n\ncharsystemRoot[MAX_PATH] = {0};\nGetSystemDirectory(systemRoot, MAX_PATH);\n\nreturnsystemRoot +\nboost::regex_replace(path,boost::regex(\"^.*[Ss]ystem32\"),\"\");\nboost::regex_replace(path,boost::regex(\"^.*?system32\"),\"\");\n}\n\nQueryDatagenDrivers(QueryContext& context) {\n@@ -259,20 +265,21 @@ QueryData genDrivers(QueryContext& context) {\n&infLen);\n}\nif(sdiRet !=TRUE) {\nVLOG(1) <<\"Failed to derive full driver INF pathwith\"\n<<GetLastError();\nVLOG(1) <<\"Failed to derive full driver INF pathfor\"\n<<r[\"device_name\"] <<\"with\"<<GetLastError();\nr[\"inf\"] = infName;\n}else{\nr[\"inf\"] = inf.data();\n}\n\n//Add the remaining columns from the APIs\nif(apiDevices.find(devid) != apiDevices.end()) {\nr[\"service\"] = apiDevices[devid][\"service\"];\nr[\"service_key\"] = apiDevices[devid][\"service_key\"];\nr[\"image\"] = apiDevices[devid][\"image\"];\nr[\"driver_key\"] = apiDevices[devid][\"driver_key\"];\nr[\"date\"] = apiDevices[devid][\"date\"];\nautodev = apiDevices.find(devid);\nif(dev != apiDevices.end()) {\nr[\"service\"] = dev->second[\"service\"];\nr[\"service_key\"] = dev->second[\"service_key\"];\nr[\"image\"] = dev->second[\"image\"];\nr[\"driver_key\"] = dev->second[\"driver_key\"];\nr[\"date\"] = dev->second[\"date\"];\n}\n\nresults.push_back(r);\n"}
{"Commit title": "Change the watcher limits to default=loose", "Commit body": "@@ -50,7 +50,7 @@ const std::string kExtensionExtension = \".ext\";\n\nCLI_FLAG(int32,\nwatchdog_level,\n1,\n0,\n\"Performance limit level (0=loose, 1=normal, 2=restrictive, 3=debug)\");\n\nCLI_FLAG(bool, disable_watchdog,false,\"Disable userland watchdog process\");\n"}
{"Commit title": "fix lint", "Commit body": "@@ -44,7 +44,7 @@ MXNET_REGISTER_API(\"_npi.unravel_index\")\nattrs.parsed= param;\nattrs.op= op;\nSetAttrDict<op::RavelParam>(&attrs);\nNDArray* inputs[] = {args[0].operatormxnet::NDArray*()};\nNDArray* inputs[] = {args[0].operatormxnet::NDArray*()};\nintnum_inputs    =1;\nintnum_outputs   =0;\nautondoutputs    =Invoke(op, &attrs, num_inputs, inputs, &num_outputs,nullptr);\n"}
{"Commit title": "Use index_t instead of int in index_array kernels", "Commit body": "@@ -36,7 +36,7 @@ enum IndexArrayOpResource {kTempSpace};\n\ntemplate<intreq>\nstructIndexArrayKernel{\nMSHADOW_XINLINEstaticvoidMap(inti,\nMSHADOW_XINLINEstaticvoidMap(index_ti,\nint64_t* out_data,\nconstintn,\nconstint64_t* workspace) {\n@@ -50,7 +50,7 @@ struct IndexArrayKernel {\n\ntemplate<intreq>\nstructIndexArrayDefaultKernel{\nMSHADOW_XINLINEstaticvoidMap(inti,\nMSHADOW_XINLINEstaticvoidMap(index_ti,\nint64_t* out_data,\nconstintndim,\nconstdim_t* shape) {\n"}
{"Commit title": "fix for duplicate inputs", "Commit body": "@@ -536,27 +536,30 @@ void FindOutputEntries(nnvm::Graph* g,\nvoidCutGraphInputs(conststd::vector<nnvm::NodeEntry*> &input_entries,\nstd::vector<nnvm::NodeEntry> *orig_entries,\nconstboolskip_var =false) {\norig_entries->resize(input_entries.size());\n//map for creating unique var nodes for deduplicating entries from the same node\nstd::unordered_map<std::string,int> name_count_map;\nstd::unordered_map<std::string,nnvm::NodeEntry> name_count_map;\nfor(size_ti =0; i < input_entries.size(); ++i) {\nnnvm::NodeEntry *e = input_entries[i];\n//If the node is a variable itself, we may want to skip the node.\nif(e->node->is_variable() && skip_var) {\ncontinue;\n}\n\norig_entries->at(i) = *e;\nnnvm::Symbol sym;\nsym.outputs.push_back(*e);\nconstautooutput_names = sym.ListOutputNames();\nCHECK_EQ(output_names.size(),1U);\nconststd::string& var_name = output_names[0];\n\nautoit = name_count_map.find(var_name);\nif(name_count_map.end() == it) {\nname_count_map.emplace(var_name,0);\n}else{\n++(it->second);\n//if the node is not yet an input to the subgraph, create a node in the subgraph\nnnvm::NodePtr n =nnvm::CreateVariableNode(var_name);\nnnvm::NodeEntry e_ = nnvm::NodeEntry{n,0,0};\norig_entries->push_back(i) = *e;\n\n//store the node in the map\nname_count_map.emplace(var_name, e_);\n}\nnnvm::ObjectPtr n =nnvm::CreateVariableNode(\nvar_name +std::to_string(name_count_map[var_name]));\n@@ -565,7 +568,8 @@ void CutGraphInputs(const std::vector<nnvm::NodeEntry*> &input_entries,\nn->attrs.dict[\"isArg\"] =\"True\";\nelse\nn->attrs.dict[\"isArg\"] =\"False\";\n*e = nnvm::NodeEntry{n,0,0};\n//lookup the name of the node and set it as the input dependency\n*e = name_count_map[var_name];\n}\n}\n\n"}
{"Commit title": "fix alignment", "Commit body": "@@ -404,7 +404,7 @@ void TopKImpl(const RunContext &ctx,\nbooldo_transpose =false;\nboolis_ascend =false;\nindex_tk =0;\nsize_talignment =std::max(sizeof(DType),sizeof(int));\nsize_talignment =std::max(sizeof(DType),sizeof(index_t));\nmxnet::TShape target_shape;\nParseTopKParam(src.shape_, param,\n&target_shape, &batch_size, &element_num, &axis, &k, &do_transpose, &is_ascend);\n@@ -417,27 +417,27 @@ void TopKImpl(const RunContext &ctx,\nsize_ttemp_size =0;\n//Temp space needed by the gpu-based full sorts.\ntemp_size =std::max(temp_size,\nmxnet::op::SortByKeyWorkspaceSize<int,int, xpu>(src.Size()));\nmxnet::op::SortByKeyWorkspaceSize<index_t,index_t, xpu>(src.Size()));\ntemp_size =std::max(temp_size,\nmxnet::op::SortByKeyWorkspaceSize<int, DType, xpu>(src.Size()));\nmxnet::op::SortByKeyWorkspaceSize<index_t, DType, xpu>(src.Size()));\ntemp_size =std::max(temp_size,\nmxnet::op::SortByKeyWorkspaceSize<DType,int, xpu>(src.Size()));\nmxnet::op::SortByKeyWorkspaceSize<DType,index_t, xpu>(src.Size()));\n//Additional temp space for gpu full sorts for batch ids.\ntemp_size +=PadBytes(sizeof(index_t) * src.Size(), alignment);\n//Temp space for cpu sorts.\ntemp_size =std::max(temp_size,static_cast<size_t>(sizeof(DType) * src.Size()));\nsize_tworkspace_size = temp_size +PadBytes(sizeof(DType) * src.Size(), alignment)\n+PadBytes(sizeof(index_t) * src.Size(), alignment);\nif(param.ret_typ== topk_enum::kReturnMask) {\nworkspace_size +=PadBytes(sizeof(int) * batch_size * k, alignment);\nworkspace_size +=PadBytes(sizeof(index_t) * batch_size * k, alignment);\n}\nworkspace = resource.get_space_typed<xpu,1,char>(Shape1(workspace_size), s);\nchar* workspace_curr_ptr = workspace.dptr_;\nsorted_dat = Tensor<xpu,1, DType>(reinterpret_cast<DType*>(workspace_curr_ptr),\nShape1(src.Size()), s);//contain sorted dat\nShape1(src.Size()), s);//contain sorted dat\nworkspace_curr_ptr +=PadBytes(sizeof(DType) * src.Size(), alignment);\nindices = Tensor<xpu,1,index_t>(reinterpret_cast<index_t*>(workspace_curr_ptr),\nShape1(src.Size()), s);//indices in the original matrix\nShape1(src.Size()), s);//indices in the original matrix\nworkspace_curr_ptr +=PadBytes(sizeof(index_t) * src.Size(), alignment);\n\nif(param.ret_typ== topk_enum::kReturnMask) {\n"}
{"Commit title": "Effective multinomial", "Commit body": "@@ -122,25 +122,28 @@ inline bool SampleMultinomialOpType(const nnvm::NodeAttrs& attrs,\nstructSampleMultinomialKernel{\ntemplate<typenameDType,typenameIType>\nMSHADOW_XINLINEstaticvoidMap(inti,index_tK,index_tM,\nDType* dist,float* uniform, IType* out,\nDType* prob) {\nDType* dist,float* uniform,float* cum_table,\nIType* out, DType* prob) {\ncum_table[i*K] =0.0;\n//CDF table\nfor(index_tc =1; c < K +1; ++c) {\ncum_table[i*K + c] = cum_table[i*K + c -1] + dist[i*K + c -1];\n}\nfor(index_tj =0; j < M; ++j) {\nindex_tleft =0, right = K;\nindex_tmiddle = left + (right - left) /2;\nDType loc =static_cast<DType>(uniform[i*M + j]);\nDType acc =0;\nboolfound =false;\nfor(index_tk =0; k < K; ++k) {\nacc += dist[i*K + k];\nif(acc > loc) {\nfound =true;\nout[i*M + j] =static_cast<IType>(k);\nif(prob !=nullptr) prob[i*M + j] =logf(dist[i*K + k]);\nbreak;\nwhile(right - left >0) {\nmiddle = left + (right - left) /2;\nDType cum_prob = cum_table[i*K + middle];\nif(cum_prob < loc) {\nleft = middle +1;\n}else{\nright = middle;\n}\n}\nif(!found) {\nout[i*M + j] =static_cast<IType>(K-1);\nif(prob !=nullptr) prob[i*M + j] =logf(dist[i*K + K -1]);\n}\nout[i*M + j] =static_cast<IType>(middle);\nif(prob !=nullptr) prob[i*M + j] =logf(dist[i*K + middle -1]);\n}\n}\n};\n@@ -163,12 +166,14 @@ void SampleMultinomialForward(const nnvm::NodeAttrs& attrs,\nStream<xpu> *s = ctx.get_stream<xpu>();\nMSHADOW_REAL_TYPE_SWITCH(inputs[0].type_flag_, DType, {\nRandom<xpu,float> *prnd = ctx.requested[0].get_random<xpu,float>(s);\nTensor<xpu,1,float> uniform =\nctx.requested[1].get_space_typed<xpu,1,float>(Shape1(N*M), s);\nTensor<xpu,1,float> workspace =\nctx.requested[1].get_space_typed<xpu,1,float>(Shape1(N*M + N*(K +1)), s);\nTensor<xpu,1,float>uniform(workspace.dptr_,Shape1(N*M));\nprnd->SampleUniform(&uniform,0,1);\nMSHADOW_TYPE_SWITCH(outputs[0].type_flag_, IType, {\nKernel<SampleMultinomialKernel, xpu>::Launch(\ns, N, K, M, inputs[0].dptr<DType>(), uniform.dptr_, outputs[0].dptr<IType>(),\ns, N, K, M, inputs[0].dptr<DType>(), workspace.dptr_, workspace.dptr_+ N*M,\noutputs[0].dptr<IType>(),\nparam.get_prob? outputs[1].dptr<DType>() :nullptr);\n});\n});\n"}
{"Commit title": "Faster Scala NDArray to BufferedImage function", "Commit body": "@@ -174,16 +174,18 @@ object Image {\ndeftoImage(src:NDArray):BufferedImage={\nrequire(src.dtype==DType.UInt8,\"The input NDArray must be bytes\")\nrequire(src.shape.length==3,\"The input should contains height, width and channel\")\nrequire(src.shape(2)==3,\"There should be three channels: RGB\")\nvalheight=src.shape.get(0)\nvalwidth=src.shape.get(1)\nvalimg=newBufferedImage(width, height,BufferedImage.TYPE_INT_RGB)\nvalarr=src.toArray\n(0until height).par.foreach(r=>{\n(0until width).par.foreach(c=>{\nvalarr=src.at(r).at(c).toArray\n//NDArray in RGB\nvalred=arr(0).toByte &0xFF\nvalgreen=arr(1).toByte &0xFF\nvalblue=arr(2).toByte &0xFF\nvalcellIndex=r*width*3+c*3\nvalred=arr(cellIndex).toByte &0xFF\nvalgreen=arr(cellIndex+1).toByte &0xFF\nvalblue=arr(cellIndex+2).toByte &0xFF\nvalrgb=(red<<16)|(green<<8)|blue\nimg.setRGB(c, r, rgb)\n})\n"}
{"Commit title": "Add Issue number in comment", "Commit body": "@@ -713,7 +713,7 @@ def test_fully_connected():\ncheck_symbolic_forward(fc, {'data':data_np,'weight':fc_weight.asnumpy(),'bias':fc_bias_np}, {'fc_output':res})\ncheck_numeric_gradient(fc, {'data':data_np,'weight':fc_weight.asnumpy(),'bias':fc_bias_np},\nnumeric_eps=1e-2,rtol=1e-4,atol=1e-2)\n# TODO: Fix Bug when bias has ndim > 1\n# TODO: Fix Bug#15032when bias has ndim > 1\n#check_symbolic_forward(fc, {'data': data_np, 'weight': fc_weight.asnumpy(), 'bias': fc_bias2.asnumpy()}, {'fc_output': res})\n\n\n"}
{"Commit title": "upgrade cuDNN & NCCL", "Commit body": "@@ -31,14 +31,14 @@ if [[ $VARIANT == cu100* ]]; then\nCUDA_VERSION='10.0.130-1'\nCUDA_PATCH_VERSION='10.0.130-1'\nLIBCUDA_VERSION='410.48-0ubuntu1'\nLIBCUDNN_VERSION='7.3.1.20-1+cuda10.0'\nLIBNCCL_VERSION='2.3.4-1+cuda9.2'\nLIBCUDNN_VERSION='7.5.1.10-1+cuda10.0'\nLIBNCCL_VERSION='2.4.2-1+cuda10.0'\nelif[[$VARIANT==cu92*]];then\nCUDA_VERSION='9.2.148-1'\nCUDA_PATCH_VERSION='9.2.148.1-1'\nLIBCUDA_VERSION='396.44-0ubuntu1'\nLIBCUDNN_VERSION='7.3.1.20-1+cuda9.2'\nLIBNCCL_VERSION='2.3.4-1+cuda9.2'\nLIBCUDNN_VERSION='7.5.1.10-1+cuda9.2'\nLIBNCCL_VERSION='2.4.2-1+cuda9.2'\nelif[[$VARIANT==cu91*]];then\nCUDA_VERSION='9.1.85-1'\nCUDA_PATCH_VERSION='9.1.85.3-1'\n@@ -49,8 +49,8 @@ elif [[ $VARIANT == cu90* ]]; then\nCUDA_VERSION='9.0.176-1'\nCUDA_PATCH_VERSION='9.0.176.3-1'\nLIBCUDA_VERSION='384.145-0ubuntu1'\nLIBCUDNN_VERSION='7.3.1.20-1+cuda9.0'\nLIBNCCL_VERSION='2.3.4-1+cuda9.0'\nLIBCUDNN_VERSION='7.5.1.10-1+cuda9.0'\nLIBNCCL_VERSION='2.4.2-1+cuda9.0'\nelif[[$VARIANT==cu80*]];then\nCUDA_VERSION='8.0.61-1'\nCUDA_PATCH_VERSION='8.0.61.2-1'\n"}
{"Commit title": "downgrade tensorRT to 7.5", "Commit body": "@@ -35,7 +35,7 @@ ARG USER_ID=0\nCOPYinstall/ubuntu_adduser.sh /work/\nRUN/work/ubuntu_adduser.sh\n\nENVCUDNN_VERSION=7.6.0.64\nENVCUDNN_VERSION=7.5.0.56\nCOPYinstall/ubuntu_cudnn.sh /work/\nRUN/work/ubuntu_cudnn.sh\n\n"}
{"Commit title": "address the comment", "Commit body": "@@ -94,10 +94,10 @@ fi\nif[[$VARIANT==cu101*]];then\ncuda_files=( \\\n\"cuda-core-${CUDA_MAJOR_DASH}_${CUDA_VERSION}_amd64.deb\"\\\n\"cuda-cublas-${CUDA_MAJOR_DASH}_${CUDA_PATCH_VERSION}_amd64.deb\"\\\n\"cuda-cublas-dev-${CUDA_MAJOR_DASH}_${CUDA_PATCH_VERSION}_amd64.deb\"\\\n\"cuda-cudart-${CUDA_MAJOR_DASH}_${CUDA_PATCH_VERSION}_amd64.deb\"\\\n\"cuda-cudart-dev-${CUDA_MAJOR_DASH}_${CUDA_PATCH_VERSION}_amd64.deb\"\\\n\"libcublas10_${CUDA_PATCH_VERSION}_amd64.deb\"\\\n\"libcublas-dev_${CUDA_PATCH_VERSION}_amd64.deb\"\\\n\"cuda-cudart-${CUDA_MAJOR_DASH}_${CUDA_VERSION}_amd64.deb\"\\\n\"cuda-cudart-dev-${CUDA_MAJOR_DASH}_${CUDA_VERSION}_amd64.deb\"\\\n\"cuda-curand-${CUDA_MAJOR_DASH}_${CUDA_VERSION}_amd64.deb\"\\\n\"cuda-curand-dev-${CUDA_MAJOR_DASH}_${CUDA_VERSION}_amd64.deb\"\\\n\"cuda-cufft-${CUDA_MAJOR_DASH}_${CUDA_VERSION}_amd64.deb\"\\\n@@ -107,7 +107,7 @@ if [[ $VARIANT == cu101* ]]; then\n\"cuda-cusolver-${CUDA_MAJOR_DASH}_${CUDA_VERSION}_amd64.deb\"\\\n\"cuda-cusolver-dev-${CUDA_MAJOR_DASH}_${CUDA_VERSION}_amd64.deb\"\\\n\"cuda-misc-headers-${CUDA_MAJOR_DASH}_${CUDA_VERSION}_amd64.deb\"\\\n\"cuda-nvcc-${CUDA_MAJOR_DASH}_${CUDA_PATCH_VERSION}_amd64.deb\"\\\n\"cuda-nvcc-${CUDA_MAJOR_DASH}_${CUDA_VERSION}_amd64.deb\"\\\n\"libcuda1-${LIBCUDA_MAJOR}_${LIBCUDA_VERSION}_amd64.deb\"\\\n\"nvidia-${LIBCUDA_MAJOR}_${LIBCUDA_VERSION}_amd64.deb\"\\\n)\n"}
{"Commit title": "bump up cudnn to 7.5.1 & nccl 2.4.2", "Commit body": "@@ -31,14 +31,14 @@ if [[ $VARIANT == cu100* ]]; then\nCUDA_VERSION='10.0.130-1'\nCUDA_PATCH_VERSION='10.0.130-1'\nLIBCUDA_VERSION='410.48-0ubuntu1'\nLIBCUDNN_VERSION='7.3.1.20-1+cuda10.0'\nLIBNCCL_VERSION='2.3.4-1+cuda9.2'\nLIBCUDNN_VERSION='7.5.1.10-1+cuda10.0'\nLIBNCCL_VERSION='2.4.2-1+cuda10.0'\nelif[[$VARIANT==cu92*]];then\nCUDA_VERSION='9.2.148-1'\nCUDA_PATCH_VERSION='9.2.148.1-1'\nLIBCUDA_VERSION='396.44-0ubuntu1'\nLIBCUDNN_VERSION='7.3.1.20-1+cuda9.2'\nLIBNCCL_VERSION='2.3.4-1+cuda9.2'\nLIBCUDNN_VERSION='7.5.1.10-1+cuda9.2'\nLIBNCCL_VERSION='2.4.2-1+cuda9.2'\nelif[[$VARIANT==cu91*]];then\nCUDA_VERSION='9.1.85-1'\nCUDA_PATCH_VERSION='9.1.85.3-1'\n@@ -49,8 +49,8 @@ elif [[ $VARIANT == cu90* ]]; then\nCUDA_VERSION='9.0.176-1'\nCUDA_PATCH_VERSION='9.0.176.3-1'\nLIBCUDA_VERSION='384.145-0ubuntu1'\nLIBCUDNN_VERSION='7.3.1.20-1+cuda9.0'\nLIBNCCL_VERSION='2.3.4-1+cuda9.0'\nLIBCUDNN_VERSION='7.5.1.10-1+cuda9.0'\nLIBNCCL_VERSION='2.4.2-1+cuda9.0'\nelif[[$VARIANT==cu80*]];then\nCUDA_VERSION='8.0.61-1'\nCUDA_PATCH_VERSION='8.0.61.2-1'\n"}
{"Commit title": "fix compiler warning", "Commit body": "@@ -462,7 +462,7 @@ mkldnn::memory::primitive_desc GetPrimitiveDesc(mkldnn::memory::primitive_desc p\nmkldnn_memory_format_tformat);\n\ninlineboolsame_shape(constmxnet::TShape &shape,constmkldnn_dims_tdims,intndims) {\nif(shape.ndim() !=(size_t)ndims)\nif(shape.ndim() != ndims)\nreturnfalse;\nfor(inti =0; i < ndims; i++)\nif(shape[i] != dims[i])\n"}
{"Commit title": "fix compiler warning", "Commit body": "@@ -462,7 +462,7 @@ mkldnn::memory::primitive_desc GetPrimitiveDesc(mkldnn::memory::primitive_desc p\nmkldnn_memory_format_tformat);\n\ninlineboolsame_shape(constmxnet::TShape &shape,constmkldnn_dims_tdims,intndims) {\nif(shape.ndim() !=(size_t)ndims)\nif(shape.ndim() != ndims)\nreturnfalse;\nfor(inti =0; i < ndims; i++)\nif(shape[i] != dims[i])\n"}
{"Commit title": "skip adding nullptr into GPU memory pool", "Commit body": "@@ -155,6 +155,10 @@ void GPUPooledStorageManager::Alloc(Storage::Handle* handle) {\n}\n\nvoidGPUPooledStorageManager::Free(Storage::Handlehandle) {\n//Do nothing if dptr is nullptr. Otherwise, nullptr may be reused\n//which can cause illegal memory access error.\nif(handle.dptr==nullptr)return;\n\nstd::lock_guard<std::mutex>lock(Storage::Get()->GetMutex(Context::kGPU));\nsize_tsize =RoundAllocSize(handle.size);\nauto&& reuse_pool = memory_pool_[size];\n@@ -312,6 +316,10 @@ void GPUPooledRoundedStorageManager::Alloc(Storage::Handle* handle) {\n}\n\nvoidGPUPooledRoundedStorageManager::Free(Storage::Handlehandle) {\n//Do nothing if dptr is nullptr. Otherwise, nullptr may be reused\n//which can cause illegal memory access error.\nif(handle.dptr==nullptr)return;\n\nstd::lock_guard<std::mutex>lock(Storage::Get()->GetMutex(Context::kGPU));\nintbucket =get_bucket(handle.size);\nauto&& reuse_pool = memory_pool_[bucket];\n"}
{"Commit title": "fix compiler warning", "Commit body": "@@ -462,7 +462,7 @@ mkldnn::memory::primitive_desc GetPrimitiveDesc(mkldnn::memory::primitive_desc p\nmkldnn_memory_format_tformat);\n\ninlineboolsame_shape(constmxnet::TShape &shape,constmkldnn_dims_tdims,intndims) {\nif(shape.ndim() !=(size_t)ndims)\nif(shape.ndim() != ndims)\nreturnfalse;\nfor(inti =0; i < ndims; i++)\nif(shape[i] != dims[i])\n"}
{"Commit title": "Revert \"Manually track num_max_thread (#12380)\"", "Commit body": "@@ -73,14 +73,18 @@ void OpenMP::set_reserve_cores(int cores) {\nCHECK_GE(cores,0);\nreserve_cores_ = cores;\n#ifdef_OPENMP\nomp_thread_max_ =std::max(omp_thread_max_ - reserve_cores_,1);\nif(reserve_cores_ >= omp_thread_max_) {\nomp_set_num_threads(1);\n}else{\nomp_set_num_threads(omp_thread_max_ - reserve_cores_);\n}\n#endif\n}\n\nintOpenMP::GetRecommendedOMPThreadCount(boolexclude_reserved)const{\n#ifdef_OPENMP\nif(omp_num_threads_set_in_environment_) {\nreturnomp_thread_max_;\nreturnomp_get_max_threads();\n}\nif(enabled_) {\nintthread_count =omp_get_max_threads();\n@@ -97,8 +101,10 @@ int OpenMP::GetRecommendedOMPThreadCount(bool exclude_reserved) const {\n}\nreturnomp_thread_max_;\n}\n#endif\nreturn1;\n#else\nreturn1;\n#endif\n}\n\nOpenMP *__init_omp__ = OpenMP::Get();\n"}
{"Commit title": "update MKL-DNN CI id", "Commit body": ""}
{"Commit title": "change into parallel", "Commit body": "@@ -97,14 +97,12 @@ class ImageClassifier(modelPathPrefix: String,\ndefclassifyImageBatch(inputBatch:Traversable[BufferedImage],topK:Option[Int]=None):\nIndexedSeq[IndexedSeq[(String,Float)]]={\n\nvalimageBatch=ListBuffer[NDArray]()\nfor(image<-inputBatch) {\nvalscaledImage=ImageClassifier.reshapeImage(image, width, height)\nvalpixelsNDArray=ImageClassifier.bufferedImageToPixels(scaledImage, inputShape.drop(1))\nimageBatch+=pixelsNDArray\n}\nvalinputBatchSeq=inputBatch.toIndexedSeq\nvalimageBatch=inputBatchSeq.indices.par.map(idx=>{\nvalscaledImage=ImageClassifier.reshapeImage(inputBatchSeq(idx), width, height)\nImageClassifier.bufferedImageToPixels(scaledImage, inputShape.drop(1))\n}).toList\nvalop=NDArray.concatenate(imageBatch)\n\nvalresult=super.classifyWithNDArray(IndexedSeq(op), topK)\nhandler.execute(op.dispose())\nhandler.execute(imageBatch.foreach(_.dispose()))\n"}
{"Commit title": "add codepath for implicit conversion to half precision for batch_gemm…", "Commit body": "@@ -315,9 +315,57 @@ void linalg_gemm<gpu, mshadow::half::half_t>(const Tensor<gpu, 2, mshadow::half:\n&beta, C.dptr_, C.stride_, C.size(1) * C.stride_, A.size(0))) \\\n}\n\nLINALG_GPU_BATCH_GEMM(SgemmStridedBatched,float)\nLINALG_GPU_BATCH_GEMM(DgemmStridedBatched,double)\n\n#ifCUDA_VERSION <9010\nLINALG_GPU_BATCH_GEMM(SgemmStridedBatched,float)\n#else\ntemplate<>\ninlinevoidlinalg_batch_gemm<gpu,float>(constTensor<gpu,3,float>& A,\nconstTensor<gpu,3,float>& B,\nconstTensor<gpu,3,float>& C,\nfloatalpha,floatbeta,booltA,\nbooltB, Stream<gpu>* s) {\nusingnamespacemxnet;\nusingmshadow::gpu;\nCHECK_NOTNULL(s);\nlinalg_check_batch_size(A.size(0), B.size(0), C.size(0));\ncheck_gemm(A[0], B[0], C[0], alpha, beta, tA, tB);\nautoblas_handle = Stream<gpu>::GetBlasHandle(s);\nbooluse_tensor_ops =\nGetEnvAllowTensorCore() &&GetEnvAllowTensorCoreConversion();\n\nusingnamespacemshadow::cuda;\nautocublas_math_mode =\nuse_tensor_ops ? CUBLAS_TENSOR_OP_MATH : CUBLAS_DEFAULT_MATH;\nautoprevious_math_mode =SetCublasMathMode(blas_handle, cublas_math_mode);\n\n//cublasGemmStridedBatchedEx is only supported for GPU with architecture\n//capabilities equal or greater than 5.0. Fall back to\n//cublasSgemmStridedBatched, which doesn't support implicit conversion\n//to half-precision to use TensorCores\nautocc_major = (s->prop).major;\nif((cc_major >=5) && use_tensor_ops) {\nCUBLAS_CALL(cublasGemmStridedBatchedEx(\nblas_handle, (tB ? CUBLAS_OP_T : CUBLAS_OP_N),\n(tA ? CUBLAS_OP_T : CUBLAS_OP_N), C.size(2), C.size(0),\n(tB ? B.size(2) : B.size(0)), &alpha, B.dptr_, CUDA_R_16F,\nB.stride_, B.size(1) * B.stride_, A.dptr_, CUDA_R_16F, A.stride_,\nA.size(1) * A.stride_, &beta, C.dptr_, CUDA_R_16F, C.stride_,\nC.size(1) * C.stride_, A.size(0), CUDA_R_32F,\nCUBLAS_GEMM_DEFAULT_TENSOR_OP));\n}else{\nCUBLAS_CALL(cublasSgemmStridedBatched(\nblas_handle, (tB ? CUBLAS_OP_T : CUBLAS_OP_N),\n(tA ? CUBLAS_OP_T : CUBLAS_OP_N), C.size(2), C.size(0),\n(tB ? B.size(2) : B.size(0)), &alpha, B.dptr_, B.stride_,\nB.size(1) * B.stride_, A.dptr_, A.stride_, A.size(1) * A.stride_,\n&beta, C.dptr_, C.stride_, C.size(1) * C.stride_, A.size(0)));\n}\nSetCublasMathMode(blas_handle, previous_math_mode);\n}\n#endif//CUDA_VERSION < 9010\n\n//Version where matrix rows are given by second axis.\n#defineLINALG_GPU_BATCH_GEMM_AXIS(fname, DType) \\\ntemplate<>inline\\\n"}
{"Commit title": "sanity", "Commit body": "@@ -268,7 +268,7 @@ MSHADOW_XINLINE void binary_broadcast_assign(const index_t idx, const bool addto\n}\n\ntemplate<typenameReducer,intndim,typenameAType,typenameDType,typenameOType,\ntypenameOP,booluse_index =false>\ntypenameOP,booluse_index =false>\nMSHADOW_XINLINEvoidseq_reduce_assign(constindex_tidx,constsize_tM,constbooladdto,\nconstDType* __restrict big, OType *small,\nconstShape<ndim>& bshape,constShape<ndim>& sshape,\n@@ -317,7 +317,7 @@ void BinaryBroadcastComputeImpl(Stream<cpu> *s, const OpReqType req,\n}\n\ntemplate<typenameReducer,intndim,typenameAType,typenameDType,typenameOType,typenameOP,\nbooluse_index =false>\nbooluse_index =false>\nvoidseq_reduce_compute(constsize_tN,constsize_tM,constbooladdto,\nconstDType *big, OType *small,constShape<ndim> bshape,\nconstShape<ndim> sshape,constShape<ndim> rshape,\n"}
{"Commit title": "Update activations.py", "Commit body": "@@ -120,7 +120,7 @@ class PReLU(HybridBlock):\n----------\nalpha_initializer : Initializer\nInitializer for the `embeddings` matrix.\nin_channels : int\nin_channels : int, default 1\nNumber of channels (alpha parameters) to learn. Can either be 1\nor `n` where `n` is the size of the second dimension of the input\ntensor.\n"}
{"Commit title": "leaky relu forward speed", "Commit body": "@@ -39,6 +39,7 @@\n#include\"./mshadow_op.h\"\n#include\"./random/sampler.h\"\n#include\"./random/sample_op.h\"\n#include\"./tensor/elemwise_binary_broadcast_op.h\"\n\nnamespacemxnet{\nnamespaceop{\n@@ -114,13 +115,21 @@ class LeakyReLUOp : public Operator {\n}\ncaseleakyrelu::kPReLU: {\nweight = in_data[leakyrelu::kGamma].get<xpu,1, DType>(s);\nif(weight.shape_.Size() ==1) {\nAssign(out, req[leakyrelu::kOut],\nF<mshadow_op::xelu>(data,mshadow::expr::broadcast_scalar(weight, out.shape_)));\n}else{\nAssign(out, req[leakyrelu::kOut],\nF<mshadow_op::xelu>(data, mshadow::expr::broadcast<1>(weight, out.shape_)));\n}\nTShape new_lshape, new_rshape, new_oshape;\nintndim =op::BinaryBroadcastShapeCompact(in_data[leakyrelu::kData].shape_,\nin_data[leakyrelu::kGamma].shape_,\nout_data[leakyrelu::kOut].shape_,\n&new_lshape, &new_rshape, &new_oshape);\nBROADCAST_NDIM_SWITCH(ndim, NDim, {\nmshadow::Shape<NDim> oshape = new_oshape.get<NDim>();\nmshadow::Shape<NDim> lstride =mxnet_op::calc_stride(new_lshape.get<NDim>());\nmshadow::Shape<NDim> rstride =mxnet_op::calc_stride(new_rshape.get<NDim>());\nmxnet_op::Kernel<mxnet_op::binary_broadcast_kernel<NDim, DType,\nop::mshadow_op::xelu>, xpu>::\ntemplateLaunchEx(s, new_oshape.Size(), req[leakyrelu::kOut], lstride, rstride, oshape,\nin_data[leakyrelu::kData].dptr<DType>(), in_data[leakyrelu::kGamma].dptr<DType>(),\nout_data[leakyrelu::kOut].dptr<DType>());\n});\nbreak;\n}\ncaseleakyrelu::kRReLU: {\n"}
{"Commit title": "Accelerate the performance of topk for CPU side", "Commit body": "@@ -170,11 +170,12 @@ MSHADOW_FORCE_INLINE void TopKSort<cpu>(const Tensor<cpu, 1, real_t>& dat,\n//Use full sort when K is relatively large.\nconstboolfull_sort(K*8> N);\n//Batch size.\nconstintM(dat.size(0)/N);\nconstintM(work.size(0)/(sizeof(real_t)*N));\nconstintomp_threads(engine::OpenMP::Get()->GetRecommendedOMPThreadCount());\n#pragmaomp parallel for num_threads(omp_threads)\nfor(inti =0; i < M; ++i) {\nreal_t*vals = dat.dptr_;\nreal_t*vals =reinterpret_cast<real_t*>(work.dptr_);\nreal_t*sorted_vals = dat.dptr_+i*N;\nint*indices = ind.dptr_+i*N;\nif(is_ascend) {\nif(full_sort) {\n@@ -193,11 +194,9 @@ MSHADOW_FORCE_INLINE void TopKSort<cpu>(const Tensor<cpu, 1, real_t>& dat,\n[&](constint& i1,constint& i2){returnvals[i1] > vals[i2]; });\n}\n}\nreal_t*buff =reinterpret_cast<real_t*>(work.dptr_)+i*K;\nfor(intj =0; j < K; ++j) {\nbuff[j] = vals[indices[j]];\nsorted_vals[j] = vals[indices[j]];\n}\nstd::copy(buff, buff+K, &vals[i*N]);\n}\n}\n\n@@ -380,16 +379,7 @@ void TopKImpl(RunContext ctx,\nindices = Tensor<xpu,1,int>(reinterpret_cast<int*>(workspace_curr_ptr),\nShape1(src.Size()), s);//indices in the original matrix\nworkspace_curr_ptr +=sizeof(int) * src.Size();\nif(do_transpose) {\nsorted_dat =reshape(transpose(dat,Shape3(0,2,1)),Shape1(src.Size()));\n}else{\nsorted_dat =reshape(dat,Shape1(src.Size()));\n}\nmxnet_op::Kernel<range_fwd, xpu>::Launch(s, batch_size * element_num,1,0,1,\nkWriteTo, indices.dptr_);\n\nCHECK_EQ(sorted_dat.CheckContiguous(),true);\nCHECK_EQ(indices.CheckContiguous(),true);\nif(param.ret_typ== topk_enum::kReturnMask) {\nsel_indices = Tensor<xpu,1,int>(reinterpret_cast<int*>(workspace_curr_ptr),\nShape1(batch_size * k), s);\n@@ -401,8 +391,35 @@ void TopKImpl(RunContext ctx,\nCHECK_EQ(sel_indices.CheckContiguous(),true);\nCHECK_EQ(mask_val.CheckContiguous(),true);\n}\ntemp_workspace = Tensor<xpu,1,char>(workspace_curr_ptr,Shape1(temp_size), s);//temp space\nworkspace_curr_ptr += temp_size;\n\nif(std::is_same<xpu, cpu>::value) {\nTensor<xpu,1,real_t> flattened_data;\nif(do_transpose) {\nflattened_data = Tensor<xpu,1,real_t>(reinterpret_cast<real_t*>(workspace_curr_ptr),\nShape1(src.Size()), s);\nworkspace_curr_ptr +=sizeof(real_t) * src.Size();\nflattened_data =reshape(transpose(dat,Shape3(0,2,1)),Shape1(src.Size()));\nCHECK_EQ(flattened_data.CheckContiguous(),true);\n}else{\nflattened_data = src.FlatTo1D<xpu,real_t>(s);\n}\ntemp_workspace = Tensor<xpu,1,char>(reinterpret_cast<char*>(flattened_data.dptr_),\nShape1(sizeof(real_t)*src.Size()), s);\nCHECK_EQ(temp_workspace.CheckContiguous(),true);\n}else{\nif(do_transpose) {\nsorted_dat =reshape(transpose(dat,Shape3(0,2,1)),Shape1(src.Size()));\n}else{\nsorted_dat =reshape(dat,Shape1(src.Size()));\n}\nCHECK_EQ(sorted_dat.CheckContiguous(),true);\ntemp_workspace = Tensor<xpu,1,char>(workspace_curr_ptr,Shape1(temp_size), s);//temp space\nworkspace_curr_ptr += temp_size;\n}\n\nmxnet_op::Kernel<range_fwd, xpu>::Launch(s, batch_size * element_num,1,0,1,\nkWriteTo, indices.dptr_);\nCHECK_EQ(indices.CheckContiguous(),true);\n\n//2. Perform inplace batch sort.\n//After sorting, each batch in `sorted_dat` will be sorted in the corresponding order\n@@ -427,7 +444,6 @@ void TopKImpl(RunContext ctx,\n}\nIndexFill(ret_mask, sel_indices, mask_val);\n}elseif(param.ret_typ== topk_enum::kReturnIndices) {\nindices = F<mshadow_op::mod>(indices, element_num);\nif(do_transpose) {\nTensor<xpu,3,real_t> ret_indices = ret[0].FlatTo3D<xpu,real_t>(axis, axis, s);\nret_indices = tcast<real_t>(transpose(\n@@ -437,14 +453,15 @@ void TopKImpl(RunContext ctx,\nelement_num)),\n0, k),\nShape3(0,2,1)));\nret_indices = F<mshadow_op::mod>(ret_indices, element_num);\n}else{\nTensor<xpu,2,real_t> ret_indices =\nret[0].get_with_shape<xpu,2,real_t>(Shape2(batch_size, k), s);\nret_indices = tcast<real_t>(slice<1>(\ninplace_reshape(indices,Shape2(batch_size, element_num)),0, k));\nret_indices = F<mshadow_op::mod>(ret_indices, element_num);\n}\n}else{\nindices = F<mshadow_op::mod>(indices, element_num);\nif(do_transpose) {\nTensor<xpu,3,real_t> ret_value = ret[0].FlatTo3D<xpu,real_t>(axis, axis, s);\nTensor<xpu,3,real_t> ret_indices = ret[1].FlatTo3D<xpu,real_t>(axis, axis, s);\n@@ -460,6 +477,7 @@ void TopKImpl(RunContext ctx,\nelement_num)),\n0, k),\nShape3(0,2,1)));\nret_indices = F<mshadow_op::mod>(ret_indices, element_num);\n}else{\nTensor<xpu,2,real_t> ret_value =\nret[0].get_with_shape<xpu,2,real_t>(Shape2(batch_size, k), s);\n@@ -468,6 +486,7 @@ void TopKImpl(RunContext ctx,\nret_value = slice<1>(inplace_reshape(sorted_dat,Shape2(batch_size, element_num)),0, k);\nret_indices = tcast<real_t>(slice<1>(\ninplace_reshape(indices,Shape2(batch_size, element_num)),0, k));\nret_indices = F<mshadow_op::mod>(ret_indices, element_num);\n}\n}\n}\n"}
{"Commit title": "replace binary search with hash map", "Commit body": "@@ -27,6 +27,7 @@\n\n#include<mxnet/operator_util.h>\n#include<vector>\n#include<unordered_map>\n#include<algorithm>\n#include<utility>\n#include<type_traits>\n@@ -443,46 +444,22 @@ struct DotCsrRspDnsByRowBlocks {\nconstnnvm::dim_tnnr_r,\nconstnnvm::dim_tnum_rows,\nconstnnvm::dim_tnum_cols,\nconstnnvm::dim_tseg_len) {\nconstnnvm::dim_tseg_len,\nconststd::unordered_map<RType, nnvm::dim_t>* row_idx_map) {\nusingnnvm::dim_t;\nconstdim_tseg_start = i * seg_len;\nif(seg_start >= num_rows)return;\nconstdim_tseg_end =std::min(seg_start + seg_len, num_rows);\nfor(dim_tj = seg_start; j < seg_end; ++j) {\nif(indptr_l[j] == indptr_l[j+1])continue;\nconstdim_toffset_out = j * num_cols;\n//Use binary search to find the lower_bound of val in row_idx array\nconstRType* first = row_idx_r;\nconstRType* last = row_idx_r + nnr_r;\nconstCType val = col_idx_l[indptr_l[j]];\nconstRType* it;\nintcount = last - first, step;\nwhile(count >0) {\nit = first;\nstep = count /2;\nit += step;\nif(*it < val) {\nfirst = ++it;\ncount -= step +1;\n}else{\ncount = step;\n}\n}\nconstRType* row_idx_ptr = first;\n//end of binary search\nif(row_idx_ptr == row_idx_r+nnr_r || *row_idx_ptr > col_idx_l[indptr_l[j+1]-1])continue;\nfor(IType k = indptr_l[j]; k < indptr_l[j+1] && row_idx_ptr != row_idx_r+nnr_r;) {\nif(col_idx_l[k] == *row_idx_ptr) {\nconstdim_toffset_r = (row_idx_ptr - row_idx_r) * num_cols;\nfor(dim_tl =0; l < num_cols; ++l) {\nfor(IType k = indptr_l[j]; k < indptr_l[j+1]; k++) {\nfor(dim_tl =0; l < num_cols; ++l) {\nif(row_idx_map->count(static_cast<RType>(k))) {\nautoit = row_idx_map->find(static_cast<RType>(k));\nconstdim_toffset_r = it->second* num_cols;\nout[offset_out+l] += data_l[k] * data_r[offset_r+l];\n}\n++k;\n++row_idx_ptr;\n}elseif(col_idx_l[k] < *row_idx_ptr) {\n++k;\n}else{\n++row_idx_ptr;\n}\n}\n}\n@@ -782,6 +759,7 @@ inline void DotCsrRspDnsImpl(const OpContext& ctx,\nconstTBlob col_idx_l = lhs.aux_data(csr::kIdx);\nconstTBlob data_r = rhs.data();\nconstTBlob row_idx_r = rhs.aux_data(rowsparse::kIdx);\nconstnnvm::dim_tnnr = rhs.storage_shape()[0];\n\nMSHADOW_SGL_DBL_TYPE_SWITCH(data_l.type_flag_, DType, {//data type\nMSHADOW_IDX_TYPE_SWITCH(indptr_l.type_flag_, IType, {//indptr type\n@@ -798,11 +776,16 @@ inline void DotCsrRspDnsImpl(const OpContext& ctx,\nif(trans_lhs) {\nLOG(FATAL) <<\"DotCsrRspDnsImpl has not implemented dot(csr.T, rsp) = dns yet\";\n}else{\nconstRType* row_idx_ptr = row_idx_r.dptr<RType>();\nstd::unordered_map<RType,dim_t> row_idx_map;\nfor(dim_tind =0; ind < nnr; ind++) {\nrow_idx_map.emplace(row_idx_ptr[ind], ind);\n}\nmxnet_op::Kernel<DotCsrRspDnsByRowBlocks, cpu>::Launch(s, num_threads,\nret->dptr<DType>(), data_l.dptr<DType>(),\nindptr_l.dptr<IType>(), col_idx_l.dptr<CType>(), data_r.dptr<DType>(),\nrow_idx_r.dptr<RType>(), rhs.storage_shape()[0],\nret->shape_[0], ret->shape_[1], seg_len);\nret->shape_[0], ret->shape_[1], seg_len, &row_idx_map);\n}\n});\n});\n"}
{"Commit title": "Make std::vector thread local.", "Commit body": "@@ -692,15 +692,16 @@ void BatchNormGradCompute<gpu>(const nnvm::NodeAttrs& attrs,\nconststd::vector<TBlob>& outputs) {\nCHECK_EQ(inputs.size(),8U);\nBatchNormParam param = nnvm::get<BatchNormParam>(attrs.parsed);\nstd::vector<TBlob>out_grad(1, inputs[0]);\nstd::vector<TBlob>out_data(3);\nstaticthread_localstd::vector<TBlob>out_grad(1);\nstaticthread_localstd::vector<TBlob>out_data(3);\nstaticthread_localstd::vector<TBlob>in_data(3);\nstaticthread_localstd::vector<TBlob>aux_states(2);\nout_grad[0] = inputs[0];\nout_data[batchnorm::kMean] = inputs[1];\nout_data[batchnorm::kVar] = inputs[2];\nstd::vector<TBlob>in_data(3);\nin_data[batchnorm::kData] = inputs[3];\nin_data[batchnorm::kGamma] = inputs[4];\nstd::vector<TBlob>aux_states(2);\nstd::vector<TBlob>in_grad(outputs.begin(), outputs.begin() +3);\nstd::vector<TBlob> &in_grad = outputs;\nintdtype = inputs[0].type_flag_;\nTShape shape = inputs[0].shape_;\n\n"}
{"Commit title": "remove waittowrite", "Commit body": "@@ -272,7 +272,6 @@ class KVStoreDist : public KVStoreLocal {\nconstautostorage_type = merged.storage_type();\nif(merged.ctx().dev_mask() == cpu::kDevMask) {\n//make sure the previous push/pull is completed\nsend_buf.WaitToWrite();\nsend_buf = merged;//avoid memory copy\n}else{\nif(send_buf.is_none()) {\n"}
{"Commit title": "Merge branch 'master' ofhttps://github.com/apache/incubator-mxnet", "Commit body": "@@ -182,7 +182,7 @@ def test_l1_loss():\nassertmod.score(data_iter,eval_metric=mx.metric.Loss())[0][1]<0.1\n\n\n@with_seed(1234)\n@with_seed()\ndeftest_ctc_loss():\nloss=gluon.loss.CTCLoss()\nl=loss(mx.nd.ones((2,20,4)),mx.nd.array([[1,0,-1,-1],[2,1,1,-1]]))\n"}
{"Commit title": "correction to pr#7522 - testb specialization", "Commit body": "@@ -1575,7 +1575,7 @@ Y(cmpwm, movzwl, loadw, cmpl)\nvoidlower(Vunit& u, testb& i, Vlabel b,size_tz) {\nlower_impl(u, b, z, [&] (Vout& v) {\nif(i.s0== i.s1) {\nv << testbi{(int8_t)0xff, i.s1, i.sf};\nv << testbi{(uint8_t)0xff, i.s1, i.sf};\n}else{\nautos0 = v.makeReg();\nautos1 = v.makeReg();\n"}
{"Commit title": "Replace bitset with string_charmask() in ucwords.", "Commit body": "@@ -46,7 +46,6 @@\n#include\"hphp/zend/html-table.h\"\n\n#include<folly/Unicode.h>\n#include<bitset>\n#include<locale.h>\n\nnamespaceHPHP{\n@@ -448,18 +447,15 @@ String HHVM_FUNCTION(ucwords,\n\nif(!delimiters.length())returnstrcopy;\n\nstd::bitset<256> delimiters_set;\nintdelimiters_len = delimiters.length();\nfor(inti =0; i < delimiters_len; i++) {\ndelimiters_set.set(static_cast<uint8_t>(delimiters[i]));\n}\ncharmask[256];\nstring_charmask(delimiters.c_str(), delimiters.size(), mask);\n\nuint8_tlast ='';\nreturnstringForEach<true>(strcopy.size(), strcopy, [&] (charc) {\ncharret = delimiters_set.test(last) ?toupper(c) : c;\nlast = c;\nreturnret;\n});\nfor(char* end = string + strcopy.size(); string < end; ) {\nif(mask[(unsignedchar)*string++]) {\n*string =toupper(*string);\n}\n}\nreturnstrcopy;\n}\n\nStringHHVM_FUNCTION(strip_tags,\n"}
{"Commit title": "Log parse errors", "Commit body": "@@ -216,6 +216,9 @@ bool Parser::parse() {\n}\nreturntrue;\n}catch(constParseTimeFatalException& e) {\nLogger::Error(\"Error parsing %s:%d: %s\", m_fileName, e.m_line,\ne.getMessage().c_str());\n\nm_file->cleanupForError(m_ar);\nif(e.m_parseFatal) {\nm_file->makeParseFatal(m_ar, e.getMessage(), e.m_line);\n"}
{"Commit title": "Only call mbfl_strlen in invalid conditions in mb_substr.", "Commit body": "@@ -2282,15 +2282,23 @@ static Variant php_mb_substr(const String& str, int from,\n}\n}\n\nintsize;\nintlen = vlen.toInt64();\nintsize =0;\n\nif(substr) {\nsize =mbfl_strlen(&string);\nintsize_tmp = -1;\nif(vlen.isNull() || len ==0x7FFFFFFF) {\nsize_tmp =mbfl_strlen(&string);\nlen = size_tmp;\n}\nif(from <0|| len <0) {\nsize = size_tmp <0?mbfl_strlen(&string) : size_tmp;\n}\n}else{\nsize = str.size();\n}\nintlen = vlen.toInt64();\nif(vlen.isNull() || len ==0x7FFFFFFF) {\nlen = size;\nif(vlen.isNull() || len ==0x7FFFFFFF) {\nlen = size;\n}\n}\n\n/*if \"from\" position is negative, count start position from the end\n@@ -2313,11 +2321,8 @@ static Variant php_mb_substr(const String& str, int from,\n}\n}\n\nif(from > size) {\nif(!substr) {\nreturnfalse;\n}\nfrom = size;\nif(!substr && from > size) {\nreturnfalse;\n}\n\nmbfl_string result;\n"}
{"Commit title": "Set and use PCRE_NO_UTF8_CHECK the same way as php-src in preg_split", "Commit body": "@@ -1649,6 +1649,13 @@ Variant preg_split(const String& pattern, const String& subject,\nstart_offset, g_notempty | utf8_check,\noffsets, size_offsets);\n\n/*Subsequent calls to pcre_exec don't need to bother with the\n* utf8 validity check: if the subject isn't valid, the first\n* call to pcre_exec will have failed, and as long as we only\n* set start_offset to known character boundaries we won't\n* supply an invalid offset.*/\nutf8_check = PCRE_NO_UTF8_CHECK;\n\n/*Check for too many substrings condition.*/\nif(count ==0) {\nraise_warning(\"Matched, but too many substrings\");\n@@ -1657,13 +1664,6 @@ Variant preg_split(const String& pattern, const String& subject,\n\n/*If something matched*/\nif(count >0) {\n/*Subsequent calls to pcre_exec don't need to bother with the\n* utf8 validity check: if the subject isn't valid, the first\n* call to pcre_exec will have failed, and as long as we only\n* set start_offset to known character boundaries we won't\n* supply an invalid offset.*/\nutf8_check = PCRE_NO_UTF8_CHECK;\n\nif(!no_empty || subject.data() + offsets[0] != last_match) {\nif(offset_capture) {\n/*Add (match, offset) pair to the return value*/\n@@ -1721,7 +1721,7 @@ Variant preg_split(const String& pattern, const String& subject,\ninit_local_extra(&bump_extra, bump_pce->extra);\ncount =pcre_exec(bump_pce->re, &bump_extra, subject.data(),\nsubject.size(), start_offset,\n0, offsets, size_offsets);\nutf8_check, offsets, size_offsets);\nif(count <1) {\nraise_warning(\"Unknown error\");\noffsets[0] = start_offset;\n"}
{"Commit title": "Correctly check useLocalStaticStringMap.", "Commit body": "@@ -188,15 +188,16 @@ Unit* compile_string(const char* s,\nsize_tsz,\nconstchar* fname/*= nullptr*/,\nbooluseLocalStaticStringMap/*= false*/) {\nif(useLocalStaticStringMap)\n*StaticStringConfig::s_useLocalMap =true;\n\nautomd5string =string_md5(s, sz);\nMD5md5(md5string.c_str());\nUnit* u =Repo::get().loadUnit(fname ? fname :\"\", md5).release();\nif(u !=nullptr) {\nreturnu;\n}\n\nif(useLocalStaticStringMap)\n*StaticStringConfig::s_useLocalMap =true;\n\n//NB: fname needs to be long-lived if generating a bytecode repo because it\n//can be cached via a Location ultimately contained by ErrorInfo for printing\n//code errors.\n"}
{"Commit title": "Fix a memory leak in simplexml", "Commit body": "@@ -1658,7 +1658,12 @@ c_SimpleXMLElementIterator::c_SimpleXMLElementIterator(Class* cb) :\nExtObjectData(cb), sxe(nullptr) {\n}\n\nc_SimpleXMLElementIterator::~c_SimpleXMLElementIterator() { }\nc_SimpleXMLElementIterator::~c_SimpleXMLElementIterator() {\nif(sxe) {\ndecRefObj(sxe.get());\nsxe =nullptr;\n}\n}\n\nvoidc_SimpleXMLElementIterator::t___construct() {\n}\n"}
{"Commit title": "fix a memory leak in simplexml", "Commit body": "@@ -1656,7 +1656,7 @@ c_SimpleXMLElementIterator::c_SimpleXMLElementIterator(Class* cb) :\n\nc_SimpleXMLElementIterator::~c_SimpleXMLElementIterator() {\nif(sxe) {\nsxe->decRefCount();\ndecRefObj(sxe);\nsxe =nullptr;\n}\n}\n"}
{"Commit title": "make it optional to use bundled tzdata, and fatal otherwise if it's n…", "Commit body": "@@ -169,7 +169,11 @@ add_definitions(-DHAVE_INTTYPES_H)\n#Works on Linux and Mac; given that we don't want to (and basically don't)\n#update the bundled tzdata, if this doesn't work on some platform, please\n#blacklist the platform rather than whitelisting others.\nif(EXISTS/usr/share/zoneinfo)\noption(USE_BUNDLED_TZDATA\"Use bundled system tzdata instead of /usr/share/zoneinfo\"OFF)\nif(NOT${USE_BUNDLED_TZDATA})\nif(NOTEXISTS/usr/share/zoneinfo)\nmessage(FATAL_ERROR\"/usr/share/zoneinfo does not exist, and USE_BUNDLED_TZDATA not set\")\nendif()\nadd_definitions(-DHAVE_SYSTEM_TZDATA)\nendif()\n\n"}
{"Commit title": "[PATCH] proxygenCVE-2019-9512to 9515", "Commit body": "@@ -0,0 +1,391 @@\nFrom 7df1726dd4e11cc38a6eff12d968a134412e341d Mon Sep 17 00:00:00 2001\nFrom: jjergus <jjergus@fb.com>\nDate: Fri, 9 Aug 2019 14:49:41 -0700\nSubject: [PATCH] proxygen CVE-2019-9512 to 9515\n\n---\nproxygen/lib/http/session/HTTPSession.cpp     |  98 ++++++++++++++-\nproxygen/lib/http/session/HTTPSession.h       |  60 +++++++++\n.../test/HTTPDownstreamSessionTest.cpp        | 119 ++++++++++++++++++\n3 files changed, 274 insertions(+), 3 deletions(-)\n\ndiff --git a/third-party/proxygen/src/proxygen/lib/http/session/HTTPSession.cpp b/third-party/proxygen/src/proxygen/lib/http/session/HTTPSession.cpp\nindex 3f980740..60cfd744 100644\n--- a/third-party/proxygen/src/proxygen/lib/http/session/HTTPSession.cpp\n+++ b/third-party/proxygen/src/proxygen/lib/http/session/HTTPSession.cpp\n@@ -169,6 +169,8 @@HTTPSession::HTTPSession(\nif (!sock_->isReplaySafe()) {\nsock_->setReplaySafetyCallback(this);\n}\n+\n+rateLimitingCounters_ = std::make_shared<RateLimitingCounters>();\n}\n\nvoid HTTPSession::setupCodec() {\n@@ -1048,7 +1050,12 @@void HTTPSession::onError(HTTPCodec::StreamID streamID,\ninfoCallback_->onRequestBegin(*this);\n}\nif (txn) {\n-handleErrorDirectly(txn, error);\n+if (incrementDirectErrorHandlingInCurInterval()) {\n+// The rate limit has been exceeded\n+return;\n+} else {\n+handleErrorDirectly(txn, error);\n+}\n}\n} else if (newTxn) {\nonNewTransactionParseError(streamID, error);\n@@ -1061,7 +1068,12 @@void HTTPSession::onError(HTTPCodec::StreamID streamID,\n\nif (!txn->getHandler() &&\ntxn->getEgressState() == HTTPTransactionEgressSM::State::Start) {\n-handleErrorDirectly(txn, error);\n+if (incrementDirectErrorHandlingInCurInterval()) {\n+// The rate limit has been exceeded\n+return;\n+} else {\n+handleErrorDirectly(txn, error);\n+}\nreturn;\n}\n\n@@ -1077,6 +1089,11 @@void HTTPSession::onAbort(HTTPCodec::StreamID streamID,\nErrorCode code) {\nVLOG(4) << \"stream abort on \" << *this << \", streamID=\" << streamID\n<< \", code=\" << getErrorCodeString(code);\n+\n+if (incrementNumControlMsgsInCurInterval(http2::FrameType::RST_STREAM)) {\n+return;\n+}\n+\nHTTPTransaction* txn = findTransaction(streamID);\nif (!txn) {\nVLOG(4) << *this << \" abort for unrecognized transaction, streamID= \"\n@@ -1172,6 +1189,10 @@void HTTPSession::onGoaway(uint64_t lastGoodStreamID,\nvoid HTTPSession::onPingRequest(uint64_t uniqueID) {\nVLOG(4) << *this << \" got ping request with id=\" << uniqueID;\n\n+if (incrementNumControlMsgsInCurInterval(http2::FrameType::PING)) {\n+return;\n+}\n+\nTimePoint timestamp = getCurrentTime();\n\n// Insert the ping reply to the head of writeBuf_\n@@ -1214,6 +1235,9 @@void HTTPSession::onWindowUpdate(HTTPCodec::StreamID streamID,\n\nvoid HTTPSession::onSettings(const SettingsList& settings) {\nDestructorGuard g(this);\n+if (incrementNumControlMsgsInCurInterval(http2::FrameType::SETTINGS)) {\n+return;\n+}\nfor (auto& setting: settings) {\nif (setting.id == SettingsId::INITIAL_WINDOW_SIZE) {\nonSetSendWindow(setting.value);\n@@ -1238,7 +1262,8 @@void HTTPSession::onSettingsAck() {\n\nvoid HTTPSession::onPriority(HTTPCodec::StreamID streamID,\nconst HTTPMessage::HTTPPriority& pri) {\n-if (!getHTTP2PrioritiesEnabled()) {\n+if (!getHTTP2PrioritiesEnabled() ||\n+incrementNumControlMsgsInCurInterval(http2::FrameType::PRIORITY)) {\nreturn;\n}\nhttp2::PriorityUpdate h2Pri{std::get<0>(pri), std::get<1>(pri),\n@@ -2429,6 +2454,73 @@HTTPSession::incrementOutgoingStreams() {\nHTTPSessionBase::onNewOutgoingStream(outgoingStreams_);\n}\n\n+bool HTTPSession::incrementNumControlMsgsInCurInterval(\n+http2::FrameType frameType) {\n+if (rateLimitingCounters_->numControlMsgsInCurrentInterval == 0) {\n+// The first time we get a \"control message\", we schedule a\n+// function on the event base that clears out the value of\n+// numControlMsgsInCurrentInterval. Once it is cleared, the next\n+// such event that fires causes the function to be scheduled, and the\n+// cycle repeats.\n+scheduleResetNumControlMsgs();\n+}\n+\n+(rateLimitingCounters_->numControlMsgsInCurrentInterval)++;\n+if (rateLimitingCounters_->numControlMsgsInCurrentInterval >\n+maxControlMsgsPerInterval_) {\n+LOG(ERROR) << \" dropping connection due to too many control messages, \"\n+<< \"num control messages = \"\n+<< rateLimitingCounters_->numControlMsgsInCurrentInterval\n+<< \", most recent frame type = \"\n+<< getFrameTypeString(frameType) << \" \"\n+<< *this;\n+dropConnection();\n+return true;\n+}\n+\n+return false;\n+}\n+\n+bool HTTPSession::incrementDirectErrorHandlingInCurInterval() {\n+if (rateLimitingCounters_->numDirectErrorHandlingInCurrentInterval == 0) {\n+// The first time a direct error handling event fires, we schedule a\n+// function on the event base that clears out the value of\n+// numDirectErrorHandlingInCurrentInterval. Once it is cleared, the next\n+// such event that fires causes the function to be scheduled, and the\n+// cycle repeats.\n+scheduleResetDirectErrorHandling();\n+}\n+\n+(rateLimitingCounters_->numDirectErrorHandlingInCurrentInterval)++;\n+if (rateLimitingCounters_->numDirectErrorHandlingInCurrentInterval >\n+maxDirectErrorHandlingPerInterval_) {\n+LOG(ERROR) << \" dropping connection due to too many newly created txns \"\n+<< \" when directly handling errors,\"\n+<< \"num direct error handling cases = \"\n+<< rateLimitingCounters_->numDirectErrorHandlingInCurrentInterval\n+<< \" \"\n+<< *this;\n+dropConnection();\n+return true;\n+}\n+\n+return false;\n+}\n+\n+void HTTPSession::scheduleResetNumControlMsgs() {\n+auto ptr = rateLimitingCounters_;\n+sock_->getEventBase()->runAfterDelay([ptr]() {\n+ptr->numControlMsgsInCurrentInterval = 0;\n+}, controlMsgIntervalDuration_);\n+}\n+\n+void HTTPSession::scheduleResetDirectErrorHandling() {\n+auto ptr = rateLimitingCounters_;\n+sock_->getEventBase()->runAfterDelay([ptr]() {\n+ptr->numDirectErrorHandlingInCurrentInterval = 0;\n+}, directErrorHandlingIntervalDuration_);\n+}\n+\nvoid\nHTTPSession::onWriteSuccess(uint64_t bytesWritten) {\nDestructorGuard dg(this);\ndiff --git a/third-party/proxygen/src/proxygen/lib/http/session/HTTPSession.h b/third-party/proxygen/src/proxygen/lib/http/session/HTTPSession.h\nindex e3405945..f64609fe 100644\n--- a/third-party/proxygen/src/proxygen/lib/http/session/HTTPSession.h\n+++ b/third-party/proxygen/src/proxygen/lib/http/session/HTTPSession.h\n@@ -36,6 +36,13 @@class HTTPSessionStats;\n\n#define PROXYGEN_HTTP_SESSION_USES_BASE  1\n\n+// These constants define the rate at which we limit certain events.\n+constexpr uint32_t kDefaultMaxControlMsgsPerInterval = 50000;\n+constexpr uint32_t kDefaultControlMsgDuration = 100; // milliseconds\n+\n+constexpr uint32_t kDefaultMaxDirectErrorHandlingPerInterval = 100;\n+constexpr uint32_t kDefaultDirectErrorHandlingDuration = 100; // milliseconds\n+\nclass HTTPSession:\npublic HTTPSessionBase,\npublic HTTPTransaction::Transport,\n@@ -256,6 +263,23 @@class HTTPSession:\n\n~HTTPSession() override;\n\n+void setMaxControlMsgsPerInterval(uint32_t val) {\n+maxControlMsgsPerInterval_ = val;\n+}\n+\n+void setControlMsgIntervalDuration(uint32_t val) {\n+controlMsgIntervalDuration_ = val;\n+}\n+\n+void setMaxDirectErrorHandlingPerInterval(uint32_t val) {\n+maxDirectErrorHandlingPerInterval_ = val;\n+}\n+\n+void setDirectErrorHandlingIntervalDuration(uint32_t val) {\n+directErrorHandlingIntervalDuration_ = val;\n+}\n+\n+\n/**\n* Called by onHeadersComplete(). This function allows downstream and\n* upstream to do any setup (like preparing a handler) when headers are\n@@ -804,6 +828,16 @@class HTTPSession:\n\nvoid incrementOutgoingStreams();\n\n+// returns true if the threshold has been exceeded\n+bool incrementNumControlMsgsInCurInterval(http2::FrameType frameType);\n+\n+// returns true if the rate limiting threshold has been exceeded\n+bool incrementDirectErrorHandlingInCurInterval();\n+\n+void scheduleResetNumControlMsgs();\n+\n+void scheduleResetDirectErrorHandling();\n+\n// private members\n\nstd::list<ReplaySafetyCallback*> waitingForReplaySafety_;\n@@ -921,6 +955,32 @@class HTTPSession:\n*/\nuint64_t bodyBytesPerWriteBuf_{0};\n\n+struct RateLimitingCounters {\n+/**\n+* The two variables below keep track of the number of Control messages,\n+* and the number of error handling events that are handled by a newly created\n+* transaction handler seen in the current interval, respectively. These are\n+* shared_ptrs, as opposed to uint64_ts because we don't want to run into\n+* lifetime issues where the HTTPSession is destructed, and the rate\n+* limiting function is still scheduled to run on the event base.\n+*/\n+uint64_t numControlMsgsInCurrentInterval{0};\n+uint64_t numDirectErrorHandlingInCurrentInterval{0};\n+};\n+\n+std::shared_ptr<RateLimitingCounters> rateLimitingCounters_;\n+\n+/*\n+* If the number of control messages in a controlMsgIntervalDuration_\n+* millisecond interval exceeds maxControlMsgsPerInterval_, we drop the\n+* connection\n+*/\n+uint32_t maxControlMsgsPerInterval_{kDefaultMaxControlMsgsPerInterval};\n+uint32_t controlMsgIntervalDuration_{kDefaultControlMsgDuration};\n+\n+uint32_t maxDirectErrorHandlingPerInterval_{kDefaultMaxDirectErrorHandlingPerInterval};\n+uint32_t directErrorHandlingIntervalDuration_{kDefaultDirectErrorHandlingDuration};\n+\n/**\n* Container to hold the results of HTTP2PriorityQueue::nextEgress\n*/\ndiff --git a/third-party/proxygen/src/proxygen/lib/http/session/test/HTTPDownstreamSessionTest.cpp b/third-party/proxygen/src/proxygen/lib/http/session/test/HTTPDownstreamSessionTest.cpp\nindex 3105ada0..4280eade 100644\n--- a/third-party/proxygen/src/proxygen/lib/http/session/test/HTTPDownstreamSessionTest.cpp\n+++ b/third-party/proxygen/src/proxygen/lib/http/session/test/HTTPDownstreamSessionTest.cpp\n@@ -3523,3 +3523,122 @@TEST_F(HTTP2DownstreamSessionTest, test_set_egress_settings) {\nflushRequestsAndLoop();\ngracefulShutdown();\n}\n+\n+TEST_F(HTTP2DownstreamSessionTest, TestControlMsgRateLimitExceeded) {\n+auto streamid = clientCodec_->createStream();\n+\n+httpSession_->setMaxControlMsgsPerInterval(10);\n+\n+// Send 7 PRIORITY, 1 SETTINGS, and 3 PING frames. This should exceed the\n+// limit of 10, causing us to drop the connection.\n+for (int i = 0; i < 7; i++) {\n+clientCodec_->generatePriority(\n+requests_, streamid, HTTPMessage::HTTPPriority(0, false, 3));\n+}\n+\n+clientCodec_->generateSettings(requests_);\n+\n+for (int i = 0; i < 3; i++) {\n+clientCodec_->generatePingRequest(requests_);\n+}\n+\n+expectDetachSession();\n+\n+flushRequestsAndLoopN(1);\n+}\n+\n+TEST_F(HTTP2DownstreamSessionTest, TestControlMsgResetRateLimitTouched) {\n+auto streamid = clientCodec_->createStream();\n+\n+httpSession_->setMaxControlMsgsPerInterval(10);\n+httpSession_->setControlMsgIntervalDuration(0);\n+\n+// Send 7 PRIORITY, 1 SETTINGS, and 2 PING frames. This doesn't exceed the\n+// limit of 10.\n+for (int i = 0; i < 7; i++) {\n+clientCodec_->generatePriority(\n+requests_, streamid, HTTPMessage::HTTPPriority(0, false, 3));\n+}\n+\n+clientCodec_->generateSettings(requests_);\n+\n+for (int i = 0; i < 2; i++) {\n+clientCodec_->generatePingRequest(requests_);\n+}\n+\n+// We should reset the number of control frames seen, enabling us to send\n+// more without hitting the rate limit\n+flushRequestsAndLoopN(2);\n+\n+// Send 10 control frames. This is just within the rate limits that we have\n+// set.\n+for (int i = 0; i < 5; i++) {\n+clientCodec_->generatePriority(\n+requests_, streamid, HTTPMessage::HTTPPriority(0, false, 3));\n+}\n+\n+clientCodec_->generateSettings(requests_);\n+\n+for (int i = 0; i < 4; i++) {\n+clientCodec_->generatePingRequest(requests_);\n+}\n+\n+flushRequestsAndLoopN(2);\n+\n+httpSession_->closeWhenIdle();\n+expectDetachSession();\n+this->eventBase_.loop();\n+}\n+\n+TEST_F(HTTP2DownstreamSessionTest, DirectErrorHandlingLimitTouched) {\n+httpSession_->setMaxDirectErrorHandlingPerInterval(10);\n+httpSession_->setDirectErrorHandlingIntervalDuration(0);\n+\n+// Send ten messages, each of which cause direct error handling. Since\n+// this doesn't exceed the limit, this should not cause the connection\n+// to be dropped.\n+for (int i = 0; i < 10; i++) {\n+auto req = getGetRequest();\n+// Invalid method, causes the error to be handled directly\n+req.setMethod(\"11111111\");\n+sendRequest(req, false);\n+}\n+\n+EXPECT_CALL(mockController_, getParseErrorHandler(_, _, _)).\n+WillRepeatedly(Return(nullptr));\n+\n+flushRequestsAndLoop();\n+\n+for (int i = 0; i < 10; i++) {\n+auto req = getGetRequest();\n+// Invalid method, causes the error to be handled directly\n+req.setMethod(\"11111111\");\n+sendRequest(req, false);\n+}\n+\n+EXPECT_CALL(mockController_, getParseErrorHandler(_, _, _)).\n+WillRepeatedly(Return(nullptr));\n+\n+flushRequestsAndLoop();\n+gracefulShutdown();\n+}\n+\n+TEST_F(HTTP2DownstreamSessionTest, DirectErrorHandlingLimitExceeded) {\n+httpSession_->setMaxDirectErrorHandlingPerInterval(10);\n+httpSession_->setDirectErrorHandlingIntervalDuration(0);\n+\n+// Send eleven messages, each of which causes direct error handling. Since\n+// this exceeds the limit, the connection should be dropped.\n+for (int i = 0; i < 11; i++) {\n+auto req = getGetRequest();\n+// Invalid method, causes the error to be handled directly\n+req.setMethod(\"11111111\");\n+sendRequest(req, false);\n+}\n+\n+EXPECT_CALL(mockController_, getParseErrorHandler(_, _, _)).\n+WillRepeatedly(Return(nullptr));\n+\n+expectDetachSession();\n+flushRequestsAndLoopN(2);\n+}\n--\n2.17.1\n"}
{"Commit title": "Fix race between sync signals and fork", "Commit body": "@@ -127,6 +127,7 @@ void* handle_signals(void*) {\n\n//Clear state in child process after fork().\nvoidpostfork_clear() {\nreset_sync_signals();\ng_handler_thread_started.store(false, std::memory_order_release);\ng_handler_thread =pthread_t{};\n}\n@@ -140,7 +141,7 @@ void block_sync_signals() {\n//If we ever fork(), avoid affecting child processes.\nstaticstd::atomic_flag flag ATOMIC_FLAG_INIT;\nif(!flag.test_and_set()) {\npthread_atfork(reset_sync_signals, block_sync_signals, postfork_clear);\npthread_atfork(nullptr,nullptr, postfork_clear);\n}\n}\n\n"}
{"Commit title": "Fix memory leak in PDOStatement::fetchAll()", "Commit body": "@@ -2865,7 +2865,7 @@ Variant c_PDOStatement::t_fetchall(int64_t how /* = 0 */,\nif(!error) {\nif((how & PDO_FETCH_GROUP)) {\ndo{\ndata.reset();\ndata.unset();\n}while(do_fetch(m_stmt,true, data, (PDOFetchType)(how | flags),\nPDO_FETCH_ORI_NEXT,0, return_all));\n}elseif(how == PDO_FETCH_KEY_PAIR ||\n@@ -2877,7 +2877,7 @@ Variant c_PDOStatement::t_fetchall(int64_t how /* = 0 */,\nreturn_value =Array::Create();\ndo{\nreturn_value.append(data);\ndata.reset();\ndata.unset();\n}while(do_fetch(m_stmt,true, data, (PDOFetchType)(how | flags),\nPDO_FETCH_ORI_NEXT,0,NULL));\n}\n"}
{"Commit title": "Add lock for calling timelib on Mac.", "Commit body": "@@ -26,6 +26,7 @@\n#include\"hphp/runtime/base/type-conversions.h\"\n\n#include\"hphp/util/functional.h\"\n#include\"hphp/util/lock.h\"\n#include\"hphp/util/logger.h\"\n#include\"hphp/util/text-util.h\"\n\n@@ -98,11 +99,20 @@ using TimeZoneValidityCacheEntry = std::pair<const char*, bool>;\n\nTimeZoneValidityCache* s_tzvCache;\n\n//Mac's setlocale() is not thread safe, so a lock is needed when calling\n//timelib_timezone_id_is_valid().\n#ifdef__APPLE__\nMutex *s_tzvMutex =nullptr;\n#endif\n\nvoidtimezone_init() {\n//Allocate enough space to cache all possible timezones, if needed.\nconstexprsize_tkMaxTimeZoneCache=1000;\ns_tzCache =TimeZoneCache::create(kMaxTimeZoneCache).release();\ns_tzvCache =TimeZoneValidityCache::create(kMaxTimeZoneCache).release();\n#ifdef__APPLE__\ns_tzvMutex =newMutex();\n#endif\n}\n\nconsttimelib_tzdb *TimeZone::GetDatabase() {\n@@ -183,6 +193,9 @@ bool TimeZone::SetCurrent(const String& zone) {\nif(it != s_tzvCache->end()) {\nvalid = it->second;\n}else{\n#ifdef__APPLE__\nLocklock(*s_tzvMutex);\n#endif\nvalid =IsValid(zone);\n\nautokey =strdup(name);\n"}
{"Commit title": "Cache time zone validity and add lock for calling timelib on Mac.", "Commit body": "@@ -26,6 +26,7 @@\n#include\"hphp/runtime/base/type-conversions.h\"\n\n#include\"hphp/util/functional.h\"\n#include\"hphp/util/lock.h\"\n#include\"hphp/util/logger.h\"\n#include\"hphp/util/text-util.h\"\n\n@@ -92,10 +93,26 @@ using TimeZoneCacheEntry = std::pair<const char*, timelib_tzinfo*>;\n\nTimeZoneCache* s_tzCache;\n\nusingTimeZoneValidityCache =\nfolly::AtomicHashArray<constchar*,bool, cstr_hash, ahm_eqstr>;\nusingTimeZoneValidityCacheEntry = std::pair<constchar*,bool>;\n\nTimeZoneValidityCache* s_tzvCache;\n\n//Mac's setlocale() is not thread safe, so a lock is needed when calling\n//timelib_timezone_id_is_valid().\n#ifdef__APPLE__\nMutex *s_tzvMutex =nullptr;\n#endif\n\nvoidtimezone_init() {\n//Allocate enough space to cache all possible timezones, if needed.\nconstexprsize_tkMaxTimeZoneCache=1000;\ns_tzCache =TimeZoneCache::create(kMaxTimeZoneCache).release();\ns_tzvCache =TimeZoneValidityCache::create(kMaxTimeZoneCache).release();\n#ifdef__APPLE__\ns_tzvMutex =newMutex();\n#endif\n}\n\nconsttimelib_tzdb *TimeZone::GetDatabase() {\n@@ -170,7 +187,28 @@ SmartPtr<TimeZone> TimeZone::Current() {\n}\n\nboolTimeZone::SetCurrent(constString& zone) {\nif(!IsValid(zone)) {\nboolvalid;\nconstchar* name = zone.data();\nautoconstit = s_tzvCache->find(name);\nif(it != s_tzvCache->end()) {\nvalid = it->second;\n}else{\n#ifdef__APPLE__\nLocklock(*s_tzvMutex);\n#endif\nvalid =IsValid(zone);\n\nautokey =strdup(name);\nautoresult = s_tzvCache->insert(TimeZoneValidityCacheEntry(name, valid));\nif(!result.second) {\n//The cache should never fill up since zones are finite.\nalways_assert(result.first!= s_tzvCache->end());\n//A collision occurred, so we don't need our strdup'ed key.\nfree(key);\n}\n}\n\nif(!valid) {\nraise_notice(\"Timezone ID '%s' is invalid\", zone.data());\nreturnfalse;\n}\n"}
{"Commit title": "We branch on GCC for performance.", "Commit body": "@@ -51,6 +51,7 @@ struct simdjson2msgpack {\nsimdjson_inlinevoid\nwrite_raw_string(simdjson::ondemand::raw_json_string rjs);\ninlinevoidrecursive_processor(simdjson::ondemand::value element);\ninlinevoidrecursive_processor_ref(simdjson::ondemand::value& element);\n\nsimdjson::ondemand::parser parser;\nuint8_t*buff{};\n@@ -88,7 +89,16 @@ simdjson2msgpack::to_msgpack(const simdjson::padded_string &json,\n}\n}else{\nsimdjson::ondemand::value val = doc;\n#defineSIMDJSON_GCC_COMPILER((__GNUC__) && !(__clang__) && !(__INTEL_COMPILER))\n#ifSIMDJSON_GCC_COMPILER\n//the GCC compiler does well with by-value passing.\n//GCC has superior recursive inlining:\n//https://stackoverflow.com/questions/29186186/why-does-gcc-generate-a-faster-program-than-clang-in-this-recursive-fibonacci-co\n//https://godbolt.org/z/TeK4doE51\nrecursive_processor(val);\n#else\nrecursive_processor_ref(val);\n#endif\n}\nif(doc.current_location().error() == simdjson::SUCCESS) {\n//Example of error detection - this won't be reached on twitter.json in the benchmark.\n@@ -175,6 +185,53 @@ void simdjson2msgpack::recursive_processor(simdjson::ondemand::value element) {\n}\n}\n\n\nvoidsimdjson2msgpack::recursive_processor_ref(simdjson::ondemand::value& element) {\nswitch(element.type()) {\ncasesimdjson::ondemand::json_type::array: {\nuint32_tcounter =0;\nwrite_byte(0xdd);\nuint8_t*location =skip_uint32();\nfor(autochild : element.get_array()) {\ncounter++;\nsimdjson::ondemand::value v = child.value();\nrecursive_processor_ref(v);\n}\nwrite_uint32_at(counter, location);\n}break;\ncasesimdjson::ondemand::json_type::object: {\nuint32_tcounter =0;\nwrite_byte(0xdf);\nuint8_t*location =skip_uint32();\nfor(autofield : element.get_object()) {\ncounter++;\nwrite_raw_string(field.key());\nsimdjson::ondemand::value v = field.value();\nrecursive_processor_ref(v);\n}\nwrite_uint32_at(counter, location);\n}break;\ncasesimdjson::ondemand::json_type::number:\nwrite_double(element.get_double());\nbreak;\ncasesimdjson::ondemand::json_type::string:\nwrite_raw_string(element.get_raw_json_string());\nbreak;\ncasesimdjson::ondemand::json_type::boolean:\nwrite_byte(0xc2+ element.get_bool());\nbreak;\ncasesimdjson::ondemand::json_type::null:\n//We check that the value is indeed null\n//otherwise: an error is thrown.\nif(element.is_null()) {\nwrite_byte(0xc0);\n}\nbreak;\ndefault:\nSIMDJSON_UNREACHABLE();\n}\n}\n\nstructsimdjson_ondemand{\nusingStringType = std::string_view;\n\n"}
{"Commit title": "Including the array header.", "Commit body": "@@ -1,6 +1,6 @@\n#include<cmath>\n#include<cstdint>\n\n#include<array>\nnamespacesimdjson{\n/*!\nimplements the Grisu2 algorithm for binary to decimal floating-point\n"}
{"Commit title": "Thinking about making the string buffer ASCII and moving the length o…", "Commit body": "@@ -89,20 +89,45 @@ really_inline bool parser::on_null_atom() noexcept {\nreally_inlineuint8_t*parser::on_start_string()noexcept{\n/*we advance the point, accounting for the fact that we have a NULL\n* termination*/\n//If we limit JSON documents to strictly less 4GB of\n//string content, then current_string_buf_loc\n//- doc.string_buf.get() fits in 32 bits. This leaves us\n//three free bytes.\nwrite_tape(current_string_buf_loc - doc.string_buf.get(), internal::tape_type::STRING);\nreturncurrent_string_buf_loc +sizeof(uint32_t);\nreturncurrent_string_buf_loc +sizeof(uint16_t);\n}\n\nreally_inlineboolparser::on_end_string(uint8_t*dst)noexcept{\nuint32_tstr_length =uint32_t(dst - (current_string_buf_loc +sizeof(uint32_t)));\n//TODO check for overflow in case someone has a crazy string (>=4GB?)\n//But only add the overflow check when the document itself exceeds 4GB\n//Currently unneeded because we refuse to parse docs larger or equal to 4GB.\nmemcpy(current_string_buf_loc, &str_length,sizeof(uint32_t));\n\n//We have two scenarios here. Either the string length is\n//less than 0x7fffff in which case, we have room in the string\n//header and all is good. Otherwise, we can encode the\n//string length in the document itself, taking care to\n//ensure that we do so in ASCII.\nif(likely(str_length <=0x7fffff)) {//likely\ndoc.tape[current_loc-1] |=uint64_t(str_length) <<32;\n//we have a string header that must be ASCII, unused in\n//this common case\ncurrent_string_buf_loc[0] =uint8_t(32);//space\ncurrent_string_buf_loc[1] =uint8_t(32);//space\n//we are done!\n}else{\n//oh gosh, we have a long string.\ndoc.tape[current_loc-1] |=uint64_t(0x800000| (str_length >>9)) <<32;\n//we have 9 bits left to code, which we do on the string buffer\n//using two bytes\ncurrent_string_buf_loc[0] =uint8_t(32+ ((str_length &0x1f0) >>4))\ncurrent_string_buf_loc[1] =32+uint8_t(str_length &0xf);\n}\n//NULL termination is still handy if you expect all your strings to\n//be NULL terminated? It comes at a small cost\n*dst =0;\ncurrent_string_buf_loc = dst +1;\n//be NULL terminated? It comes at a small cost and if it is\n//never used, we might as well drop it.\n//*dst = 0;\n//current_string_buf_loc = dst + 1;\nreturntrue;\n}\n\n"}
{"Commit title": "This seems beneficial.", "Commit body": "@@ -435,24 +435,52 @@ really_inline void flatten_bits(uint32_t *base_ptr, uint32_t &base,\nuint32_tidx,uint64_tbits) {\nuint32_tcnt =hamming(bits);\nuint32_tnext_base = base + cnt;\nwhile(bits !=0u) {\nbase_ptr[base +0] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nidx -=64;\nbase_ptr += base;\nif(bits !=0) {\nbase_ptr[0] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +1] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[1] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +2] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[2] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +3] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[3] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +4] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[4] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +5] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[5] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +6] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[6] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +7] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[7] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase +=8;\nbase_ptr +=8;\n}\nif(cnt >8) {\nbase_ptr[0] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[1] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[2] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[3] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[4] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[5] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[6] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[7] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr +=8;\n}\nif(cnt >16) {//unluckly\ndo{\nbase_ptr[0] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr++;\n}while(bits !=0);\n}\nbase = next_base;\n}\n"}
{"Commit title": "Moving car examples to exception mode", "Commit body": "@@ -379,9 +379,9 @@ bool run() {\nminify_demo() &&\nminify_demo2() &&\nminify_test() &&\ncar_example() &&\n#endif//SIMDJSON_EXCEPTIONS\nminify_exceptionless_test() &&\ncar_example() &&\ntrue;\n}\n\n"}
{"Commit title": "Update basic.md to document JSON pointer for On Demand.", "Commit body": "@@ -523,8 +523,12 @@ JSON Pointer\n------------\n\nThe simdjson library also supports[JSON pointer](https://tools.ietf.org/html/rfc6901)through the\n`at_pointer()`method, letting you reach further down into the document in a single call:\n`at_pointer()`method, letting you reach further down into the document in a single call. JSON pointer is supported by both the DOM approach as well\nas the On Demand approach.\n\n**Note:**The On Demand implementation of JSON pointer relies on`find_field`which implies that it does not unescape keys when matching.\n\n*DOM:\n```c++\nautocars_json =R\"([\n{ \"make\": \"Toyota\", \"model\": \"Camry\",  \"year\": 2018, \"tire_pressure\": [ 40.1, 39.9, 37.7, 40.4 ] },\n@@ -536,6 +540,18 @@ dom::element cars = parser.parse(cars_json);\ncout << cars.at_pointer(\"/0/tire_pressure/1\") << endl;//Prints 39.9\n```\n\n*On Demand:\n```c++\nautocars_json =R\"([\n{ \"make\": \"Toyota\", \"model\": \"Camry\",  \"year\": 2018, \"tire_pressure\": [ 40.1, 39.9, 37.7, 40.4 ] },\n{ \"make\": \"Kia\",    \"model\": \"Soul\",   \"year\": 2012, \"tire_pressure\": [ 30.1, 31.0, 28.6, 28.7 ] },\n{ \"make\": \"Toyota\", \"model\": \"Tercel\", \"year\": 1999, \"tire_pressure\": [ 29.8, 30.0, 30.2, 30.5 ] }\n])\"_padded;\nondemand::parser parser;\nautocars = parser.iterate(cars_json);\ncout << cars.at_pointer(\"/0/tire_pressure/1\") << endl;//Prints 39.9\n```\n\nA JSON Path is a sequence of segments each starting with the '/' character. Within arrays, an integer\nindexallows you to select the indexed node. Within objects, the string value of the key allows you to\nselectthe value. If your keys contain the characters '/' or '~', they must be escaped as '~1' and\n@@ -546,6 +562,7 @@ You can apply a JSON path to any node and the path gets interpreted relatively,\n\nConsider the following example:\n\n*DOM:\n```c++\nautocars_json =R\"([\n{ \"make\": \"Toyota\", \"model\": \"Camry\",  \"year\": 2018, \"tire_pressure\": [ 40.1, 39.9, 37.7, 40.4 ] },\n@@ -564,6 +581,63 @@ for (dom::element car_element : cars) {\n}\n```\n\n*On Demand:\n```c++\nautocars_json =R\"([\n{ \"make\": \"Toyota\", \"model\": \"Camry\",  \"year\": 2018, \"tire_pressure\": [ 40.1, 39.9, 37.7, 40.4 ] },\n{ \"make\": \"Kia\",    \"model\": \"Soul\",   \"year\": 2012, \"tire_pressure\": [ 30.1, 31.0, 28.6, 28.7 ] },\n{ \"make\": \"Toyota\", \"model\": \"Tercel\", \"year\": 1999, \"tire_pressure\": [ 29.8, 30.0, 30.2, 30.5 ] }\n])\"_padded;\nondemand::parser parser;\nautocars = parser.iterate(cars_json);\nfor(autocar_element : cars) {\nsimdjson::error_code error;\ndouble x;\nif ((error = car_element.at_pointer(\"/tire_pressure/1\").get(x))) { std::cerr << error << std::endl; return; }\nstd::cout << x << std::endl; // Prints 39.9, 31 and 30\n}\n```\n\nFor multiple JSON pointer queries, one can call`at_pointer`multiple times with DOM. However, with On Demand,`rewind`should be called\non the document between each`at_pointer`call to reset the iterator to point at the beggining at of the document:\n\n*DOM:\n```c++\nautocars_json =R\"([\n{ \"make\": \"Toyota\", \"model\": \"Camry\",  \"year\": 2018, \"tire_pressure\": [ 40.1, 39.9, 37.7, 40.4 ] },\n{ \"make\": \"Kia\",    \"model\": \"Soul\",   \"year\": 2012, \"tire_pressure\": [ 30.1, 31.0, 28.6, 28.7 ] },\n{ \"make\": \"Toyota\", \"model\": \"Tercel\", \"year\": 1999, \"tire_pressure\": [ 29.8, 30.0, 30.2, 30.5 ] }\n])\"_padded;\ndom::parser parser;\ndom::element cars = parser.parse(cars_json);\nsize_tsize = array(cars).size();\n\nfor(size_ti =0; i < size; i++) {\nstd::string json_pointer = \"/\" + std::to_string(i) + \"/tire_pressure/1\";\ndouble x = cars.at_pointer(json_pointer);\nstd::cout << x << std::endl; // Prints 39.9, 31 and 30\n}\n```\n\n*On Demand:\n```c++\nautocars_json =R\"([\n{ \"make\": \"Toyota\", \"model\": \"Camry\",  \"year\": 2018, \"tire_pressure\": [ 40.1, 39.9, 37.7, 40.4 ] },\n{ \"make\": \"Kia\",    \"model\": \"Soul\",   \"year\": 2012, \"tire_pressure\": [ 30.1, 31.0, 28.6, 28.7 ] },\n{ \"make\": \"Toyota\", \"model\": \"Tercel\", \"year\": 1999, \"tire_pressure\": [ 29.8, 30.0, 30.2, 30.5 ] }\n])\"_padded;\nondemand::parser parser;\nautocars = parser.iterate(cars_json);\nsize_tsize = cars.count_elements();\n\nfor(size_ti =0; i < size; i++) {\nstd::string json_pointer = \"/\" + std::to_string(i) + \"/tire_pressure/1\";\ndouble x = cars.at_pointer(json_pointer);\nstd::cout << x << std::endl; // Prints 39.9, 31 and 30\ncars.rewind();\n}\n```\n\n\n\nError Handling\n"}
{"Commit title": "This makes the float errors explicit.", "Commit body": "@@ -2,6 +2,7 @@\n\n#include<vector>\n#include<sstream>\n#include<limits>\n\ntemplate<typenameT>\nstaticbooldiff_results(benchmark::State &state,constT &result,constT &reference);\n@@ -19,6 +20,24 @@ struct result_differ {\n}\n};\n\ntemplate<>\nboolresult_differ<double>::diff(benchmark::State &state,constdouble&result,constdouble&reference) {\nif(result != reference) {\nstd::stringstream str;\n//We print it out using full precision.\nautoprior_precision = str.precision(std::numeric_limits<double>::max_digits10);\nstr <<\"result incorrect:\"<< result <<\"... reference:\"<< reference;\nstr.precision(prior_precision);//reset to prior state\nstr << std::hexfloat;//If there are floats, we want to see them in hexadecimal form!\nstr <<\"result incorrect (hexadecimal notation):\"<< result <<\"... reference:\"<< reference;\nstr << std::defaultfloat;//reset to prior state\nstate.SkipWithError(str.str().data());\nreturnfalse;\n}\nreturntrue;\n}\n\n\ntemplate<typenameT>\nstructresult_differ<std::vector<T>> {\nstaticbooldiff(benchmark::State &state,conststd::vector<T> &result,conststd::vector<T> &reference) {\n"}
{"Commit title": "Including the array header.", "Commit body": "@@ -1,6 +1,6 @@\n#include<cmath>\n#include<cstdint>\n\n#include<array>\nnamespacesimdjson{\n/*!\nimplements the Grisu2 algorithm for binary to decimal floating-point\n"}
{"Commit title": "This makes the float errors explicit.", "Commit body": "@@ -2,6 +2,7 @@\n\n#include<vector>\n#include<sstream>\n#include<limits>\n\ntemplate<typenameT>\nstaticbooldiff_results(benchmark::State &state,constT &result,constT &reference);\n@@ -19,6 +20,24 @@ struct result_differ {\n}\n};\n\ntemplate<>\nboolresult_differ<double>::diff(benchmark::State &state,constdouble&result,constdouble&reference) {\nif(result != reference) {\nstd::stringstream str;\n//We print it out using full precision.\nautoprior_precision = str.precision(std::numeric_limits<double>::max_digits10);\nstr <<\"result incorrect:\"<< result <<\"... reference:\"<< reference;\nstr.precision(prior_precision);//reset to prior state\nstr << std::hexfloat;//If there are floats, we want to see them in hexadecimal form!\nstr <<\"result incorrect (hexadecimal notation):\"<< result <<\"... reference:\"<< reference;\nstr << std::defaultfloat;//reset to prior state\nstate.SkipWithError(str.str().data());\nreturnfalse;\n}\nreturntrue;\n}\n\n\ntemplate<typenameT>\nstructresult_differ<std::vector<T>> {\nstaticbooldiff(benchmark::State &state,conststd::vector<T> &result,conststd::vector<T> &reference) {\n"}
{"Commit title": "Fix SAX benchmark to actually add tweets", "Commit body": "@@ -233,6 +233,7 @@ simdjson_really_inline bool sax_tweet_reader_visitor::in_container_child(json_it\nsimdjson_really_inlinevoidsax_tweet_reader_visitor::start_container(json_iterator &iter) {\nSIMDJSON_ASSUME(iter.depth<= MAX_SUPPORTED_DEPTH);//Asserts in debug mode\ncontainer =containers(iter.depth);\nif(container == containers::tweet) { tweets.push_back({}); }\nif(logger::LOG_ENABLED) { iter.log_start_value(STATE_NAMES[iter.depth]); }\n}\nsimdjson_really_inlinevoidsax_tweet_reader_visitor::end_container(json_iterator &iter) {\n"}
{"Commit title": "Was missing a stupid == 0 check which could lead to infinite loops. Duh!", "Commit body": "@@ -355,6 +355,13 @@ void decimal_right_shift(decimal &h, uint32_t shift) {\n\ntemplate<typenamebinary> adjusted_mantissacompute_float(decimal &d) {\nadjusted_mantissa answer;\nif(d.num_digits==0) {\n//should be zero\nanswer.power2=0;\nanswer.mantissa=0;\nreturnanswer;\n}\n//At this point, going further, we can assume that d.num_digits > 0.\n\nstaticconstuint32_tmax_shift =60;\nstaticconstuint32_tnum_powers =19;\n"}
{"Commit title": "Tweaking.", "Commit body": "@@ -142,7 +142,8 @@ static void GenStatPlus(Stat &stat, const dom::element v) {\nbreak;\ncasedom::element_type::OBJECT:\nfor(dom::key_value_pair kv :dom::object(v)) {\nGenStatPlus(stat,dom::element(kv.value));\nGenStatPlus(stat, kv.value);\nstat.stringLength+= kv.key.size();\nstat.memberCount++;\nstat.stringCount++;\n}\n"}
{"Commit title": "Fix SAX benchmark to actually add tweets", "Commit body": "@@ -233,6 +233,7 @@ simdjson_really_inline bool sax_tweet_reader_visitor::in_container_child(json_it\nsimdjson_really_inlinevoidsax_tweet_reader_visitor::start_container(json_iterator &iter) {\nSIMDJSON_ASSUME(iter.depth<= MAX_SUPPORTED_DEPTH);//Asserts in debug mode\ncontainer =containers(iter.depth);\nif(container == containers::tweet) { tweets.push_back({}); }\nif(logger::LOG_ENABLED) { iter.log_start_value(STATE_NAMES[iter.depth]); }\n}\nsimdjson_really_inlinevoidsax_tweet_reader_visitor::end_container(json_iterator &iter) {\n"}
{"Commit title": "Switching ARM64 to lookup algo. for UTF8 validation.", "Commit body": "@@ -29,7 +29,7 @@ really_inline void find_whitespace_and_operators(\nwhitespace = v.map([&](simd8<uint8_t> _v) {return_v.any_bits_set(0x18); }).to_bitmask();\n}\n\n#include\"generic/utf8_fastvalidate_algorithm.h\"\n#include\"generic/utf8_lookup_algorithm.h\"\n#include\"generic/stage1_find_marks.h\"\n\n}//namespace simdjson::arm64\n"}
{"Commit title": "Minor tweak.", "Commit body": "@@ -460,8 +460,8 @@ really_inline bool parse_number(const uint8_t *const src, W &writer) {\nif(!parse_exponent(src, p, exponent)) {returnfalse; }\n}\nif(is_float) {\nreturnwrite_float(src, negative, i, start_digits, digit_count, exponent, writer)\n&&is_structural_or_whitespace(*p);\nconstboolclean_end =is_structural_or_whitespace(*p);\nreturnwrite_float(src, negative, i, start_digits, digit_count, exponent, writer)&&clean_end;\n}\n\n//The longest negative 64-bit number is 19 digits.\n"}
{"Commit title": "Return bool from compute_float_64", "Commit body": "@@ -24,7 +24,7 @@ namespace numberparsing {\n//set to false. This should work *most of the time* (like 99% of the time).\n//We assume that power is in the [FASTFLOAT_SMALLEST_POWER,\n//FASTFLOAT_LARGEST_POWER] interval: the caller is responsible for this check.\nreally_inlinedoublecompute_float_64(int64_tpower,uint64_ti,boolnegative,bool*success) {\nreally_inlineboolcompute_float_64(int64_tpower,uint64_ti,boolnegative,double&d) {\n//we start with a fast path\n//It was described in\n//Clinger WD. How to read floating point numbers accurately.\n@@ -40,7 +40,7 @@ really_inline double compute_float_64(int64_t power, uint64_t i, bool negative,\n#endif\n//convert the integer into a double. This is lossless since\n//0 <= i <= 2^53 - 1.\ndoubled =double(i);\nd =double(i);\n//\n//The general idea is as follows.\n//If 0 <= s < 2^53 and if 10^0 <= p <= 10^22 then\n@@ -59,8 +59,7 @@ really_inline double compute_float_64(int64_t power, uint64_t i, bool negative,\nif(negative) {\nd = -d;\n}\n*success =true;\nreturnd;\nreturntrue;\n}\n//When 22 < power && power <  22 + 16, we could\n//hope for another, secondary fast path.  It wa\n@@ -85,7 +84,8 @@ really_inline double compute_float_64(int64_t power, uint64_t i, bool negative,\n//In the slow path, we need to adjust i so that it is > 1<<63 which is always\n//possible, except if i == 0, so we handle i == 0 separately.\nif(i ==0) {\nreturn0.0;\nd =0.0;\nreturntrue;\n}\n\n//We are going to need to do some 64-bit arithmetic to get a more precise product.\n@@ -135,8 +135,7 @@ really_inline double compute_float_64(int64_t power, uint64_t i, bool negative,\n//This does happen, e.g. with 7.3177701707893310e+15.\nif(((product_middle +1==0) && ((product_high &0x1FF) ==0x1FF) &&\n(product_low + i < product_low))) {//let us be prudent and bail out.\n*success =false;\nreturn0;\nreturnfalse;\n}\nupper = product_high;\nlower = product_middle;\n@@ -157,25 +156,24 @@ really_inline double compute_float_64(int64_t power, uint64_t i, bool negative,\n//floating-point values.\nif(unlikely((lower ==0) && ((upper &0x1FF) ==0) &&\n((mantissa &3) ==1))) {\n//if mantissa & 1 == 1 we might need to round up.\n//\n//Scenarios:\n//1. We are not in the middle. Then we should round up.\n//\n//2. We are right in the middle. Whether we round up depends\n//on the last significant bit: if it is \"one\" then we round\n//up (round to even) otherwise, we do not.\n//\n//So if the last significant bit is 1, we can safely round up.\n//Hence we only need to bail out if (mantissa & 3) == 1.\n//Otherwise we may need more accuracy or analysis to determine whether\n//we are exactly between two floating-point numbers.\n//It can be triggered with 1e23.\n//Note: because the factor_mantissa and factor_mantissa_low are\n//almost always rounded down (except for small positive powers),\n//almost always should round up.\n*success =false;\nreturn0;\n//if mantissa & 1 == 1 we might need to round up.\n//\n//Scenarios:\n//1. We are not in the middle. Then we should round up.\n//\n//2. We are right in the middle. Whether we round up depends\n//on the last significant bit: if it is \"one\" then we round\n//up (round to even) otherwise, we do not.\n//\n//So if the last significant bit is 1, we can safely round up.\n//Hence we only need to bail out if (mantissa & 3) == 1.\n//Otherwise we may need more accuracy or analysis to determine whether\n//we are exactly between two floating-point numbers.\n//It can be triggered with 1e23.\n//Note: because the factor_mantissa and factor_mantissa_low are\n//almost always rounded down (except for small positive powers),\n//almost always should round up.\nreturnfalse;\n}\n\nmantissa += mantissa &1;\n@@ -193,15 +191,12 @@ really_inline double compute_float_64(int64_t power, uint64_t i, bool negative,\nuint64_treal_exponent = c.exp- lz;\n//we have to check that real_exponent is in range, otherwise we bail out\nif(unlikely((real_exponent <1) || (real_exponent >2046))) {\n*success =false;\nreturn0;\nreturnfalse;\n}\nmantissa |= real_exponent <<52;\nmantissa |= (((uint64_t)negative) <<63);\ndoubled;\nmemcpy(&d, &mantissa,sizeof(d));\n*success =true;\nreturnd;\nreturntrue;\n}\n\nstaticboolparse_float_strtod(constuint8_t*ptr,double*outDouble) {\n@@ -392,9 +387,8 @@ really_inline error_code write_float(const uint8_t *const src, bool negative, ui\nwriter.skip_double();\nreturnerror;\n}\nboolsuccess =true;\ndoubled =compute_float_64(exponent, i, negative, &success);\nif(!success) {\ndoubled;\nif(!compute_float_64(exponent, i, negative, d)) {\n//we are almost never going to get here.\nif(!parse_float_strtod(src, &d)) {returnINVALID_NUMBER(src); }\n}\n@@ -713,12 +707,10 @@ UNUSED really_inline simdjson_result<double> parse_double(const uint8_t * src) n\n//\n//Assemble (or slow-parse) the float\n//\ndoubled;\nif(likely(!overflow)) {\nboolsuccess =true;\ndoubled =compute_float_64(exponent, i, negative, &success);\nif(success) {returnd; }\nif(compute_float_64(exponent, i, negative, d)) {returnd; }\n}\ndoubled;\nif(!parse_float_strtod(src-negative, &d)) {\nreturnNUMBER_ERROR;\n}\n"}
{"Commit title": "Fix arm64 build", "Commit body": "@@ -140,7 +140,7 @@ WARN_UNUSED error_code dom_parser_implementation::stage1(const uint8_t *_buf, si\n}\n\nWARN_UNUSEDboolimplementation::validate_utf8(constchar*buf,size_tlen)constnoexcept{\nreturnsimdjson::arm64::stage1::generic_validate_utf8(buf,len);\nreturnarm64::stage1::generic_validate_utf8(buf,len);\n}\n\nWARN_UNUSED error_codedom_parser_implementation::stage2(dom::document &_doc)noexcept{\n"}
{"Commit title": "Adding explicit constructor.", "Commit body": "@@ -128,6 +128,7 @@ struct event_collector {\nreturnlinux_events.is_working();\n}\n#else\nevent_collector() {}\nboolhas_events() {\nreturnfalse;\n}\n"}
{"Commit title": "Some optimization.", "Commit body": "@@ -211,10 +211,17 @@ namespace utf8_validation {\n//possibly finish them.\nthis->error|=this->prev_incomplete;\n}else{\nthis->check_utf8_bytes(input.chunks[0],this->prev_input_block);\nfor(inti=1; i<simd8x64<uint8_t>::NUM_CHUNKS; i++) {\nthis->check_utf8_bytes(input.chunks[i], input.chunks[i-1]);\n}\n//you might think that a for-loop would work, but under Visual Studio, it is not good enough.\nstatic_assert((simd8x64<uint8_t>::NUM_CHUNKS ==2) || (simd8x64<uint8_t>::NUM_CHUNKS ==4));\nif(simd8x64<uint8_t>::NUM_CHUNKS ==2) {\nthis->check_utf8_bytes(input.chunks[0],this->prev_input_block);\nthis->check_utf8_bytes(input.chunks[1], input.chunks[0]);\n}elseif(simd8x64<uint8_t>::NUM_CHUNKS ==4) {\nthis->check_utf8_bytes(input.chunks[0],this->prev_input_block);\nthis->check_utf8_bytes(input.chunks[1], input.chunks[0]);\nthis->check_utf8_bytes(input.chunks[2], input.chunks[1]);\nthis->check_utf8_bytes(input.chunks[3], input.chunks[2]);\n}\nthis->prev_incomplete=is_incomplete(input.chunks[simd8x64<uint8_t>::NUM_CHUNKS-1]);\nthis->prev_input_block= input.chunks[simd8x64<uint8_t>::NUM_CHUNKS-1];\n}\n"}
{"Commit title": "Fix arm64 build", "Commit body": "@@ -140,7 +140,7 @@ WARN_UNUSED error_code dom_parser_implementation::stage1(const uint8_t *_buf, si\n}\n\nWARN_UNUSEDboolimplementation::validate_utf8(constchar*buf,size_tlen)constnoexcept{\nreturnsimdjson::arm64::stage1::generic_validate_utf8(buf,len);\nreturnarm64::stage1::generic_validate_utf8(buf,len);\n}\n\nWARN_UNUSED error_codedom_parser_implementation::stage2(dom::document &_doc)noexcept{\n"}
{"Commit title": "Adding a new flag to tell Visual Studio to include debugging information", "Commit body": "@@ -75,7 +75,7 @@ set(THREADS_PREFER_PTHREAD_FLAG ON)\n#set(CMAKE_INTERPROCEDURAL_OPTIMIZATION TRUE)\n#endif()\n\n\noption(SIMDJSON_VISUAL_STUDIO_BUILD_WITH_DEBUG_INFO_FOR_PROFILING\"Under Visual Studio, add Zi to the compile flag and DEBUG to the link file to add debugging information to the release build for easier profiling inside tools like VTune\"OFF)\nif(MSVC)\nif(\"${MSVC_TOOLSET_VERSION}\"STREQUAL\"140\")\n#Visual Studio 2015 issues warnings and we tolerate it,  cmake -G\"Visual Studio 14\" ..\n@@ -84,6 +84,10 @@ else()\n#Recent version of Visual Studio expected (2017, 2019...). Prior versions are unsupported.\ntarget_compile_options(simdjson-internal-flagsINTERFACE/WX /W3 /sdl)\nendif()\nif(SIMDJSON_VISUAL_STUDIO_BUILD_WITH_DEBUG_INFO_FOR_PROFILING)\ntarget_link_options(simdjson-flagsINTERFACE/DEBUG )\ntarget_compile_options(simdjson-flagsINTERFACE/Zi    )\nendif()\nelse()\ntarget_compile_options(simdjson-internal-flagsINTERFACE-fPIC)\ntarget_compile_options(simdjson-internal-flagsINTERFACE-Werror -Wall -Wextra -Weffc++)\n"}
{"Commit title": "Minor tweak.", "Commit body": "@@ -460,8 +460,8 @@ really_inline bool parse_number(const uint8_t *const src, W &writer) {\nif(!parse_exponent(src, p, exponent)) {returnfalse; }\n}\nif(is_float) {\nreturnwrite_float(src, negative, i, start_digits, digit_count, exponent, writer)\n&&is_structural_or_whitespace(*p);\nconstboolclean_end =is_structural_or_whitespace(*p);\nreturnwrite_float(src, negative, i, start_digits, digit_count, exponent, writer)&&clean_end;\n}\n\n//The longest negative 64-bit number is 19 digits.\n"}
{"Commit title": "Was missing a stupid == 0 check which could lead to infinite loops. Duh!", "Commit body": "@@ -355,6 +355,13 @@ void decimal_right_shift(decimal &h, uint32_t shift) {\n\ntemplate<typenamebinary> adjusted_mantissacompute_float(decimal &d) {\nadjusted_mantissa answer;\nif(d.num_digits==0) {\n//should be zero\nanswer.power2=0;\nanswer.mantissa=0;\nreturnanswer;\n}\n//At this point, going further, we can assume that d.num_digits > 0.\n\nstaticconstuint32_tmax_shift =60;\nstaticconstuint32_tnum_powers =19;\n"}
{"Commit title": "This makes the float errors explicit.", "Commit body": "@@ -2,6 +2,7 @@\n\n#include<vector>\n#include<sstream>\n#include<limits>\n\ntemplate<typenameT>\nstaticbooldiff_results(benchmark::State &state,constT &result,constT &reference);\n@@ -19,6 +20,24 @@ struct result_differ {\n}\n};\n\ntemplate<>\nboolresult_differ<double>::diff(benchmark::State &state,constdouble&result,constdouble&reference) {\nif(result != reference) {\nstd::stringstream str;\n//We print it out using full precision.\nautoprior_precision = str.precision(std::numeric_limits<double>::max_digits10);\nstr <<\"result incorrect:\"<< result <<\"... reference:\"<< reference;\nstr.precision(prior_precision);//reset to prior state\nstr << std::hexfloat;//If there are floats, we want to see them in hexadecimal form!\nstr <<\"result incorrect (hexadecimal notation):\"<< result <<\"... reference:\"<< reference;\nstr << std::defaultfloat;//reset to prior state\nstate.SkipWithError(str.str().data());\nreturnfalse;\n}\nreturntrue;\n}\n\n\ntemplate<typenameT>\nstructresult_differ<std::vector<T>> {\nstaticbooldiff(benchmark::State &state,conststd::vector<T> &result,conststd::vector<T> &reference) {\n"}
{"Commit title": "Thinking about making the string buffer ASCII and moving the length o…", "Commit body": "@@ -89,20 +89,45 @@ really_inline bool parser::on_null_atom() noexcept {\nreally_inlineuint8_t*parser::on_start_string()noexcept{\n/*we advance the point, accounting for the fact that we have a NULL\n* termination*/\n//If we limit JSON documents to strictly less 4GB of\n//string content, then current_string_buf_loc\n//- doc.string_buf.get() fits in 32 bits. This leaves us\n//three free bytes.\nwrite_tape(current_string_buf_loc - doc.string_buf.get(), internal::tape_type::STRING);\nreturncurrent_string_buf_loc +sizeof(uint32_t);\nreturncurrent_string_buf_loc +sizeof(uint16_t);\n}\n\nreally_inlineboolparser::on_end_string(uint8_t*dst)noexcept{\nuint32_tstr_length =uint32_t(dst - (current_string_buf_loc +sizeof(uint32_t)));\n//TODO check for overflow in case someone has a crazy string (>=4GB?)\n//But only add the overflow check when the document itself exceeds 4GB\n//Currently unneeded because we refuse to parse docs larger or equal to 4GB.\nmemcpy(current_string_buf_loc, &str_length,sizeof(uint32_t));\n\n//We have two scenarios here. Either the string length is\n//less than 0x7fffff in which case, we have room in the string\n//header and all is good. Otherwise, we can encode the\n//string length in the document itself, taking care to\n//ensure that we do so in ASCII.\nif(likely(str_length <=0x7fffff)) {//likely\ndoc.tape[current_loc-1] |=uint64_t(str_length) <<32;\n//we have a string header that must be ASCII, unused in\n//this common case\ncurrent_string_buf_loc[0] =uint8_t(32);//space\ncurrent_string_buf_loc[1] =uint8_t(32);//space\n//we are done!\n}else{\n//oh gosh, we have a long string.\ndoc.tape[current_loc-1] |=uint64_t(0x800000| (str_length >>9)) <<32;\n//we have 9 bits left to code, which we do on the string buffer\n//using two bytes\ncurrent_string_buf_loc[0] =uint8_t(32+ ((str_length &0x1f0) >>4))\ncurrent_string_buf_loc[1] =32+uint8_t(str_length &0xf);\n}\n//NULL termination is still handy if you expect all your strings to\n//be NULL terminated? It comes at a small cost\n*dst =0;\ncurrent_string_buf_loc = dst +1;\n//be NULL terminated? It comes at a small cost and if it is\n//never used, we might as well drop it.\n//*dst = 0;\n//current_string_buf_loc = dst + 1;\nreturntrue;\n}\n\n"}
{"Commit title": "This seems beneficial.", "Commit body": "@@ -435,24 +435,52 @@ really_inline void flatten_bits(uint32_t *base_ptr, uint32_t &base,\nuint32_tidx,uint64_tbits) {\nuint32_tcnt =hamming(bits);\nuint32_tnext_base = base + cnt;\nwhile(bits !=0u) {\nbase_ptr[base +0] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nidx -=64;\nbase_ptr += base;\nif(bits !=0) {\nbase_ptr[0] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +1] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[1] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +2] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[2] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +3] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[3] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +4] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[4] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +5] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[5] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +6] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[6] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +7] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[7] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase +=8;\nbase_ptr +=8;\n}\nif(cnt >8) {\nbase_ptr[0] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[1] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[2] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[3] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[4] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[5] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[6] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[7] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr +=8;\n}\nif(cnt >16) {//unluckly\ndo{\nbase_ptr[0] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr++;\n}while(bits !=0);\n}\nbase = next_base;\n}\n"}
{"Commit title": "This will change the default of the parse benchmark so that it work o…", "Commit body": "@@ -62,7 +62,8 @@ void print_usage(ostream& out) {\nout <<\"-v           - Verbose output.\"<< endl;\nout <<\"-s stage1    - Stop after find_structural_bits.\"<< endl;\nout <<\"-s all       - Run all stages.\"<< endl;\nout <<\"-H           - Make the buffers hot (reduce page allocation during parsing)\"<< endl;\nout <<\"-C           - Leave the buffers cold (includes page allocation and related OS tasks during parsing, speed tied to OS performance)\"<< endl;\nout <<\"-H           - Make the buffers hot (reduce page allocation and related OS tasks during parsing) [default]\"<< endl;\nout <<\"-a IMPL      - Use the given parser implementation. By default, detects the most advanced\"<< endl;\nout <<\"implementation supported on the host machine.\"<< endl;\nfor(autoimpl : simdjson::available_implementations) {\n@@ -86,12 +87,19 @@ struct option_struct {\n\nboolverbose =false;\nbooltabbed_output =false;\nboolhotbuffers =false;\n/**\n* Benchmarking on a cold parser instance means that the parsing may include\n* memory allocation at the OS level. This may lead to apparently odd results\n* such that higher speed under the Windows Subsystem for Linux than under the\n* regular Windows, for the same machine. It is arguably misleading to benchmark\n* how the OS allocates memory, when we really want to just benchmark simdjson.\n*/\nboolhotbuffers =true;\n\noption_struct(intargc,char**argv) {\nintc;\n\nwhile((c =getopt(argc, argv,\"vtn:i:a:s:H\")) != -1) {\nwhile((c =getopt(argc, argv,\"vtn:i:a:s:HC\")) != -1) {\nswitch(c) {\ncase'n':\niterations =atoi(optarg);\n@@ -118,6 +126,9 @@ struct option_struct {\nsimdjson::active_implementation = impl;\nbreak;\n}\ncase'C':\nhotbuffers =false;\nbreak;\ncase'H':\nhotbuffers =true;\nbreak;\n"}
{"Commit title": "Silencing warnings.", "Commit body": "@@ -37,9 +37,9 @@ static void parse_twitter(State& state) {\nbenchmark::DoNotOptimize(doc);\n}\nstate.counters[\"Bytes\"] =benchmark::Counter(\nbytes, benchmark::Counter::kIsRate,\ndouble(bytes), benchmark::Counter::kIsRate,\nbenchmark::Counter::OneK::kIs1024);\nstate.counters[\"docs\"] =Counter(state.iterations(), benchmark::Counter::kIsRate);\nstate.counters[\"docs\"] =Counter(double(state.iterations()), benchmark::Counter::kIsRate);\n}\nBENCHMARK(parse_twitter)->Repetitions(10)->ComputeStatistics(\"max\", [](conststd::vector<double>& v) -> double {\nreturn*(std::max_element(std::begin(v),std::end(v)));\n@@ -73,9 +73,9 @@ static void parse_gsoc(State& state) {\nbenchmark::DoNotOptimize(doc);\n}\nstate.counters[\"Bytes\"] =benchmark::Counter(\nbytes, benchmark::Counter::kIsRate,\ndouble(bytes), benchmark::Counter::kIsRate,\nbenchmark::Counter::OneK::kIs1024);\nstate.counters[\"docs\"] =Counter(state.iterations(),benchmark::Counter::kIsRate);\nstate.counters[\"docs\"] =Counter(double(state.iterations()),benchmark::Counter::kIsRate);\n}\nBENCHMARK(parse_gsoc)->Repetitions(10)->ComputeStatistics(\"max\", [](conststd::vector<double>& v) -> double {\nreturn*(std::max_element(std::begin(v),std::end(v)));\n@@ -111,6 +111,7 @@ static void parser_parse_exception(State& state) {\ntry{\nUNUSED dom::element doc = parser.parse(EMPTY_ARRAY);\n}catch(simdjson_error &j) {\ncout << j.what() << endl;\nreturn;\n}\n}\n@@ -141,6 +142,7 @@ static void document_parse_exception(State& state) {\ndom::parser parser;\nUNUSED dom::element doc = parser.parse(EMPTY_ARRAY);\n}catch(simdjson_error &j) {\ncout << j.what() << endl;\nreturn;\n}\n}\n"}
{"Commit title": "Switching ARM64 to lookup algo. for UTF8 validation.", "Commit body": "@@ -29,7 +29,7 @@ really_inline void find_whitespace_and_operators(\nwhitespace = v.map([&](simd8<uint8_t> _v) {return_v.any_bits_set(0x18); }).to_bitmask();\n}\n\n#include\"generic/utf8_fastvalidate_algorithm.h\"\n#include\"generic/utf8_lookup_algorithm.h\"\n#include\"generic/stage1_find_marks.h\"\n\n}//namespace simdjson::arm64\n"}
{"Commit title": "This makes the float errors explicit.", "Commit body": "@@ -2,6 +2,7 @@\n\n#include<vector>\n#include<sstream>\n#include<limits>\n\ntemplate<typenameT>\nstaticbooldiff_results(benchmark::State &state,constT &result,constT &reference);\n@@ -19,6 +20,24 @@ struct result_differ {\n}\n};\n\ntemplate<>\nboolresult_differ<double>::diff(benchmark::State &state,constdouble&result,constdouble&reference) {\nif(result != reference) {\nstd::stringstream str;\n//We print it out using full precision.\nautoprior_precision = str.precision(std::numeric_limits<double>::max_digits10);\nstr <<\"result incorrect:\"<< result <<\"... reference:\"<< reference;\nstr.precision(prior_precision);//reset to prior state\nstr << std::hexfloat;//If there are floats, we want to see them in hexadecimal form!\nstr <<\"result incorrect (hexadecimal notation):\"<< result <<\"... reference:\"<< reference;\nstr << std::defaultfloat;//reset to prior state\nstate.SkipWithError(str.str().data());\nreturnfalse;\n}\nreturntrue;\n}\n\n\ntemplate<typenameT>\nstructresult_differ<std::vector<T>> {\nstaticbooldiff(benchmark::State &state,conststd::vector<T> &result,conststd::vector<T> &reference) {\n"}
{"Commit title": "This makes the float errors explicit.", "Commit body": "@@ -2,6 +2,7 @@\n\n#include<vector>\n#include<sstream>\n#include<limits>\n\ntemplate<typenameT>\nstaticbooldiff_results(benchmark::State &state,constT &result,constT &reference);\n@@ -19,6 +20,24 @@ struct result_differ {\n}\n};\n\ntemplate<>\nboolresult_differ<double>::diff(benchmark::State &state,constdouble&result,constdouble&reference) {\nif(result != reference) {\nstd::stringstream str;\n//We print it out using full precision.\nautoprior_precision = str.precision(std::numeric_limits<double>::max_digits10);\nstr <<\"result incorrect:\"<< result <<\"... reference:\"<< reference;\nstr.precision(prior_precision);//reset to prior state\nstr << std::hexfloat;//If there are floats, we want to see them in hexadecimal form!\nstr <<\"result incorrect (hexadecimal notation):\"<< result <<\"... reference:\"<< reference;\nstr << std::defaultfloat;//reset to prior state\nstate.SkipWithError(str.str().data());\nreturnfalse;\n}\nreturntrue;\n}\n\n\ntemplate<typenameT>\nstructresult_differ<std::vector<T>> {\nstaticbooldiff(benchmark::State &state,conststd::vector<T> &result,conststd::vector<T> &reference) {\n"}
{"Commit title": "Return bool from compute_float_64", "Commit body": "@@ -24,7 +24,7 @@ namespace numberparsing {\n//set to false. This should work *most of the time* (like 99% of the time).\n//We assume that power is in the [FASTFLOAT_SMALLEST_POWER,\n//FASTFLOAT_LARGEST_POWER] interval: the caller is responsible for this check.\nreally_inlinedoublecompute_float_64(int64_tpower,uint64_ti,boolnegative,bool*success) {\nreally_inlineboolcompute_float_64(int64_tpower,uint64_ti,boolnegative,double&d) {\n//we start with a fast path\n//It was described in\n//Clinger WD. How to read floating point numbers accurately.\n@@ -40,7 +40,7 @@ really_inline double compute_float_64(int64_t power, uint64_t i, bool negative,\n#endif\n//convert the integer into a double. This is lossless since\n//0 <= i <= 2^53 - 1.\ndoubled =double(i);\nd =double(i);\n//\n//The general idea is as follows.\n//If 0 <= s < 2^53 and if 10^0 <= p <= 10^22 then\n@@ -59,8 +59,7 @@ really_inline double compute_float_64(int64_t power, uint64_t i, bool negative,\nif(negative) {\nd = -d;\n}\n*success =true;\nreturnd;\nreturntrue;\n}\n//When 22 < power && power <  22 + 16, we could\n//hope for another, secondary fast path.  It wa\n@@ -85,7 +84,8 @@ really_inline double compute_float_64(int64_t power, uint64_t i, bool negative,\n//In the slow path, we need to adjust i so that it is > 1<<63 which is always\n//possible, except if i == 0, so we handle i == 0 separately.\nif(i ==0) {\nreturn0.0;\nd =0.0;\nreturntrue;\n}\n\n//We are going to need to do some 64-bit arithmetic to get a more precise product.\n@@ -135,8 +135,7 @@ really_inline double compute_float_64(int64_t power, uint64_t i, bool negative,\n//This does happen, e.g. with 7.3177701707893310e+15.\nif(((product_middle +1==0) && ((product_high &0x1FF) ==0x1FF) &&\n(product_low + i < product_low))) {//let us be prudent and bail out.\n*success =false;\nreturn0;\nreturnfalse;\n}\nupper = product_high;\nlower = product_middle;\n@@ -157,25 +156,24 @@ really_inline double compute_float_64(int64_t power, uint64_t i, bool negative,\n//floating-point values.\nif(unlikely((lower ==0) && ((upper &0x1FF) ==0) &&\n((mantissa &3) ==1))) {\n//if mantissa & 1 == 1 we might need to round up.\n//\n//Scenarios:\n//1. We are not in the middle. Then we should round up.\n//\n//2. We are right in the middle. Whether we round up depends\n//on the last significant bit: if it is \"one\" then we round\n//up (round to even) otherwise, we do not.\n//\n//So if the last significant bit is 1, we can safely round up.\n//Hence we only need to bail out if (mantissa & 3) == 1.\n//Otherwise we may need more accuracy or analysis to determine whether\n//we are exactly between two floating-point numbers.\n//It can be triggered with 1e23.\n//Note: because the factor_mantissa and factor_mantissa_low are\n//almost always rounded down (except for small positive powers),\n//almost always should round up.\n*success =false;\nreturn0;\n//if mantissa & 1 == 1 we might need to round up.\n//\n//Scenarios:\n//1. We are not in the middle. Then we should round up.\n//\n//2. We are right in the middle. Whether we round up depends\n//on the last significant bit: if it is \"one\" then we round\n//up (round to even) otherwise, we do not.\n//\n//So if the last significant bit is 1, we can safely round up.\n//Hence we only need to bail out if (mantissa & 3) == 1.\n//Otherwise we may need more accuracy or analysis to determine whether\n//we are exactly between two floating-point numbers.\n//It can be triggered with 1e23.\n//Note: because the factor_mantissa and factor_mantissa_low are\n//almost always rounded down (except for small positive powers),\n//almost always should round up.\nreturnfalse;\n}\n\nmantissa += mantissa &1;\n@@ -193,15 +191,12 @@ really_inline double compute_float_64(int64_t power, uint64_t i, bool negative,\nuint64_treal_exponent = c.exp- lz;\n//we have to check that real_exponent is in range, otherwise we bail out\nif(unlikely((real_exponent <1) || (real_exponent >2046))) {\n*success =false;\nreturn0;\nreturnfalse;\n}\nmantissa |= real_exponent <<52;\nmantissa |= (((uint64_t)negative) <<63);\ndoubled;\nmemcpy(&d, &mantissa,sizeof(d));\n*success =true;\nreturnd;\nreturntrue;\n}\n\nstaticboolparse_float_strtod(constuint8_t*ptr,double*outDouble) {\n@@ -392,9 +387,8 @@ really_inline error_code write_float(const uint8_t *const src, bool negative, ui\nwriter.skip_double();\nreturnerror;\n}\nboolsuccess =true;\ndoubled =compute_float_64(exponent, i, negative, &success);\nif(!success) {\ndoubled;\nif(!compute_float_64(exponent, i, negative, d)) {\n//we are almost never going to get here.\nif(!parse_float_strtod(src, &d)) {returnINVALID_NUMBER(src); }\n}\n@@ -713,12 +707,10 @@ UNUSED really_inline simdjson_result<double> parse_double(const uint8_t * src) n\n//\n//Assemble (or slow-parse) the float\n//\ndoubled;\nif(likely(!overflow)) {\nboolsuccess =true;\ndoubled =compute_float_64(exponent, i, negative, &success);\nif(success) {returnd; }\nif(compute_float_64(exponent, i, negative, d)) {returnd; }\n}\ndoubled;\nif(!parse_float_strtod(src-negative, &d)) {\nreturnNUMBER_ERROR;\n}\n"}
{"Commit title": "Move finish() out to walk_document", "Commit body": "@@ -43,24 +43,6 @@ struct structural_parser : structural_iterator {\nreturnfalse;\n}\n\ntemplate<boolSTREAMING>\nSIMDJSON_WARN_UNUSED simdjson_really_inline error_codefinish() {\ndom_parser.next_structural_index=uint32_t(next_structural - &dom_parser.structural_indexes[0]);\n\nif(depth !=0) {\nlog_error(\"Unclosed objects or arrays!\");\nreturnTAPE_ERROR;\n}\n\n//If we didn't make it to the end, it's an error\nif( !STREAMING && dom_parser.next_structural_index!= dom_parser.n_structural_indexes) {\nlogger::log_string(\"More than one JSON value at the root of the document, or extra characters at the end of the JSON!\");\nreturnTAPE_ERROR;\n}\n\nreturnSUCCESS;\n}\n\nsimdjson_really_inlineuint8_tlast_structural() {\nreturnbuf[dom_parser.structural_indexes[dom_parser.n_structural_indexes-1]];\n}\n@@ -214,7 +196,21 @@ array_continue: {\n\ndocument_end: {\nvisitor.end_document(*this);\nreturnfinish<STREAMING>();\n\ndom_parser.next_structural_index=uint32_t(next_structural - &dom_parser.structural_indexes[0]);\n\nif(depth !=0) {\nlog_error(\"Unclosed objects or arrays!\");\nreturnTAPE_ERROR;\n}\n\n//If we didn't make it to the end, it's an error\nif( !STREAMING && dom_parser.next_structural_index!= dom_parser.n_structural_indexes) {\nlogger::log_string(\"More than one JSON value at the root of the document, or extra characters at the end of the JSON!\");\nreturnTAPE_ERROR;\n}\n\nreturnSUCCESS;\n}//document_end:\n\n}//parse_structurals()\n"}
{"Commit title": "Thinking about making the string buffer ASCII and moving the length o…", "Commit body": "@@ -89,20 +89,45 @@ really_inline bool parser::on_null_atom() noexcept {\nreally_inlineuint8_t*parser::on_start_string()noexcept{\n/*we advance the point, accounting for the fact that we have a NULL\n* termination*/\n//If we limit JSON documents to strictly less 4GB of\n//string content, then current_string_buf_loc\n//- doc.string_buf.get() fits in 32 bits. This leaves us\n//three free bytes.\nwrite_tape(current_string_buf_loc - doc.string_buf.get(), internal::tape_type::STRING);\nreturncurrent_string_buf_loc +sizeof(uint32_t);\nreturncurrent_string_buf_loc +sizeof(uint16_t);\n}\n\nreally_inlineboolparser::on_end_string(uint8_t*dst)noexcept{\nuint32_tstr_length =uint32_t(dst - (current_string_buf_loc +sizeof(uint32_t)));\n//TODO check for overflow in case someone has a crazy string (>=4GB?)\n//But only add the overflow check when the document itself exceeds 4GB\n//Currently unneeded because we refuse to parse docs larger or equal to 4GB.\nmemcpy(current_string_buf_loc, &str_length,sizeof(uint32_t));\n\n//We have two scenarios here. Either the string length is\n//less than 0x7fffff in which case, we have room in the string\n//header and all is good. Otherwise, we can encode the\n//string length in the document itself, taking care to\n//ensure that we do so in ASCII.\nif(likely(str_length <=0x7fffff)) {//likely\ndoc.tape[current_loc-1] |=uint64_t(str_length) <<32;\n//we have a string header that must be ASCII, unused in\n//this common case\ncurrent_string_buf_loc[0] =uint8_t(32);//space\ncurrent_string_buf_loc[1] =uint8_t(32);//space\n//we are done!\n}else{\n//oh gosh, we have a long string.\ndoc.tape[current_loc-1] |=uint64_t(0x800000| (str_length >>9)) <<32;\n//we have 9 bits left to code, which we do on the string buffer\n//using two bytes\ncurrent_string_buf_loc[0] =uint8_t(32+ ((str_length &0x1f0) >>4))\ncurrent_string_buf_loc[1] =32+uint8_t(str_length &0xf);\n}\n//NULL termination is still handy if you expect all your strings to\n//be NULL terminated? It comes at a small cost\n*dst =0;\ncurrent_string_buf_loc = dst +1;\n//be NULL terminated? It comes at a small cost and if it is\n//never used, we might as well drop it.\n//*dst = 0;\n//current_string_buf_loc = dst + 1;\nreturntrue;\n}\n\n"}
{"Commit title": "Was missing a stupid == 0 check which could lead to infinite loops. Duh!", "Commit body": "@@ -355,6 +355,13 @@ void decimal_right_shift(decimal &h, uint32_t shift) {\n\ntemplate<typenamebinary> adjusted_mantissacompute_float(decimal &d) {\nadjusted_mantissa answer;\nif(d.num_digits==0) {\n//should be zero\nanswer.power2=0;\nanswer.mantissa=0;\nreturnanswer;\n}\n//At this point, going further, we can assume that d.num_digits > 0.\n\nstaticconstuint32_tmax_shift =60;\nstaticconstuint32_tnum_powers =19;\n"}
{"Commit title": "Check benchmark results in release builds", "Commit body": "@@ -10,7 +10,7 @@ template<typename B, typename R> static void JsonBenchmark(benchmark::State &sta\n{\nR reference;\nif(!reference.Run(json)) { state.SkipWithError(\"reference tweet reading failed\");return; }\nassert(bench.Result()==reference.Result());\nif(bench.Result()!=reference.Result()){ state.SkipWithError(\"results are not the same\");return; }\n}\n\n//Run the benchmark\n"}
{"Commit title": "This seems beneficial.", "Commit body": "@@ -435,24 +435,52 @@ really_inline void flatten_bits(uint32_t *base_ptr, uint32_t &base,\nuint32_tidx,uint64_tbits) {\nuint32_tcnt =hamming(bits);\nuint32_tnext_base = base + cnt;\nwhile(bits !=0u) {\nbase_ptr[base +0] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nidx -=64;\nbase_ptr += base;\nif(bits !=0) {\nbase_ptr[0] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +1] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[1] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +2] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[2] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +3] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[3] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +4] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[4] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +5] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[5] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +6] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[6] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +7] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[7] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase +=8;\nbase_ptr +=8;\n}\nif(cnt >8) {\nbase_ptr[0] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[1] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[2] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[3] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[4] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[5] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[6] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[7] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr +=8;\n}\nif(cnt >16) {//unluckly\ndo{\nbase_ptr[0] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr++;\n}while(bits !=0);\n}\nbase = next_base;\n}\n"}
{"Commit title": "Move { and [ to the start of the switch", "Commit body": "@@ -200,6 +200,12 @@ struct structural_parser : structural_iterator {\n}\nWARN_UNUSED really_inlineret_address_tparse_value(constunified_machine_addresses &addresses,ret_address_tcontinue_state) {\nswitch(advance_char()) {\ncase'{':\nFAIL_IF(start_object(continue_state) );\nreturnaddresses.object_begin;\ncase'[':\nFAIL_IF(start_array(continue_state) );\nreturnaddresses.array_begin;\ncase'\"':\nFAIL_IF(parse_string() );\nreturncontinue_state;\n@@ -223,12 +229,6 @@ struct structural_parser : structural_iterator {\ncase'5':case'6':case'7':case'8':case'9':\nFAIL_IF(parse_number() );\nreturncontinue_state;\ncase'{':\nFAIL_IF(start_object(continue_state) );\nreturnaddresses.object_begin;\ncase'[':\nFAIL_IF(start_array(continue_state) );\nreturnaddresses.array_begin;\ndefault:\nlog_error(\"Non-value found when value was expected!\");\nreturnaddresses.error;\n"}
{"Commit title": "Redesigning visit_primitive so that it is optimized for strings and", "Commit body": "@@ -303,15 +303,17 @@ simdjson_warn_unused simdjson_inline error_code json_iterator::visit_root_primit\n}\ntemplate<typenameV>\nsimdjson_warn_unused simdjson_inline error_codejson_iterator::visit_primitive(V &visitor,constuint8_t*value)noexcept{\n//Use the fact that most scalars are going to be either strings or numbers.\nif(*value =='\"') {\nreturnvisitor.visit_string(*this, value);\n}elseif(((*value -'0')  <10) || (*value =='-')) {\nreturnvisitor.visit_number(*this, value);\n}\n//true, false, null are uncommon.\nswitch(*value) {\ncase'\"':returnvisitor.visit_string(*this, value);\ncase't':returnvisitor.visit_true_atom(*this, value);\ncase'f':returnvisitor.visit_false_atom(*this, value);\ncase'n':returnvisitor.visit_null_atom(*this, value);\ncase'-':\ncase'0':case'1':case'2':case'3':case'4':\ncase'5':case'6':case'7':case'8':case'9':\nreturnvisitor.visit_number(*this, value);\ndefault:\nlog_error(\"Non-value found when value was expected!\");\nreturnTAPE_ERROR;\n"}
{"Commit title": "Adding explicit constructor.", "Commit body": "@@ -128,6 +128,7 @@ struct event_collector {\nreturnlinux_events.is_working();\n}\n#else\nevent_collector() {}\nboolhas_events() {\nreturnfalse;\n}\n"}
{"Commit title": "Move { and [ to the start of the switch", "Commit body": "@@ -200,6 +200,12 @@ struct structural_parser : structural_iterator {\n}\nWARN_UNUSED really_inlineret_address_tparse_value(constunified_machine_addresses &addresses,ret_address_tcontinue_state) {\nswitch(advance_char()) {\ncase'{':\nFAIL_IF(start_object(continue_state) );\nreturnaddresses.object_begin;\ncase'[':\nFAIL_IF(start_array(continue_state) );\nreturnaddresses.array_begin;\ncase'\"':\nFAIL_IF(parse_string() );\nreturncontinue_state;\n@@ -223,12 +229,6 @@ struct structural_parser : structural_iterator {\ncase'5':case'6':case'7':case'8':case'9':\nFAIL_IF(parse_number() );\nreturncontinue_state;\ncase'{':\nFAIL_IF(start_object(continue_state) );\nreturnaddresses.object_begin;\ncase'[':\nFAIL_IF(start_array(continue_state) );\nreturnaddresses.array_begin;\ndefault:\nlog_error(\"Non-value found when value was expected!\");\nreturnaddresses.error;\n"}
{"Commit title": "Tweaking.", "Commit body": "@@ -142,7 +142,8 @@ static void GenStatPlus(Stat &stat, const dom::element v) {\nbreak;\ncasedom::element_type::OBJECT:\nfor(dom::key_value_pair kv :dom::object(v)) {\nGenStatPlus(stat,dom::element(kv.value));\nGenStatPlus(stat, kv.value);\nstat.stringLength+= kv.key.size();\nstat.memberCount++;\nstat.stringCount++;\n}\n"}
{"Commit title": "Fix arm64 build", "Commit body": "@@ -140,7 +140,7 @@ WARN_UNUSED error_code dom_parser_implementation::stage1(const uint8_t *_buf, si\n}\n\nWARN_UNUSEDboolimplementation::validate_utf8(constchar*buf,size_tlen)constnoexcept{\nreturnsimdjson::arm64::stage1::generic_validate_utf8(buf,len);\nreturnarm64::stage1::generic_validate_utf8(buf,len);\n}\n\nWARN_UNUSED error_codedom_parser_implementation::stage2(dom::document &_doc)noexcept{\n"}
{"Commit title": "Move finish() out to walk_document", "Commit body": "@@ -43,24 +43,6 @@ struct structural_parser : structural_iterator {\nreturnfalse;\n}\n\ntemplate<boolSTREAMING>\nSIMDJSON_WARN_UNUSED simdjson_really_inline error_codefinish() {\ndom_parser.next_structural_index=uint32_t(next_structural - &dom_parser.structural_indexes[0]);\n\nif(depth !=0) {\nlog_error(\"Unclosed objects or arrays!\");\nreturnTAPE_ERROR;\n}\n\n//If we didn't make it to the end, it's an error\nif( !STREAMING && dom_parser.next_structural_index!= dom_parser.n_structural_indexes) {\nlogger::log_string(\"More than one JSON value at the root of the document, or extra characters at the end of the JSON!\");\nreturnTAPE_ERROR;\n}\n\nreturnSUCCESS;\n}\n\nsimdjson_really_inlineuint8_tlast_structural() {\nreturnbuf[dom_parser.structural_indexes[dom_parser.n_structural_indexes-1]];\n}\n@@ -214,7 +196,21 @@ array_continue: {\n\ndocument_end: {\nvisitor.end_document(*this);\nreturnfinish<STREAMING>();\n\ndom_parser.next_structural_index=uint32_t(next_structural - &dom_parser.structural_indexes[0]);\n\nif(depth !=0) {\nlog_error(\"Unclosed objects or arrays!\");\nreturnTAPE_ERROR;\n}\n\n//If we didn't make it to the end, it's an error\nif( !STREAMING && dom_parser.next_structural_index!= dom_parser.n_structural_indexes) {\nlogger::log_string(\"More than one JSON value at the root of the document, or extra characters at the end of the JSON!\");\nreturnTAPE_ERROR;\n}\n\nreturnSUCCESS;\n}//document_end:\n\n}//parse_structurals()\n"}
{"Commit title": "Thinking about making the string buffer ASCII and moving the length o…", "Commit body": "@@ -89,20 +89,45 @@ really_inline bool parser::on_null_atom() noexcept {\nreally_inlineuint8_t*parser::on_start_string()noexcept{\n/*we advance the point, accounting for the fact that we have a NULL\n* termination*/\n//If we limit JSON documents to strictly less 4GB of\n//string content, then current_string_buf_loc\n//- doc.string_buf.get() fits in 32 bits. This leaves us\n//three free bytes.\nwrite_tape(current_string_buf_loc - doc.string_buf.get(), internal::tape_type::STRING);\nreturncurrent_string_buf_loc +sizeof(uint32_t);\nreturncurrent_string_buf_loc +sizeof(uint16_t);\n}\n\nreally_inlineboolparser::on_end_string(uint8_t*dst)noexcept{\nuint32_tstr_length =uint32_t(dst - (current_string_buf_loc +sizeof(uint32_t)));\n//TODO check for overflow in case someone has a crazy string (>=4GB?)\n//But only add the overflow check when the document itself exceeds 4GB\n//Currently unneeded because we refuse to parse docs larger or equal to 4GB.\nmemcpy(current_string_buf_loc, &str_length,sizeof(uint32_t));\n\n//We have two scenarios here. Either the string length is\n//less than 0x7fffff in which case, we have room in the string\n//header and all is good. Otherwise, we can encode the\n//string length in the document itself, taking care to\n//ensure that we do so in ASCII.\nif(likely(str_length <=0x7fffff)) {//likely\ndoc.tape[current_loc-1] |=uint64_t(str_length) <<32;\n//we have a string header that must be ASCII, unused in\n//this common case\ncurrent_string_buf_loc[0] =uint8_t(32);//space\ncurrent_string_buf_loc[1] =uint8_t(32);//space\n//we are done!\n}else{\n//oh gosh, we have a long string.\ndoc.tape[current_loc-1] |=uint64_t(0x800000| (str_length >>9)) <<32;\n//we have 9 bits left to code, which we do on the string buffer\n//using two bytes\ncurrent_string_buf_loc[0] =uint8_t(32+ ((str_length &0x1f0) >>4))\ncurrent_string_buf_loc[1] =32+uint8_t(str_length &0xf);\n}\n//NULL termination is still handy if you expect all your strings to\n//be NULL terminated? It comes at a small cost\n*dst =0;\ncurrent_string_buf_loc = dst +1;\n//be NULL terminated? It comes at a small cost and if it is\n//never used, we might as well drop it.\n//*dst = 0;\n//current_string_buf_loc = dst + 1;\nreturntrue;\n}\n\n"}
{"Commit title": "This makes the float errors explicit.", "Commit body": "@@ -2,6 +2,7 @@\n\n#include<vector>\n#include<sstream>\n#include<limits>\n\ntemplate<typenameT>\nstaticbooldiff_results(benchmark::State &state,constT &result,constT &reference);\n@@ -19,6 +20,24 @@ struct result_differ {\n}\n};\n\ntemplate<>\nboolresult_differ<double>::diff(benchmark::State &state,constdouble&result,constdouble&reference) {\nif(result != reference) {\nstd::stringstream str;\n//We print it out using full precision.\nautoprior_precision = str.precision(std::numeric_limits<double>::max_digits10);\nstr <<\"result incorrect:\"<< result <<\"... reference:\"<< reference;\nstr.precision(prior_precision);//reset to prior state\nstr << std::hexfloat;//If there are floats, we want to see them in hexadecimal form!\nstr <<\"result incorrect (hexadecimal notation):\"<< result <<\"... reference:\"<< reference;\nstr << std::defaultfloat;//reset to prior state\nstate.SkipWithError(str.str().data());\nreturnfalse;\n}\nreturntrue;\n}\n\n\ntemplate<typenameT>\nstructresult_differ<std::vector<T>> {\nstaticbooldiff(benchmark::State &state,conststd::vector<T> &result,conststd::vector<T> &reference) {\n"}
{"Commit title": "fixing continuation validation", "Commit body": "@@ -178,7 +178,7 @@ really_inline void check_utf8<Architecture::WESTMERE>(\nstate.has_error=\n_mm_or_si128(_mm_cmpgt_epi8(state.previous.first_len,\n_mm_setr_epi8(9,9,9,9,9,9,9,9,9,9,\n9,9,9,3,2,1)),\n9,9,9,2,1,0)),\nstate.has_error);\n}else{\n//it is not ascii so we have to do heavy work\n@@ -193,7 +193,7 @@ really_inline void check_utf8<Architecture::WESTMERE>(\nstate.has_error=\n_mm_or_si128(_mm_cmpgt_epi8(state.previous.first_len,\n_mm_setr_epi8(9,9,9,9,9,9,9,9,9,9,\n9,9,9,3,2,1)),\n9,9,9,2,1,0)),\nstate.has_error);\n}else{\n//it is not ascii so we have to do heavy work\n"}
{"Commit title": "Adding explicit constructor.", "Commit body": "@@ -128,6 +128,7 @@ struct event_collector {\nreturnlinux_events.is_working();\n}\n#else\nevent_collector() {}\nboolhas_events() {\nreturnfalse;\n}\n"}
{"Commit title": "Thinking about making the string buffer ASCII and moving the length o…", "Commit body": "@@ -89,20 +89,45 @@ really_inline bool parser::on_null_atom() noexcept {\nreally_inlineuint8_t*parser::on_start_string()noexcept{\n/*we advance the point, accounting for the fact that we have a NULL\n* termination*/\n//If we limit JSON documents to strictly less 4GB of\n//string content, then current_string_buf_loc\n//- doc.string_buf.get() fits in 32 bits. This leaves us\n//three free bytes.\nwrite_tape(current_string_buf_loc - doc.string_buf.get(), internal::tape_type::STRING);\nreturncurrent_string_buf_loc +sizeof(uint32_t);\nreturncurrent_string_buf_loc +sizeof(uint16_t);\n}\n\nreally_inlineboolparser::on_end_string(uint8_t*dst)noexcept{\nuint32_tstr_length =uint32_t(dst - (current_string_buf_loc +sizeof(uint32_t)));\n//TODO check for overflow in case someone has a crazy string (>=4GB?)\n//But only add the overflow check when the document itself exceeds 4GB\n//Currently unneeded because we refuse to parse docs larger or equal to 4GB.\nmemcpy(current_string_buf_loc, &str_length,sizeof(uint32_t));\n\n//We have two scenarios here. Either the string length is\n//less than 0x7fffff in which case, we have room in the string\n//header and all is good. Otherwise, we can encode the\n//string length in the document itself, taking care to\n//ensure that we do so in ASCII.\nif(likely(str_length <=0x7fffff)) {//likely\ndoc.tape[current_loc-1] |=uint64_t(str_length) <<32;\n//we have a string header that must be ASCII, unused in\n//this common case\ncurrent_string_buf_loc[0] =uint8_t(32);//space\ncurrent_string_buf_loc[1] =uint8_t(32);//space\n//we are done!\n}else{\n//oh gosh, we have a long string.\ndoc.tape[current_loc-1] |=uint64_t(0x800000| (str_length >>9)) <<32;\n//we have 9 bits left to code, which we do on the string buffer\n//using two bytes\ncurrent_string_buf_loc[0] =uint8_t(32+ ((str_length &0x1f0) >>4))\ncurrent_string_buf_loc[1] =32+uint8_t(str_length &0xf);\n}\n//NULL termination is still handy if you expect all your strings to\n//be NULL terminated? It comes at a small cost\n*dst =0;\ncurrent_string_buf_loc = dst +1;\n//be NULL terminated? It comes at a small cost and if it is\n//never used, we might as well drop it.\n//*dst = 0;\n//current_string_buf_loc = dst + 1;\nreturntrue;\n}\n\n"}
{"Commit title": "Move { and [ to the start of the switch", "Commit body": "@@ -200,6 +200,12 @@ struct structural_parser : structural_iterator {\n}\nWARN_UNUSED really_inlineret_address_tparse_value(constunified_machine_addresses &addresses,ret_address_tcontinue_state) {\nswitch(advance_char()) {\ncase'{':\nFAIL_IF(start_object(continue_state) );\nreturnaddresses.object_begin;\ncase'[':\nFAIL_IF(start_array(continue_state) );\nreturnaddresses.array_begin;\ncase'\"':\nFAIL_IF(parse_string() );\nreturncontinue_state;\n@@ -223,12 +229,6 @@ struct structural_parser : structural_iterator {\ncase'5':case'6':case'7':case'8':case'9':\nFAIL_IF(parse_number() );\nreturncontinue_state;\ncase'{':\nFAIL_IF(start_object(continue_state) );\nreturnaddresses.object_begin;\ncase'[':\nFAIL_IF(start_array(continue_state) );\nreturnaddresses.array_begin;\ndefault:\nlog_error(\"Non-value found when value was expected!\");\nreturnaddresses.error;\n"}
{"Commit title": "type", "Commit body": "@@ -64,7 +64,7 @@ COMPARISONEXECUTABLES=minifiercompetition parsingcompetition parseandstatcompeti\nSUPPLEMENTARYEXECUTABLES=parse_noutf8validation parse_nonumberparsing parse_nostringparsing\n\n#Load headers and sources\nLIBHEADERS=src/simdprune_tables.h src/numberparsing.h src/jsoncharutils.h src/arm64/simd_input.h src/arm64/simdutf8check.h src/arm64/stage1_find_marks.h src/arm64/stage2_build_tape.h src/arm64/stringparsing.h src/generic/stage1_find_marks.h src/generic/stage2_build_tape.h src/generic/stringparsing.h src/haswell/simd_input.h src/haswell/simdutf8check.h src/haswell/stage1_find_marks.h src/haswell/stage2_build_tape.h src/haswell/stringparsing.h src/westmere/simd_input.h src/westmere/simdutf8check.h src/westmere/stage1_find_marks.h src/westmere/stage2_build_tape.h src/westmere/stringparsing.h src/generic/tage2_streaming_build_tape.h\nLIBHEADERS=src/simdprune_tables.h src/numberparsing.h src/jsoncharutils.h src/arm64/simd_input.h src/arm64/simdutf8check.h src/arm64/stage1_find_marks.h src/arm64/stage2_build_tape.h src/arm64/stringparsing.h src/generic/stage1_find_marks.h src/generic/stage2_build_tape.h src/generic/stringparsing.h src/haswell/simd_input.h src/haswell/simdutf8check.h src/haswell/stage1_find_marks.h src/haswell/stage2_build_tape.h src/haswell/stringparsing.h src/westmere/simd_input.h src/westmere/simdutf8check.h src/westmere/stage1_find_marks.h src/westmere/stage2_build_tape.h src/westmere/stringparsing.h src/generic/stage2_streaming_build_tape.h\nPUBHEADERS=include/simdjson/common_defs.h include/simdjson/isadetection.h include/simdjson/jsonformatutils.h include/simdjson/jsonioutil.h include/simdjson/jsonminifier.h include/simdjson/jsonparser.h include/simdjson/padded_string.h include/simdjson/parsedjson.h include/simdjson/parsedjsoniterator.h include/simdjson/portability.h include/simdjson/simdjson.h include/simdjson/simdjson_version.h include/simdjson/stage1_find_marks.h include/simdjson/stage2_build_tape.h\nHEADERS=$(PUBHEADERS)$(LIBHEADERS)\n\n"}
{"Commit title": "Fix fallback implementation", "Commit body": "@@ -73,8 +73,6 @@ class json_structural_indexer {\nreally_inlinevoidstep(constuint8_t*block, buf_block_reader<STEP_SIZE> &reader)noexcept;\nreally_inlinevoidnext(simd::simd8x64<uint8_t> in, json_block block,size_tidx);\nreally_inline error_codefinish(dom_parser_implementation &parser,size_tidx,size_tlen,boolpartial);\nstaticreally_inlineuint32_tfind_next_document_index(dom_parser_implementation &parser);\nstaticreally_inlinesize_ttrim_partial_utf8(constuint8_t*buf,size_tlen);\n\njson_scanner scanner{};\nutf8_checker checker{};\n@@ -197,91 +195,4 @@ really_inline error_code json_structural_indexer::finish(dom_parser_implementati\nreturnchecker.errors();\n}\n\n/**\n* This algorithm is used to quickly identify the last structural position that\n* makes up a complete document.\n*\n* It does this by going backwards and finding the last *document boundary* (a\n* place where one value follows another without a comma between them). If the\n* last document (the characters after the boundary) has an equal number of\n* start and end brackets, it is considered complete.\n*\n* Simply put, we iterate over the structural characters, starting from\n* the end. We consider that we found the end of a JSON document when the\n* first element of the pair is NOT one of these characters: '{' '[' ';' ','\n* and when the second element is NOT one of these characters: '}' '}' ';' ','.\n*\n* This simple comparison works most of the time, but it does not cover cases\n* where the batch's structural indexes contain a perfect amount of documents.\n* In such a case, we do not have access to the structural index which follows\n* the last document, therefore, we do not have access to the second element in\n* the pair, and means that we cannot identify the last document. To fix this\n* issue, we keep a count of the open and closed curly/square braces we found\n* while searching for the pair. When we find a pair AND the count of open and\n* closed curly/square braces is the same, we know that we just passed a\n* complete\n* document, therefore the last json buffer location is the end of the batch\n*/\nreally_inlineuint32_tjson_structural_indexer::find_next_document_index(dom_parser_implementation &parser) {\n//TODO don't count separately, just figure out depth\nautoarr_cnt =0;\nautoobj_cnt =0;\nfor(autoi = parser.n_structural_indexes-1; i >0; i--) {\nautoidxb = parser.structural_indexes[i];\nswitch(parser.buf[idxb]) {\ncase':':\ncase',':\ncontinue;\ncase'}':\nobj_cnt--;\ncontinue;\ncase']':\narr_cnt--;\ncontinue;\ncase'{':\nobj_cnt++;\nbreak;\ncase'[':\narr_cnt++;\nbreak;\n}\nautoidxa = parser.structural_indexes[i -1];\nswitch(parser.buf[idxa]) {\ncase'{':\ncase'[':\ncase':':\ncase',':\ncontinue;\n}\n//Last document is complete, so the next document will appear after!\nif(!arr_cnt && !obj_cnt) {\nreturnparser.n_structural_indexes;\n}\n//Last document is incomplete; mark the document at i + 1 as the next one\nreturni;\n}\nreturn0;\n}\n\n//Skip the last character if it is partial\nreally_inlinesize_tjson_structural_indexer::trim_partial_utf8(constuint8_t*buf,size_tlen) {\nif(unlikely(len <3)) {\nswitch(len) {\ncase2:\nif(buf[len-1] >=0b11000000) {returnlen-1; }//2-, 3- and 4-byte characters with only 1 byte left\nif(buf[len-2] >=0b11100000) {returnlen-2; }//3- and 4-byte characters with only 2 bytes left\nreturnlen;\ncase1:\nif(buf[len-1] >=0b11000000) {returnlen-1; }//2-, 3- and 4-byte characters with only 1 byte left\nreturnlen;\ncase0:\nreturnlen;\n}\n}\nif(buf[len-1] >=0b11000000) {returnlen-1; }//2-, 3- and 4-byte characters with only 1 byte left\nif(buf[len-2] >=0b11100000) {returnlen-2; }//3- and 4-byte characters with only 1 byte left\nif(buf[len-3] >=0b11110000) {returnlen-3; }//4-byte characters with only 3 bytes left\nreturnlen;\n}\n\n}//namespace stage1"}
{"Commit title": "investigate adding BMI2 to the haswell kernel", "Commit body": "@@ -4,7 +4,7 @@\n#include\"simdjson/haswell/intrinsics.h\"\n\n#if!SIMDJSON_CAN_ALWAYS_RUN_HASWELL\nSIMDJSON_TARGET_REGION(\"avx2,bmi,pclmul,lzcnt,popcnt\")\nSIMDJSON_TARGET_REGION(\"avx2,bmi,bmi2,pclmul,lzcnt,popcnt\")\n#endif\n\n#include\"simdjson/haswell/bitmanipulation.h\"\n"}
{"Commit title": "type", "Commit body": "@@ -64,7 +64,7 @@ COMPARISONEXECUTABLES=minifiercompetition parsingcompetition parseandstatcompeti\nSUPPLEMENTARYEXECUTABLES=parse_noutf8validation parse_nonumberparsing parse_nostringparsing\n\n#Load headers and sources\nLIBHEADERS=src/simdprune_tables.h src/numberparsing.h src/jsoncharutils.h src/arm64/simd_input.h src/arm64/simdutf8check.h src/arm64/stage1_find_marks.h src/arm64/stage2_build_tape.h src/arm64/stringparsing.h src/generic/stage1_find_marks.h src/generic/stage2_build_tape.h src/generic/stringparsing.h src/haswell/simd_input.h src/haswell/simdutf8check.h src/haswell/stage1_find_marks.h src/haswell/stage2_build_tape.h src/haswell/stringparsing.h src/westmere/simd_input.h src/westmere/simdutf8check.h src/westmere/stage1_find_marks.h src/westmere/stage2_build_tape.h src/westmere/stringparsing.h src/generic/tage2_streaming_build_tape.h\nLIBHEADERS=src/simdprune_tables.h src/numberparsing.h src/jsoncharutils.h src/arm64/simd_input.h src/arm64/simdutf8check.h src/arm64/stage1_find_marks.h src/arm64/stage2_build_tape.h src/arm64/stringparsing.h src/generic/stage1_find_marks.h src/generic/stage2_build_tape.h src/generic/stringparsing.h src/haswell/simd_input.h src/haswell/simdutf8check.h src/haswell/stage1_find_marks.h src/haswell/stage2_build_tape.h src/haswell/stringparsing.h src/westmere/simd_input.h src/westmere/simdutf8check.h src/westmere/stage1_find_marks.h src/westmere/stage2_build_tape.h src/westmere/stringparsing.h src/generic/stage2_streaming_build_tape.h\nPUBHEADERS=include/simdjson/common_defs.h include/simdjson/isadetection.h include/simdjson/jsonformatutils.h include/simdjson/jsonioutil.h include/simdjson/jsonminifier.h include/simdjson/jsonparser.h include/simdjson/padded_string.h include/simdjson/parsedjson.h include/simdjson/parsedjsoniterator.h include/simdjson/portability.h include/simdjson/simdjson.h include/simdjson/simdjson_version.h include/simdjson/stage1_find_marks.h include/simdjson/stage2_build_tape.h\nHEADERS=$(PUBHEADERS)$(LIBHEADERS)\n\n"}
{"Commit title": "Better comment.", "Commit body": "@@ -217,9 +217,8 @@ static const double power_of_ten[] = {\n\n//the mantissas of powers of ten from -308 to 308, extended out to sixty four\n//bits\n\n//The array power_of_ten_components contain the powers of ten approximated\n//as a 64-bit mantissa, with an exponent part. It goes from 10^\n//The array contains the powers of ten approximated\n//as a 64-bit mantissa. It goes from 10^\n//FASTFLOAT_SMALLEST_POWER to\n//10^FASTFLOAT_LARGEST_POWER (inclusively). The mantissa is truncated, and\n//never rounded up.\n"}
{"Commit title": "Move document stream state to implementation", "Commit body": "@@ -73,6 +73,8 @@ class json_structural_indexer {\nreally_inlinevoidstep(constuint8_t*block, buf_block_reader<STEP_SIZE> &reader)noexcept;\nreally_inlinevoidnext(simd::simd8x64<uint8_t> in, json_block block,size_tidx);\nreally_inline error_codefinish(dom_parser_implementation &parser,size_tidx,size_tlen,boolpartial);\nstaticreally_inlineuint32_tfind_next_document_index(dom_parser_implementation &parser);\nstaticreally_inlinesize_ttrim_partial_utf8(constuint8_t*buf,size_tlen);\n\njson_scanner scanner{};\nutf8_checker checker{};\n@@ -195,4 +197,91 @@ really_inline error_code json_structural_indexer::finish(dom_parser_implementati\nreturnchecker.errors();\n}\n\n/**\n* This algorithm is used to quickly identify the last structural position that\n* makes up a complete document.\n*\n* It does this by going backwards and finding the last *document boundary* (a\n* place where one value follows another without a comma between them). If the\n* last document (the characters after the boundary) has an equal number of\n* start and end brackets, it is considered complete.\n*\n* Simply put, we iterate over the structural characters, starting from\n* the end. We consider that we found the end of a JSON document when the\n* first element of the pair is NOT one of these characters: '{' '[' ';' ','\n* and when the second element is NOT one of these characters: '}' '}' ';' ','.\n*\n* This simple comparison works most of the time, but it does not cover cases\n* where the batch's structural indexes contain a perfect amount of documents.\n* In such a case, we do not have access to the structural index which follows\n* the last document, therefore, we do not have access to the second element in\n* the pair, and means that we cannot identify the last document. To fix this\n* issue, we keep a count of the open and closed curly/square braces we found\n* while searching for the pair. When we find a pair AND the count of open and\n* closed curly/square braces is the same, we know that we just passed a\n* complete\n* document, therefore the last json buffer location is the end of the batch\n*/\nreally_inlineuint32_tjson_structural_indexer::find_next_document_index(dom_parser_implementation &parser) {\n//TODO don't count separately, just figure out depth\nautoarr_cnt =0;\nautoobj_cnt =0;\nfor(autoi = parser.n_structural_indexes-1; i >0; i--) {\nautoidxb = parser.structural_indexes[i];\nswitch(parser.buf[idxb]) {\ncase':':\ncase',':\ncontinue;\ncase'}':\nobj_cnt--;\ncontinue;\ncase']':\narr_cnt--;\ncontinue;\ncase'{':\nobj_cnt++;\nbreak;\ncase'[':\narr_cnt++;\nbreak;\n}\nautoidxa = parser.structural_indexes[i -1];\nswitch(parser.buf[idxa]) {\ncase'{':\ncase'[':\ncase':':\ncase',':\ncontinue;\n}\n//Last document is complete, so the next document will appear after!\nif(!arr_cnt && !obj_cnt) {\nreturnparser.n_structural_indexes;\n}\n//Last document is incomplete; mark the document at i + 1 as the next one\nreturni;\n}\nreturn0;\n}\n\n//Skip the last character if it is partial\nreally_inlinesize_tjson_structural_indexer::trim_partial_utf8(constuint8_t*buf,size_tlen) {\nif(unlikely(len <3)) {\nswitch(len) {\ncase2:\nif(buf[len-1] >=0b11000000) {returnlen-1; }//2-, 3- and 4-byte characters with only 1 byte left\nif(buf[len-2] >=0b11100000) {returnlen-2; }//3- and 4-byte characters with only 2 bytes left\nreturnlen;\ncase1:\nif(buf[len-1] >=0b11000000) {returnlen-1; }//2-, 3- and 4-byte characters with only 1 byte left\nreturnlen;\ncase0:\nreturnlen;\n}\n}\nif(buf[len-1] >=0b11000000) {returnlen-1; }//2-, 3- and 4-byte characters with only 1 byte left\nif(buf[len-2] >=0b11100000) {returnlen-2; }//3- and 4-byte characters with only 1 byte left\nif(buf[len-3] >=0b11110000) {returnlen-3; }//4-byte characters with only 3 bytes left\nreturnlen;\n}\n\n}//namespace stage1"}
{"Commit title": "This makes the float errors explicit.", "Commit body": "@@ -2,6 +2,7 @@\n\n#include<vector>\n#include<sstream>\n#include<limits>\n\ntemplate<typenameT>\nstaticbooldiff_results(benchmark::State &state,constT &result,constT &reference);\n@@ -19,6 +20,24 @@ struct result_differ {\n}\n};\n\ntemplate<>\nboolresult_differ<double>::diff(benchmark::State &state,constdouble&result,constdouble&reference) {\nif(result != reference) {\nstd::stringstream str;\n//We print it out using full precision.\nautoprior_precision = str.precision(std::numeric_limits<double>::max_digits10);\nstr <<\"result incorrect:\"<< result <<\"... reference:\"<< reference;\nstr.precision(prior_precision);//reset to prior state\nstr << std::hexfloat;//If there are floats, we want to see them in hexadecimal form!\nstr <<\"result incorrect (hexadecimal notation):\"<< result <<\"... reference:\"<< reference;\nstr << std::defaultfloat;//reset to prior state\nstate.SkipWithError(str.str().data());\nreturnfalse;\n}\nreturntrue;\n}\n\n\ntemplate<typenameT>\nstructresult_differ<std::vector<T>> {\nstaticbooldiff(benchmark::State &state,conststd::vector<T> &result,conststd::vector<T> &reference) {\n"}
{"Commit title": "Move document stream state to implementation", "Commit body": "@@ -73,6 +73,8 @@ class json_structural_indexer {\nreally_inlinevoidstep(constuint8_t*block, buf_block_reader<STEP_SIZE> &reader)noexcept;\nreally_inlinevoidnext(simd::simd8x64<uint8_t> in, json_block block,size_tidx);\nreally_inline error_codefinish(dom_parser_implementation &parser,size_tidx,size_tlen,boolpartial);\nstaticreally_inlineuint32_tfind_next_document_index(dom_parser_implementation &parser);\nstaticreally_inlinesize_ttrim_partial_utf8(constuint8_t*buf,size_tlen);\n\njson_scanner scanner{};\nutf8_checker checker{};\n@@ -195,4 +197,91 @@ really_inline error_code json_structural_indexer::finish(dom_parser_implementati\nreturnchecker.errors();\n}\n\n/**\n* This algorithm is used to quickly identify the last structural position that\n* makes up a complete document.\n*\n* It does this by going backwards and finding the last *document boundary* (a\n* place where one value follows another without a comma between them). If the\n* last document (the characters after the boundary) has an equal number of\n* start and end brackets, it is considered complete.\n*\n* Simply put, we iterate over the structural characters, starting from\n* the end. We consider that we found the end of a JSON document when the\n* first element of the pair is NOT one of these characters: '{' '[' ';' ','\n* and when the second element is NOT one of these characters: '}' '}' ';' ','.\n*\n* This simple comparison works most of the time, but it does not cover cases\n* where the batch's structural indexes contain a perfect amount of documents.\n* In such a case, we do not have access to the structural index which follows\n* the last document, therefore, we do not have access to the second element in\n* the pair, and means that we cannot identify the last document. To fix this\n* issue, we keep a count of the open and closed curly/square braces we found\n* while searching for the pair. When we find a pair AND the count of open and\n* closed curly/square braces is the same, we know that we just passed a\n* complete\n* document, therefore the last json buffer location is the end of the batch\n*/\nreally_inlineuint32_tjson_structural_indexer::find_next_document_index(dom_parser_implementation &parser) {\n//TODO don't count separately, just figure out depth\nautoarr_cnt =0;\nautoobj_cnt =0;\nfor(autoi = parser.n_structural_indexes-1; i >0; i--) {\nautoidxb = parser.structural_indexes[i];\nswitch(parser.buf[idxb]) {\ncase':':\ncase',':\ncontinue;\ncase'}':\nobj_cnt--;\ncontinue;\ncase']':\narr_cnt--;\ncontinue;\ncase'{':\nobj_cnt++;\nbreak;\ncase'[':\narr_cnt++;\nbreak;\n}\nautoidxa = parser.structural_indexes[i -1];\nswitch(parser.buf[idxa]) {\ncase'{':\ncase'[':\ncase':':\ncase',':\ncontinue;\n}\n//Last document is complete, so the next document will appear after!\nif(!arr_cnt && !obj_cnt) {\nreturnparser.n_structural_indexes;\n}\n//Last document is incomplete; mark the document at i + 1 as the next one\nreturni;\n}\nreturn0;\n}\n\n//Skip the last character if it is partial\nreally_inlinesize_tjson_structural_indexer::trim_partial_utf8(constuint8_t*buf,size_tlen) {\nif(unlikely(len <3)) {\nswitch(len) {\ncase2:\nif(buf[len-1] >=0b11000000) {returnlen-1; }//2-, 3- and 4-byte characters with only 1 byte left\nif(buf[len-2] >=0b11100000) {returnlen-2; }//3- and 4-byte characters with only 2 bytes left\nreturnlen;\ncase1:\nif(buf[len-1] >=0b11000000) {returnlen-1; }//2-, 3- and 4-byte characters with only 1 byte left\nreturnlen;\ncase0:\nreturnlen;\n}\n}\nif(buf[len-1] >=0b11000000) {returnlen-1; }//2-, 3- and 4-byte characters with only 1 byte left\nif(buf[len-2] >=0b11100000) {returnlen-2; }//3- and 4-byte characters with only 1 byte left\nif(buf[len-3] >=0b11110000) {returnlen-3; }//4-byte characters with only 3 bytes left\nreturnlen;\n}\n\n}//namespace stage1"}
{"Commit title": "Move { and [ to the start of the switch", "Commit body": "@@ -200,6 +200,12 @@ struct structural_parser : structural_iterator {\n}\nWARN_UNUSED really_inlineret_address_tparse_value(constunified_machine_addresses &addresses,ret_address_tcontinue_state) {\nswitch(advance_char()) {\ncase'{':\nFAIL_IF(start_object(continue_state) );\nreturnaddresses.object_begin;\ncase'[':\nFAIL_IF(start_array(continue_state) );\nreturnaddresses.array_begin;\ncase'\"':\nFAIL_IF(parse_string() );\nreturncontinue_state;\n@@ -223,12 +229,6 @@ struct structural_parser : structural_iterator {\ncase'5':case'6':case'7':case'8':case'9':\nFAIL_IF(parse_number() );\nreturncontinue_state;\ncase'{':\nFAIL_IF(start_object(continue_state) );\nreturnaddresses.object_begin;\ncase'[':\nFAIL_IF(start_array(continue_state) );\nreturnaddresses.array_begin;\ndefault:\nlog_error(\"Non-value found when value was expected!\");\nreturnaddresses.error;\n"}
{"Commit title": "Parse 18-digit unsigned integers quickly", "Commit body": "@@ -542,6 +542,22 @@ really_inline bool parse_number(UNUSED const uint8_t *const src,\n}\n}else{\nif(unlikely(digit_count >=18)) {//this is uncommon!!!\n//18-digit positive numbers never overflow, but we have to figure out whether they will\n//fit in a signed integer or not.\nif(!negative && digit_count ==18) {\nif(i <=uint64_t(INT64_MAX)) {\nwriter.append_s64(i);\n#ifdefJSON_TEST_NUMBERS//for unit testing\nfound_integer(i, src);\n#endif\n}else{\nwriter.append_u64(i);\n#ifdefJSON_TEST_NUMBERS//for unit testing\nfound_unsigned_integer(i, src);\n#endif\n}\nreturntrue;\n}\n//there is a good chance that we had an overflow, so we need\n//need to recover: we parse the whole thing again.\nboolsuccess =parse_large_integer(src, writer, found_minus);\n"}
{"Commit title": "Redesigning visit_primitive so that it is optimized for strings and", "Commit body": "@@ -303,15 +303,17 @@ simdjson_warn_unused simdjson_inline error_code json_iterator::visit_root_primit\n}\ntemplate<typenameV>\nsimdjson_warn_unused simdjson_inline error_codejson_iterator::visit_primitive(V &visitor,constuint8_t*value)noexcept{\n//Use the fact that most scalars are going to be either strings or numbers.\nif(*value =='\"') {\nreturnvisitor.visit_string(*this, value);\n}elseif(((*value -'0')  <10) || (*value =='-')) {\nreturnvisitor.visit_number(*this, value);\n}\n//true, false, null are uncommon.\nswitch(*value) {\ncase'\"':returnvisitor.visit_string(*this, value);\ncase't':returnvisitor.visit_true_atom(*this, value);\ncase'f':returnvisitor.visit_false_atom(*this, value);\ncase'n':returnvisitor.visit_null_atom(*this, value);\ncase'-':\ncase'0':case'1':case'2':case'3':case'4':\ncase'5':case'6':case'7':case'8':case'9':\nreturnvisitor.visit_number(*this, value);\ndefault:\nlog_error(\"Non-value found when value was expected!\");\nreturnTAPE_ERROR;\n"}
{"Commit title": "Adding test.", "Commit body": "@@ -349,6 +349,27 @@ namespace document_stream_tests {\n}\n\n\nboolissue1668() {\nTEST_START();\nautojson =R\"([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100])\"_padded;\n\nondemand::parser odparser;\nondemand::document_stream odstream;\nautooderror = odparser.iterate_many(json.data(), json.length(),50).get(odstream);\n\nif(oderror) { std::cerr <<\"ondemand iterate_many error:\"<< oderror << std::endl;returnfalse; }\n\nfor(auto&doc: odstream) {\nondemand::value val;\nautoerr = doc.at_pointer(\"/40\").get(val);\nif(err) {\nstd::cout <<\"ondemand: error accessing jsonpointer:\"<< err << std::endl;\n}else{\nstd::cout <<\"ondemand:\"<< val << std::endl;\n}\n}\nTEST_SUCCEED();\n}\nbooldocument_stream_utf8_test() {\nTEST_START();\nfflush(NULL);\n@@ -424,6 +445,7 @@ namespace document_stream_tests {\n\nboolrun() {\nreturn\nissue1668() &&\nsimple_document_iteration() &&\nsimple_document_iteration_multiple_batches() &&\nsimple_document_iteration_with_parsing() &&\n"}
{"Commit title": "Thinking about making the string buffer ASCII and moving the length o…", "Commit body": "@@ -89,20 +89,45 @@ really_inline bool parser::on_null_atom() noexcept {\nreally_inlineuint8_t*parser::on_start_string()noexcept{\n/*we advance the point, accounting for the fact that we have a NULL\n* termination*/\n//If we limit JSON documents to strictly less 4GB of\n//string content, then current_string_buf_loc\n//- doc.string_buf.get() fits in 32 bits. This leaves us\n//three free bytes.\nwrite_tape(current_string_buf_loc - doc.string_buf.get(), internal::tape_type::STRING);\nreturncurrent_string_buf_loc +sizeof(uint32_t);\nreturncurrent_string_buf_loc +sizeof(uint16_t);\n}\n\nreally_inlineboolparser::on_end_string(uint8_t*dst)noexcept{\nuint32_tstr_length =uint32_t(dst - (current_string_buf_loc +sizeof(uint32_t)));\n//TODO check for overflow in case someone has a crazy string (>=4GB?)\n//But only add the overflow check when the document itself exceeds 4GB\n//Currently unneeded because we refuse to parse docs larger or equal to 4GB.\nmemcpy(current_string_buf_loc, &str_length,sizeof(uint32_t));\n\n//We have two scenarios here. Either the string length is\n//less than 0x7fffff in which case, we have room in the string\n//header and all is good. Otherwise, we can encode the\n//string length in the document itself, taking care to\n//ensure that we do so in ASCII.\nif(likely(str_length <=0x7fffff)) {//likely\ndoc.tape[current_loc-1] |=uint64_t(str_length) <<32;\n//we have a string header that must be ASCII, unused in\n//this common case\ncurrent_string_buf_loc[0] =uint8_t(32);//space\ncurrent_string_buf_loc[1] =uint8_t(32);//space\n//we are done!\n}else{\n//oh gosh, we have a long string.\ndoc.tape[current_loc-1] |=uint64_t(0x800000| (str_length >>9)) <<32;\n//we have 9 bits left to code, which we do on the string buffer\n//using two bytes\ncurrent_string_buf_loc[0] =uint8_t(32+ ((str_length &0x1f0) >>4))\ncurrent_string_buf_loc[1] =32+uint8_t(str_length &0xf);\n}\n//NULL termination is still handy if you expect all your strings to\n//be NULL terminated? It comes at a small cost\n*dst =0;\ncurrent_string_buf_loc = dst +1;\n//be NULL terminated? It comes at a small cost and if it is\n//never used, we might as well drop it.\n//*dst = 0;\n//current_string_buf_loc = dst + 1;\nreturntrue;\n}\n\n"}
{"Commit title": "Fix arm64 build", "Commit body": "@@ -140,7 +140,7 @@ WARN_UNUSED error_code dom_parser_implementation::stage1(const uint8_t *_buf, si\n}\n\nWARN_UNUSEDboolimplementation::validate_utf8(constchar*buf,size_tlen)constnoexcept{\nreturnsimdjson::arm64::stage1::generic_validate_utf8(buf,len);\nreturnarm64::stage1::generic_validate_utf8(buf,len);\n}\n\nWARN_UNUSED error_codedom_parser_implementation::stage2(dom::document &_doc)noexcept{\n"}
{"Commit title": "This makes the float errors explicit.", "Commit body": "@@ -2,6 +2,7 @@\n\n#include<vector>\n#include<sstream>\n#include<limits>\n\ntemplate<typenameT>\nstaticbooldiff_results(benchmark::State &state,constT &result,constT &reference);\n@@ -19,6 +20,24 @@ struct result_differ {\n}\n};\n\ntemplate<>\nboolresult_differ<double>::diff(benchmark::State &state,constdouble&result,constdouble&reference) {\nif(result != reference) {\nstd::stringstream str;\n//We print it out using full precision.\nautoprior_precision = str.precision(std::numeric_limits<double>::max_digits10);\nstr <<\"result incorrect:\"<< result <<\"... reference:\"<< reference;\nstr.precision(prior_precision);//reset to prior state\nstr << std::hexfloat;//If there are floats, we want to see them in hexadecimal form!\nstr <<\"result incorrect (hexadecimal notation):\"<< result <<\"... reference:\"<< reference;\nstr << std::defaultfloat;//reset to prior state\nstate.SkipWithError(str.str().data());\nreturnfalse;\n}\nreturntrue;\n}\n\n\ntemplate<typenameT>\nstructresult_differ<std::vector<T>> {\nstaticbooldiff(benchmark::State &state,conststd::vector<T> &result,conststd::vector<T> &reference) {\n"}
{"Commit title": "Keep current JSON after checking primitive type", "Commit body": "@@ -279,7 +279,8 @@ WARN_UNUSED static really_inline error_code parse_structurals(dom_parser_impleme\n//Read first value\n//\n{\nswitch(parser.advance_char()) {\nconstuint8_t*value = parser.advance();\nswitch(*value) {\ncase'{': {\nif(parser.empty_object()) {gotodocument_end; }\nSIMDJSON_TRY( parser.start_object() );\n@@ -297,14 +298,14 @@ WARN_UNUSED static really_inline error_code parse_structurals(dom_parser_impleme\n}\ngotoarray_begin;\n}\ncase'\"':SIMDJSON_TRY( parser.parse_string(parser.current()) );gotodocument_end;\ncase't':SIMDJSON_TRY( parser.parse_root_true_atom(parser.current()) );gotodocument_end;\ncase'f':SIMDJSON_TRY( parser.parse_root_false_atom(parser.current()) );gotodocument_end;\ncase'n':SIMDJSON_TRY( parser.parse_root_null_atom(parser.current()) );gotodocument_end;\ncase'\"':SIMDJSON_TRY( parser.parse_string(value) );gotodocument_end;\ncase't':SIMDJSON_TRY( parser.parse_root_true_atom(value) );gotodocument_end;\ncase'f':SIMDJSON_TRY( parser.parse_root_false_atom(value) );gotodocument_end;\ncase'n':SIMDJSON_TRY( parser.parse_root_null_atom(value) );gotodocument_end;\ncase'-':\ncase'0':case'1':case'2':case'3':case'4':\ncase'5':case'6':case'7':case'8':case'9':\nSIMDJSON_TRY( parser.parse_root_number(parser.current()) );gotodocument_end;\nSIMDJSON_TRY( parser.parse_root_number(value) );gotodocument_end;\ndefault:\nparser.log_error(\"Document starts with a non-value character\");\nreturnTAPE_ERROR;\n@@ -327,7 +328,8 @@ object_begin: {\n\nobject_field: {\nif(unlikely( parser.advance_char() !=':')) { parser.log_error(\"Missing colon after key in object\");returnTAPE_ERROR; }\nswitch(parser.advance_char()) {\nconstuint8_t*value = parser.advance();\nswitch(*value) {\ncase'{': {\nif(parser.empty_object()) {break; };\nSIMDJSON_TRY( parser.start_object() );\n@@ -338,14 +340,14 @@ object_field: {\nSIMDJSON_TRY( parser.start_array() );\ngotoarray_begin;\n}\ncase'\"':SIMDJSON_TRY( parser.parse_string(parser.current()) );break;\ncase't':SIMDJSON_TRY( parser.parse_true_atom(parser.current()) );break;\ncase'f':SIMDJSON_TRY( parser.parse_false_atom(parser.current()) );break;\ncase'n':SIMDJSON_TRY( parser.parse_null_atom(parser.current()) );break;\ncase'\"':SIMDJSON_TRY( parser.parse_string(value) );break;\ncase't':SIMDJSON_TRY( parser.parse_true_atom(value) );break;\ncase'f':SIMDJSON_TRY( parser.parse_false_atom(value) );break;\ncase'n':SIMDJSON_TRY( parser.parse_null_atom(value) );break;\ncase'-':\ncase'0':case'1':case'2':case'3':case'4':\ncase'5':case'6':case'7':case'8':case'9':\nSIMDJSON_TRY( parser.parse_number(parser.current()) );break;\nSIMDJSON_TRY( parser.parse_number(value) );break;\ndefault:\nparser.log_error(\"Non-value found when value was expected!\");\nreturnTAPE_ERROR;\n@@ -384,7 +386,8 @@ array_begin: {\n}//array_begin:\n\narray_value: {\nswitch(parser.advance_char()) {\nconstuint8_t*value = parser.advance();\nswitch(*value) {\ncase'{': {\nif(parser.empty_object()) {break; };\nSIMDJSON_TRY( parser.start_object() );\n@@ -395,14 +398,14 @@ array_value: {\nSIMDJSON_TRY( parser.start_array() );\ngotoarray_begin;\n}\ncase'\"':SIMDJSON_TRY( parser.parse_string(parser.current()) );break;\ncase't':SIMDJSON_TRY( parser.parse_true_atom(parser.current()) );break;\ncase'f':SIMDJSON_TRY( parser.parse_false_atom(parser.current()) );break;\ncase'n':SIMDJSON_TRY( parser.parse_null_atom(parser.current()) );break;\ncase'\"':SIMDJSON_TRY( parser.parse_string(value) );break;\ncase't':SIMDJSON_TRY( parser.parse_true_atom(value) );break;\ncase'f':SIMDJSON_TRY( parser.parse_false_atom(value) );break;\ncase'n':SIMDJSON_TRY( parser.parse_null_atom(value) );break;\ncase'-':\ncase'0':case'1':case'2':case'3':case'4':\ncase'5':case'6':case'7':case'8':case'9':\nSIMDJSON_TRY( parser.parse_number(parser.current()) );break;\nSIMDJSON_TRY( parser.parse_number(value) );break;\ndefault:\nparser.log_error(\"Non-value found when value was expected!\");\nreturnTAPE_ERROR;\n"}
{"Commit title": "Thinking about making the string buffer ASCII and moving the length o…", "Commit body": "@@ -89,20 +89,45 @@ really_inline bool parser::on_null_atom() noexcept {\nreally_inlineuint8_t*parser::on_start_string()noexcept{\n/*we advance the point, accounting for the fact that we have a NULL\n* termination*/\n//If we limit JSON documents to strictly less 4GB of\n//string content, then current_string_buf_loc\n//- doc.string_buf.get() fits in 32 bits. This leaves us\n//three free bytes.\nwrite_tape(current_string_buf_loc - doc.string_buf.get(), internal::tape_type::STRING);\nreturncurrent_string_buf_loc +sizeof(uint32_t);\nreturncurrent_string_buf_loc +sizeof(uint16_t);\n}\n\nreally_inlineboolparser::on_end_string(uint8_t*dst)noexcept{\nuint32_tstr_length =uint32_t(dst - (current_string_buf_loc +sizeof(uint32_t)));\n//TODO check for overflow in case someone has a crazy string (>=4GB?)\n//But only add the overflow check when the document itself exceeds 4GB\n//Currently unneeded because we refuse to parse docs larger or equal to 4GB.\nmemcpy(current_string_buf_loc, &str_length,sizeof(uint32_t));\n\n//We have two scenarios here. Either the string length is\n//less than 0x7fffff in which case, we have room in the string\n//header and all is good. Otherwise, we can encode the\n//string length in the document itself, taking care to\n//ensure that we do so in ASCII.\nif(likely(str_length <=0x7fffff)) {//likely\ndoc.tape[current_loc-1] |=uint64_t(str_length) <<32;\n//we have a string header that must be ASCII, unused in\n//this common case\ncurrent_string_buf_loc[0] =uint8_t(32);//space\ncurrent_string_buf_loc[1] =uint8_t(32);//space\n//we are done!\n}else{\n//oh gosh, we have a long string.\ndoc.tape[current_loc-1] |=uint64_t(0x800000| (str_length >>9)) <<32;\n//we have 9 bits left to code, which we do on the string buffer\n//using two bytes\ncurrent_string_buf_loc[0] =uint8_t(32+ ((str_length &0x1f0) >>4))\ncurrent_string_buf_loc[1] =32+uint8_t(str_length &0xf);\n}\n//NULL termination is still handy if you expect all your strings to\n//be NULL terminated? It comes at a small cost\n*dst =0;\ncurrent_string_buf_loc = dst +1;\n//be NULL terminated? It comes at a small cost and if it is\n//never used, we might as well drop it.\n//*dst = 0;\n//current_string_buf_loc = dst + 1;\nreturntrue;\n}\n\n"}
{"Commit title": "Fix benchfeatures.rb on Ruby 1.9", "Commit body": "@@ -1,3 +1,5 @@\n# encoding: utf-8\n\nclassChunkWriter\ndefinitialize(output_dir,miss_templates,file_size=640*1000,block_size=64)\n@@output_dir=output_dir\n@@ -12,7 +14,7 @@ def prepare_chunk(chunks, include_newline)\nend.join(\"\")\nend\n\ndefwrite_files(filename,start1,repeat1,end1,repeat2:'',include_newline:true)\ndefwrite_files(filename,start1,repeat1,end1,repeat2='',include_newline=true)\nstart1=prepare_chunk(start1,include_newline)\nrepeat1=prepare_chunk(repeat1,include_newline)\nend1=prepare_chunk(end1,include_newline)\n@@ -87,8 +89,8 @@ def write_chunks(file, start1, repeat1, end1, size)\nmiss_templates=File.expand_path(\"miss-templates\",File.dirname(__FILE__))\nDir.mkdir(output_dir)unlessFile.directory?(output_dir)\nw=ChunkWriter.new(output_dir,miss_templates)\nw.write_files\"utf-8\",'[\"֏\",\"֏\",{}',',\"֏\",\"֏\",{}',',\"֏\",\"֏\",\"֏\"]',repeat2:',\"ab\",\"ab\",{}'\nw.write_files\"escape\",'[\"\\\\\"\",\"\\\\\"\",{}',',\"\\\\\"\",\"\\\\\"\",{}',',\"\\\\\"\",\"\\\\\"\",\"\\\\\"\"]',repeat2:',\"ab\",\"ab\",{}'\nw.write_files\"utf-8\",'[\"֏\",\"֏\",{}',',\"֏\",\"֏\",{}',',\"֏\",\"֏\",\"֏\"]',',\"ab\",\"ab\",{}'\nw.write_files\"escape\",'[\"\\\\\"\",\"\\\\\"\",{}',',\"\\\\\"\",\"\\\\\"\",{}',',\"\\\\\"\",\"\\\\\"\",\"\\\\\"\"]',',\"ab\",\"ab\",{}'\nw.write_files\"0-structurals\",'\"ab\"','',''\n# w.write_files \"1-structurals\",  [ '[', '\"ab\"' ], [ ',', '\"ab\"' ], [ ',', '{', '}', ']' ]\n# w.write_files \"2-structurals\",  '[\"ab\"', ',\"ab\"', [',{', '}]']\n"}
{"Commit title": "Adding explicit constructor.", "Commit body": "@@ -128,6 +128,7 @@ struct event_collector {\nreturnlinux_events.is_working();\n}\n#else\nevent_collector() {}\nboolhas_events() {\nreturnfalse;\n}\n"}
{"Commit title": "Adding explicit constructor.", "Commit body": "@@ -128,6 +128,7 @@ struct event_collector {\nreturnlinux_events.is_working();\n}\n#else\nevent_collector() {}\nboolhas_events() {\nreturnfalse;\n}\n"}
{"Commit title": "CMake: Fix LTO configuration", "Commit body": "@@ -1,9 +1,4 @@\ncmake_minimum_required(VERSION3.9)#CMP0069 NEW\ninclude(CheckIPOSupported)\ncheck_ipo_supported(RESULTltoresult)\nif(ltoresult)\nset(CMAKE_INTERPROCEDURAL_OPTIMIZATION TRUE)\nendif()\n\n#usage: cmake -DSIMDJSON_DISABLE_AVX=on ..\noption(SIMDJSON_DISABLE_AVX\"Forcefully disable AVX even if hardware supports it\"OFF)\n@@ -27,6 +22,12 @@ set(PROJECT_VERSION_PATCH 1)\nset(SIMDJSON_LIB_VERSION\"0.2.1\"CACHESTRING\"simdjson library version\")\nset(SIMDJSON_LIB_SOVERSION\"0\"CACHESTRING\"simdjson library soversion\")\n\ninclude(CheckIPOSupported)\ncheck_ipo_supported(RESULTltoresult)\nif(ltoresult)\nset(CMAKE_INTERPROCEDURAL_OPTIMIZATION TRUE)\nendif()\n\nif(NOTMSVC)\noption(SIMDJSON_BUILD_STATIC\"Build a static library\"OFF)#turning it on disables the production of a dynamic library\nelse()\n"}
{"Commit title": "Switching ARM64 to lookup algo. for UTF8 validation.", "Commit body": "@@ -29,7 +29,7 @@ really_inline void find_whitespace_and_operators(\nwhitespace = v.map([&](simd8<uint8_t> _v) {return_v.any_bits_set(0x18); }).to_bitmask();\n}\n\n#include\"generic/utf8_fastvalidate_algorithm.h\"\n#include\"generic/utf8_lookup_algorithm.h\"\n#include\"generic/stage1_find_marks.h\"\n\n}//namespace simdjson::arm64\n"}
{"Commit title": "This seems beneficial.", "Commit body": "@@ -435,24 +435,52 @@ really_inline void flatten_bits(uint32_t *base_ptr, uint32_t &base,\nuint32_tidx,uint64_tbits) {\nuint32_tcnt =hamming(bits);\nuint32_tnext_base = base + cnt;\nwhile(bits !=0u) {\nbase_ptr[base +0] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nidx -=64;\nbase_ptr += base;\nif(bits !=0) {\nbase_ptr[0] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +1] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[1] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +2] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[2] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +3] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[3] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +4] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[4] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +5] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[5] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +6] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[6] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +7] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[7] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase +=8;\nbase_ptr +=8;\n}\nif(cnt >8) {\nbase_ptr[0] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[1] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[2] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[3] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[4] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[5] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[6] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[7] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr +=8;\n}\nif(cnt >16) {//unluckly\ndo{\nbase_ptr[0] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr++;\n}while(bits !=0);\n}\nbase = next_base;\n}\n"}
{"Commit title": "Parse 18-digit unsigned integers quickly", "Commit body": "@@ -542,6 +542,22 @@ really_inline bool parse_number(UNUSED const uint8_t *const src,\n}\n}else{\nif(unlikely(digit_count >=18)) {//this is uncommon!!!\n//18-digit positive numbers never overflow, but we have to figure out whether they will\n//fit in a signed integer or not.\nif(!negative && digit_count ==18) {\nif(i <=uint64_t(INT64_MAX)) {\nwriter.append_s64(i);\n#ifdefJSON_TEST_NUMBERS//for unit testing\nfound_integer(i, src);\n#endif\n}else{\nwriter.append_u64(i);\n#ifdefJSON_TEST_NUMBERS//for unit testing\nfound_unsigned_integer(i, src);\n#endif\n}\nreturntrue;\n}\n//there is a good chance that we had an overflow, so we need\n//need to recover: we parse the whole thing again.\nboolsuccess =parse_large_integer(src, writer, found_minus);\n"}
{"Commit title": "Redesigning visit_primitive so that it is optimized for strings and", "Commit body": "@@ -303,15 +303,17 @@ simdjson_warn_unused simdjson_inline error_code json_iterator::visit_root_primit\n}\ntemplate<typenameV>\nsimdjson_warn_unused simdjson_inline error_codejson_iterator::visit_primitive(V &visitor,constuint8_t*value)noexcept{\n//Use the fact that most scalars are going to be either strings or numbers.\nif(*value =='\"') {\nreturnvisitor.visit_string(*this, value);\n}elseif(((*value -'0')  <10) || (*value =='-')) {\nreturnvisitor.visit_number(*this, value);\n}\n//true, false, null are uncommon.\nswitch(*value) {\ncase'\"':returnvisitor.visit_string(*this, value);\ncase't':returnvisitor.visit_true_atom(*this, value);\ncase'f':returnvisitor.visit_false_atom(*this, value);\ncase'n':returnvisitor.visit_null_atom(*this, value);\ncase'-':\ncase'0':case'1':case'2':case'3':case'4':\ncase'5':case'6':case'7':case'8':case'9':\nreturnvisitor.visit_number(*this, value);\ndefault:\nlog_error(\"Non-value found when value was expected!\");\nreturnTAPE_ERROR;\n"}
{"Commit title": "Adding test.", "Commit body": "@@ -349,6 +349,27 @@ namespace document_stream_tests {\n}\n\n\nboolissue1668() {\nTEST_START();\nautojson =R\"([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100])\"_padded;\n\nondemand::parser odparser;\nondemand::document_stream odstream;\nautooderror = odparser.iterate_many(json.data(), json.length(),50).get(odstream);\n\nif(oderror) { std::cerr <<\"ondemand iterate_many error:\"<< oderror << std::endl;returnfalse; }\n\nfor(auto&doc: odstream) {\nondemand::value val;\nautoerr = doc.at_pointer(\"/40\").get(val);\nif(err) {\nstd::cout <<\"ondemand: error accessing jsonpointer:\"<< err << std::endl;\n}else{\nstd::cout <<\"ondemand:\"<< val << std::endl;\n}\n}\nTEST_SUCCEED();\n}\nbooldocument_stream_utf8_test() {\nTEST_START();\nfflush(NULL);\n@@ -424,6 +445,7 @@ namespace document_stream_tests {\n\nboolrun() {\nreturn\nissue1668() &&\nsimple_document_iteration() &&\nsimple_document_iteration_multiple_batches() &&\nsimple_document_iteration_with_parsing() &&\n"}
{"Commit title": "This enables /Ob3 under Visual Studio 2019", "Commit body": "@@ -81,6 +81,12 @@ else()\n#Recent version of Visual Studio expected (2017, 2019...). Prior versions are unsupported.\ntarget_compile_options(simdjson-internal-flagsINTERFACE/WX /W3 /sdl)\nendif()\nif(${CMAKE_VS_PLATFORM_TOOLSET}STRGREATER\"v141\")\nstring(REPLACE\"/Ob2\"\"/Ob2\"CMAKE_CXX_FLAGS_RELEASE\"${CMAKE_CXX_FLAGS_RELEASE}\")#VS 2019\nmessage(STATUS\" Visual Studio 2019 detected, new compile flags${CMAKE_CXX_FLAGS_RELEASE}\")\nendif()\n\n\nelse()\ntarget_compile_options(simdjson-internal-flagsINTERFACE-fPIC)\ntarget_compile_options(simdjson-internal-flagsINTERFACE-Werror -Wall -Wextra -Weffc++)\n"}
{"Commit title": "Revamp design documentation to match new design", "Commit body": "@@ -29,11 +29,6 @@ auto doc = parser.iterate(json);\nfor(autotweet : doc[\"statuses\"]) {\nstd::string_view text        = tweet[\"text\"];\nstd::string_view screen_name = tweet[\"user\"][\"screen_name\"];\nstd::string_view screen_name;\n{\nondemand::object user        = tweet[\"user\"];\nscreen_name                  = user[\"screen_name\"];\n}\nuint64_t         retweets    = tweet[\"retweet_count\"];\nuint64_t         favorites   = tweet[\"favorite_count\"];\ncout << screen_name << \" (\" << retweets << \" retweets / \" << favorites << \" favorites): \" << text << endl;\n@@ -66,7 +61,10 @@ Such code would be apply to a JSON document such as the following JSON mimicking\n\"retweet_count\":82,\n\"favorite_count\":42\n}\n]\n],\n\"search_metadata\": {\n\"count\":100,\n}\n}\n```\n\n@@ -91,7 +89,6 @@ The On Demand approach is designed around several principles:\n***Validate What You Use:**On Demand deliberately validates the values you use and the structure leading to it, but nothing else. The goal is a guarantee that the value you asked for is the correct one and is not malformed: there must be no confusion over whether you got the right value.\n\n\n\nTo understand why On Demand is different, it is helpful to review the major\napproaches to parsing and parser APIs in use today.\n\n@@ -119,8 +116,7 @@ for (auto tweet : doc[\"statuses\"]) {\nstd::string_view text        = tweet[\"text\"];\nstd::string_view screen_name = tweet[\"user\"][\"screen_name\"];\nuint64_t         retweets    = tweet[\"retweet_count\"];\nuint64_t         favorites   = tweet[\"favorite_count\"];\ncout << screen_name << \" (\" << retweets << \" retweets / \" << favorites << \" favorites): \" << text << endl;\ncout << screen_name << \" (\" << retweets << \" retweets): \" << text << endl;\n}\n```\n\n@@ -273,9 +269,10 @@ To help visualize the algorithm, we'll walk through the example C++ given at the\n```json\n{\n\"statuses\": [\n{ \"id\": 1, \"text\": \"first!\", \"user\": { \"screen_name\": \"lemire\", \"name\": \"Daniel\" }, \"favorite_count\": 100, \"retweet_count\": 40 },\n{ \"id\": 2, \"text\": \"second!\", \"user\": { \"screen_name\": \"jkeiser2\", \"name\": \"John\" }, \"favorite_count\": 2, \"retweet_count\": 3 }\n]\n{ \"id\": 1, \"text\": \"first!\", \"user\": { \"screen_name\": \"lemire\", \"name\": \"Daniel\" }, \"retweet_count\": 40 },\n{ \"id\": 2, \"text\": \"second!\", \"user\": { \"screen_name\": \"jkeiser2\", \"name\": \"John\" }, \"retweet_count\": 3 }\n],\n\"search_metadata\": { \"count\": 2 }\n}\n```\n\n@@ -318,57 +315,84 @@ To help visualize the algorithm, we'll walk through the example C++ given at the\nrely on error chaining, so it is possible to delay error checks: we shall shortly explain error\nchaining more fully.\n\nNOTE: You should always have such a`document`instance (here`doc`) and it should remain in scope for the duration\nof your parsing function. E.g., you should not use the returned document as a temporary (e.g.,`auto x = parser.iterate(json).get_object();`)\nfollowed by other operations as the destruction of the`document`instance makes all of the derived instances\nill-defined.\n>NOTE: You should always have such a`document`instance (here`doc`) and it should remain in scope for the duration\n>of your parsing function. E.g., you should not use the returned document as a temporary (e.g.,`auto x = parser.iterate(json).get_object();`)\n>followed by other operations as the destruction of the`document`instance makes all of the derived instances\n>ill-defined.\n\nAt this point, the iterator is at the start of the JSON:\n\n```json\n{\n^ (depth 1)\n\n\"statuses\": [\n{\"id\":1,\"text\":\"first!\",\"user\": {\"screen_name\":\"lemire\",\"name\":\"Daniel\"},\"retweet_count\":40},\n{\"id\":2,\"text\":\"second!\",\"user\": {\"screen_name\":\"jkeiser2\",\"name\":\"John\"},\"retweet_count\":3}\n],\n\"search_metadata\": {\"count\":2}\n}\n```\n\n3. We iterate over the \"statuses\" field using a typical C++ iterator, reading past the initial\n`{ \"statuses\": [ {`.\n\n```c++\nfor (ondemand::object tweet : doc[\"statuses\"]) {\n```\nThis shorthand does much, and it is helpful to see what it expands to.\nComments in front of each one explain what's going on:\n```c++\n// Validate that the top-level value is an object: check for {\nondemand::object top = doc.get_object();\n\n// Find the field statuses by:\n// 1. Check whether the object is empty (check for }). (We do not really need to do this unless the key lookup fails!)\n// 2. Check if we're at the field by looking for the string \"statuses\" using byte-by-byte comparison.\n// 3. Validate that there is a `:` after it.\nauto tweets_field = top[\"statuses\"];\n\n// Validate that the field value is an array: check for [\n// Also mark the array as finished if there is a ] next, which would cause the while () statement to exit immediately.\nondemand::array tweets = tweets_field.get_array();\n// These three method calls do nothing substantial (the real checking happens in get_array() and ++)\n// != checks whether the array is marked as finished (if we have found a ]).\nondemand::array_iterator tweets_iter = tweets.begin();\nwhile (tweets_iter != tweets.end()) {\nauto tweet_value = *tweets_iter;\n\n// Validate that the array element is an object: check for {\nondemand::object tweet = tweet_value.get_object();\n...\n}\n```\nWhat is not explained in this code expansion is*error chaining*.\nGenerally, you can use`document`methods on a`simdjson_result<...>`value; any errors will\njust be passed down the chain. Many method calls\ncan be chained in this manner. So`for (object tweet : doc[\"statuses\"])`, which is the equivalent of\n`object tweet = *(doc.get_object()[\"statuses\"].get_array().begin()).get_object()`, could fail in any of\n6 method calls, and the error will only be checked at the end,\nwhen you attempt to cast the final`simdjson_result<object>`to object. Upon casting, an exception is\nthrown if there was an error.\n\nNOTE: while the document can be queried once for a key as if it were an object, it is not an actual object\ninstance. If you need to treat it as an object (e.g., to query more than one keys), you can cast it as\nsuch`ondemand::object root_object = doc.get_object();`.\nThis shorthand does a lot, and it is helpful to see what it expands to.\nComments in front of each one explain what's going on:\n\n```c++\n// Validate that the top-level value is an object: check for {. Increase depth to 2 (root > field).\nondemand::object top = doc.get_object();\n\n// Find the field statuses by:\n// 1. Check whether the object is empty (check for }). (We do not really need to do this unless\n//    the key lookup fails!)\n// 2. Check if we're at the field by looking for the string \"statuses\" using byte-by-byte comparison.\n// 3. Validate that there is a `:` after it.\nauto tweets_field = top[\"statuses\"];\n\n// - Validate that the field value is an array: check for [\n// - If the array is empty (if there is a ] next), decrease depth back to 0.\n// - If not, increase depth to 3 (root > statuses > tweet).\nondemand::array tweets = tweets_field.get_array();\n// These three method calls do nothing substantial (the real checking happens in get_array() and ++)\n// != checks whether the array is finished (if we found a ] and decreased depth back to 0).\nondemand::array_iterator tweets_iter = tweets.begin();\nwhile (tweets_iter != tweets.end()) {\nauto tweet_value = *tweets_iter;\n\n// - Validate that the array element is an object: check for {\n// - If the object is empty (if there is a } next), decrease depth back to 1.\n// - If not, increase depth to 4 (root > statuses > tweet > field).\nondemand::object tweet = tweet_value.get_object();\n...\n}\n```\n\n> NOTE: What is not explained in this code expansion is *error chaining*.\n> Generally, you can use `document` methods on a `simdjson_result<...>` value; any errors will\n> just be passed down the chain. Many method calls\n> can be chained in this manner. So `for (object tweet : doc[\"statuses\"])`, which is the equivalent of\n> `object tweet = *(doc.get_object()[\"statuses\"].get_array().begin()).get_object()`, could fail in any of\n> 6 method calls, and the error will only be checked at the end,\n> when you attempt to cast the final `simdjson_result<object>` to object. Upon casting, an exception is\n> thrown if there was an error.\n\n```json\n{\n\"statuses\": [\n{\"id\":1,\"text\":\"first!\",\"user\": {\"screen_name\":\"lemire\",\"name\":\"Daniel\"},\"retweet_count\":40},\n^ (depth 4 - root > statuses > tweet > field)\n\n{\"id\":2,\"text\":\"second!\",\"user\": {\"screen_name\":\"jkeiser2\",\"name\":\"John\"},\"retweet_count\":3}\n],\n\"search_metadata\": {\"count\":2}\n}\n```\n\n4. We get the `\"text\"` field as a string.\n\n@@ -382,45 +406,109 @@ To help visualize the algorithm, we'll walk through the example C++ given at the\n\nThe second field is matched (`\"text\"`), so we validate the `:` and move to the actual value.\n\nNOTE:`[\"text\"]`does a*raw match*, comparing the key directly against the raw JSON. This means\nthat keys with escapes in them may not be matched and the letter case must match exactly.\n> NOTE: `[\"text\"]` does a *raw match*, comparing the key directly against the raw JSON. This\n> allows simdjson to do field lookup very, very quickly when the keys you want to match have\n> letters, numbers and punctuation. However, this means that fields with escapes in them will not\n> be matched.\n\nTo convert to a string, we check for `\"` and use simdjson's fast unescaping algorithm to copy\n`first!` (plus a terminating `\\0`) into a buffer managed by the `document`. This buffer stores\nall strings from a single iteration. The next string will be written after the `\\0`.\n\nA `string_view` is returned which points to that buffer, and contains the length.\n\nWe advance to the comma, and decrease depth to 3 (root > statuses > tweet).\n\nAt this point, we are here in the JSON:\n\n```json\n{\n\"statuses\": [\n{\"id\":1,\"text\":\"first!\",\"user\": {\"screen_name\":\"lemire\",\"name\":\"Daniel\"},\"retweet_count\":40},\n^ (depth 2 - root > statuses > tweet)\n\n{\"id\":2,\"text\":\"second!\",\"user\": {\"screen_name\":\"jkeiser2\",\"name\":\"John\"},\"retweet_count\":3}\n],\n\"search_metadata\": {\"count\":2}\n}\n```\n\n4. We get the `\"screen_name\"` from the `\"user\"` object.\n\n```c++\nondemand::object user        = tweet[\"user\"];\nscreen_name                  = user[\"screen_name\"];\n```\n\nFirst, `[\"user\"]`checks whether there are any more object fields by lookingforeither `,`or\n`}`. Then it matches `\"user\"`andvalidates the `:`.\nFirst, `[\"user\"]`finds the `,`, discovers the next key is `\"user\"`, validates that the `:`\nis there, and increases depth to 4 (root > statuses > tweet > field).\n\n`[\"screen_name\"]` then converts to object, checkingfor`{`, and finds`\"screen_name\"`.\nNext, the cast to ondemand::object checks for `{` and increases depth to 5 (root > statuses >\ntweet > user > field).\n\n`[\"screen_name\"]` finds the first field `\"screen_name\"` and validates the `:`.\n\nTo convert the result to usable string (i.e., the screen name `lemire`), the characters are written to the document's\nstring buffer (after possibly escaping them), which now has *two* string_views pointing into it, and looks like `first!\\0lemire\\0`.\n\nFinally, the temporary user object is destroyed, causing it to skip the remainder of the object\n(`}`).\nThe iterator advances to the comma and decreases depth back to 4 (root > statuses > tweet > user).\n\nAt this point, the iterator is here in the JSON:\n\nNOTE: You may only have one active array or object active at any given time. An array or an object becomes\nactive when the`ondemand::object`or`ondemand::array`is created, and it releases its 'focus' when\nits destructor is called. If you create an array or an object located inside a parent object or array,\nthe child array or object becomes active while the parent becomes temporarily inactive. If you access\nseveral sibling objects or arrays, you must ensure that the destructor is called by scoping each access\n(see Iteration Safety section below for further details).\n```json\n{\n\"statuses\": [\n{\"id\":1,\"text\":\"first!\",\"user\": {\"screen_name\":\"lemire\",\"name\":\"Daniel\"},\"retweet_count\":40},\n^ (depth 4 - root > statuses > tweet > user)\n\n{\"id\":2,\"text\":\"second!\",\"user\": {\"screen_name\":\"jkeiser2\",\"name\":\"John\"},\"retweet_count\":3}\n],\n\"search_metadata\": {\"count\":2}\n}\n```\n\n5.We get`\"retweet_count\"`and`\"favorite_count\"`as unsignedintegers.\n5. We get `\"retweet_count\"` asanunsignedinteger.\n\n```c++\nuint64_t         retweets    = tweet[\"retweet_count\"];\nuint64_tfavorites   = tweet[\"favorite_count\"];\n```\n\nFirst, `[\"retweet_count\"]` checks whether the previous field value is finished (if it was, depth\nwould be 3 (root > statuses > tweet). Since it's not, we skip JSON until depth is 3. This brings\nthe iterator to the `,` after the user object:\n\n```json\n{\n\"statuses\": [\n{\"id\":1,\"text\":\"first!\",\"user\": {\"screen_name\":\"lemire\",\"name\":\"Daniel\"},\"retweet_count\":40},\n^ (depth 4 - root > statuses > tweet > user)\n\n{\"id\":2,\"text\":\"second!\",\"user\": {\"screen_name\":\"jkeiser2\",\"name\":\"John\"},\"retweet_count\":3}\n],\n\"search_metadata\": {\"count\":2}\n}\n```\n\nBecause of the cast to uint64_t, simdjson knows it's parsing an unsigned integer. This lets\nus use a fast parser which *only* knows how to parse digits. It validates that it is an integer\nby rejecting negative numbers, strings, and other values based on the fact that they are not the\ndigits 0-9. This type specificity is part of why parsing with on demand is so fast: you lose all\nthe code that has to understand those other types.\n\nThe iterator is advanced to the `}`, and depth decreased back to 3 (root > statuses > tweet).\n\nAt this point, we are here in the JSON:\n\n```json\n{\n\"statuses\": [\n{\"id\":1,\"text\":\"first!\",\"user\": {\"screen_name\":\"lemire\",\"name\":\"Daniel\"},\"retweet_count\":40},\n^ (depth 3 - root > statuses > tweet)\n\n{\"id\":2,\"text\":\"second!\",\"user\": {\"screen_name\":\"jkeiser2\",\"name\":\"John\"},\"retweet_count\":3}\n],\n\"search_metadata\": {\"count\":2}\n}\n```\n\n6. We loop to the next tweet.\n@@ -441,25 +529,77 @@ To help visualize the algorithm, we'll walk through the example C++ given at the\n}\n```\n\nFirst, the`tweet`destructor runs, skipping the remainder of the object which in this case is\njust`}`.\nFirst, `iter++` (remember, this is the array of tweets) checks whether the previous object was\nfully iterated. It was not--depth is 3 (root > statuses > tweet), so we skip until it's 2--which\nin this case just means consuming the `}`, leaving the iterator at the next comma. Depth is now 2\n(root > statuses).\n\nNext, `iter++` finds the `,` and advances past it to the `{`, increasing depth to 3 (root >\nstatuses > tweet).\n\nFinally, `ondemand::object tweet = *iter` validates the `{` and increases depth to 4 (root >\nstatuses > tweet > field). This leaves the iterator here:\n\n```json\n{\n\"statuses\": [\n{\"id\":1,\"text\":\"first!\",\"user\": {\"screen_name\":\"lemire\",\"name\":\"Daniel\"},\"retweet_count\":40},\n{\"id\":2,\"text\":\"second!\",\"user\": {\"screen_name\":\"jkeiser2\",\"name\":\"John\"},\"retweet_count\":3}\n^ (depth 3 - root > statuses > tweet)\n],\n\"search_metadata\": {\"count\":2}\n}\n```\n\n7. This tweet is processed just like the previous one, leaving the iterator here:\n\n```json\n{\n\"statuses\": [\n{\"id\":1,\"text\":\"first!\",\"user\": {\"screen_name\":\"lemire\",\"name\":\"Daniel\"},\"retweet_count\":40},\n{\"id\":2,\"text\":\"second!\",\"user\": {\"screen_name\":\"jkeiser2\",\"name\":\"John\"},\"retweet_count\":3}\n^ (depth 3 - root > statuses > tweet)\n],\n\"search_metadata\": {\"count\":2}\n}\n```\n\nNext,`iter++`checks whether there are more values and finds`,`. The loop continues.\n8. The loop ends. Recall the relevant parts of the statuses loop:\n\n```c++\nwhile (iter != statuses.end()) {\nondemand::object tweet = *iter;\n...\niter++;\n}\n```\n\nFinally,`ondemand::object tweet = *iter`checks for`{`andreturns the object.\nFirst, `iter++` finishes up any children, consuming the `}` andleaving depth at 2 (root > statuses).\n\nThis tweet is processed just like the previous one.\nNext, `iter++` notices the `]` and ends the array by decreasing depth to 1. This leaves the iterator\nhere in the JSON:\n\n7.We finish the last tweet.\n```json\n{\n\"statuses\": [\n{\"id\":1,\"text\":\"first!\",\"user\": {\"screen_name\":\"lemire\",\"name\":\"Daniel\"},\"retweet_count\":40},\n{\"id\":2,\"text\":\"second!\",\"user\": {\"screen_name\":\"jkeiser2\",\"name\":\"John\"},\"retweet_count\":3}\n],\n^ (depth 1 - root)\n\"search_metadata\": {\"count\":2}\n}\n```\n\nAt the end of the loop, the`tweet`is first destroyed, skipping the remainder of the tweet\nobject (`}`).\n9. The remainder of the file is skipped.\n\nThe`iter++`instruction from`for (ondemand::object tweet : doc[\"statuses\"])`then checks whether there are\nmore values and finds that there are none (`]`). It marks the array iteration as finished and the for\nloop terminates.\nBecause no more action is taken, JSON processing stops: processing only occurs when you ask for\nvalues.\n\nThen the outer object is destroyed, skipping everything up to the`}`.\nThis means you can very efficiently do things like read a single value from a JSON file, or take\nthe top N, for example. It also means the things you don't use won't be fully validated. This is\na general principle of On Demand: don't validate what you don't use. We still fully validate\nvalues you do use, however, as well as the objects and arrays that lead to them, so that you can\nbe sure you get the information you need.\n\nDesign Features\n---------------\n"}
{"Commit title": "Move document stream state to implementation", "Commit body": "@@ -73,6 +73,8 @@ class json_structural_indexer {\nreally_inlinevoidstep(constuint8_t*block, buf_block_reader<STEP_SIZE> &reader)noexcept;\nreally_inlinevoidnext(simd::simd8x64<uint8_t> in, json_block block,size_tidx);\nreally_inline error_codefinish(dom_parser_implementation &parser,size_tidx,size_tlen,boolpartial);\nstaticreally_inlineuint32_tfind_next_document_index(dom_parser_implementation &parser);\nstaticreally_inlinesize_ttrim_partial_utf8(constuint8_t*buf,size_tlen);\n\njson_scanner scanner{};\nutf8_checker checker{};\n@@ -195,4 +197,91 @@ really_inline error_code json_structural_indexer::finish(dom_parser_implementati\nreturnchecker.errors();\n}\n\n/**\n* This algorithm is used to quickly identify the last structural position that\n* makes up a complete document.\n*\n* It does this by going backwards and finding the last *document boundary* (a\n* place where one value follows another without a comma between them). If the\n* last document (the characters after the boundary) has an equal number of\n* start and end brackets, it is considered complete.\n*\n* Simply put, we iterate over the structural characters, starting from\n* the end. We consider that we found the end of a JSON document when the\n* first element of the pair is NOT one of these characters: '{' '[' ';' ','\n* and when the second element is NOT one of these characters: '}' '}' ';' ','.\n*\n* This simple comparison works most of the time, but it does not cover cases\n* where the batch's structural indexes contain a perfect amount of documents.\n* In such a case, we do not have access to the structural index which follows\n* the last document, therefore, we do not have access to the second element in\n* the pair, and means that we cannot identify the last document. To fix this\n* issue, we keep a count of the open and closed curly/square braces we found\n* while searching for the pair. When we find a pair AND the count of open and\n* closed curly/square braces is the same, we know that we just passed a\n* complete\n* document, therefore the last json buffer location is the end of the batch\n*/\nreally_inlineuint32_tjson_structural_indexer::find_next_document_index(dom_parser_implementation &parser) {\n//TODO don't count separately, just figure out depth\nautoarr_cnt =0;\nautoobj_cnt =0;\nfor(autoi = parser.n_structural_indexes-1; i >0; i--) {\nautoidxb = parser.structural_indexes[i];\nswitch(parser.buf[idxb]) {\ncase':':\ncase',':\ncontinue;\ncase'}':\nobj_cnt--;\ncontinue;\ncase']':\narr_cnt--;\ncontinue;\ncase'{':\nobj_cnt++;\nbreak;\ncase'[':\narr_cnt++;\nbreak;\n}\nautoidxa = parser.structural_indexes[i -1];\nswitch(parser.buf[idxa]) {\ncase'{':\ncase'[':\ncase':':\ncase',':\ncontinue;\n}\n//Last document is complete, so the next document will appear after!\nif(!arr_cnt && !obj_cnt) {\nreturnparser.n_structural_indexes;\n}\n//Last document is incomplete; mark the document at i + 1 as the next one\nreturni;\n}\nreturn0;\n}\n\n//Skip the last character if it is partial\nreally_inlinesize_tjson_structural_indexer::trim_partial_utf8(constuint8_t*buf,size_tlen) {\nif(unlikely(len <3)) {\nswitch(len) {\ncase2:\nif(buf[len-1] >=0b11000000) {returnlen-1; }//2-, 3- and 4-byte characters with only 1 byte left\nif(buf[len-2] >=0b11100000) {returnlen-2; }//3- and 4-byte characters with only 2 bytes left\nreturnlen;\ncase1:\nif(buf[len-1] >=0b11000000) {returnlen-1; }//2-, 3- and 4-byte characters with only 1 byte left\nreturnlen;\ncase0:\nreturnlen;\n}\n}\nif(buf[len-1] >=0b11000000) {returnlen-1; }//2-, 3- and 4-byte characters with only 1 byte left\nif(buf[len-2] >=0b11100000) {returnlen-2; }//3- and 4-byte characters with only 1 byte left\nif(buf[len-3] >=0b11110000) {returnlen-3; }//4-byte characters with only 3 bytes left\nreturnlen;\n}\n\n}//namespace stage1"}
{"Commit title": "Short-circuit empty objects/arrays", "Commit body": "@@ -84,6 +84,30 @@ struct structural_parser : structural_iterator {\nend_scope(internal::tape_type::ROOT, internal::tape_type::ROOT);\n}\n\nreally_inlinevoidempty_container(internal::tape_type start, internal::tape_type end) {\nautostart_index =next_tape_index();\ntape.append(start_index+2, start);\ntape.append(start_index, end);\n}\nWARN_UNUSED really_inlineboolempty_object() {\nif(peek_next_char() =='}') {\nadvance_char();\nlog_value(\"empty object\");\nempty_container(internal::tape_type::START_OBJECT, internal::tape_type::END_OBJECT);\nreturntrue;\n}\nreturnfalse;\n}\nWARN_UNUSED really_inlineboolempty_array() {\nif(peek_next_char() ==']') {\nadvance_char();\nlog_value(\"empty array\");\nempty_container(internal::tape_type::START_ARRAY, internal::tape_type::END_ARRAY);\nreturntrue;\n}\nreturnfalse;\n}\n\n//increment_count increments the count of keys in an object or values in an array.\n//Note that if you are at the level of the values or elements, the count\n//must be increment in the preceding depth (depth-1) where the array or\n@@ -261,10 +285,13 @@ WARN_UNUSED static really_inline error_code parse_structurals(dom_parser_impleme\n//Read first value\n//\nswitch(parser.current_char()) {\ncase'{':\ncase'{': {\nif(parser.empty_object()) {gotofinish; }\nSIMDJSON_TRY( parser.start_object(false) );\ngotoobject_begin;\ncase'[':\n}\ncase'[': {\nif(parser.empty_array()) {gotofinish; }\nSIMDJSON_TRY( parser.start_array(false) );\n//Make sure the outer array is closed before continuing; otherwise, there are ways we could get\n//into memory corruption. See https://github.com/simdjson/simdjson/issues/906\n@@ -274,6 +301,7 @@ WARN_UNUSED static really_inline error_code parse_structurals(dom_parser_impleme\n}\n}\ngotoarray_begin;\n}\ncase'\"':SIMDJSON_TRY( parser.parse_string() );gotofinish;\ncase't':SIMDJSON_TRY( parser.parse_root_true_atom() );gotofinish;\ncase'f':SIMDJSON_TRY( parser.parse_root_false_atom() );gotofinish;\n@@ -291,25 +319,27 @@ WARN_UNUSED static really_inline error_code parse_structurals(dom_parser_impleme\n//Object parser states\n//\nobject_begin:\nswitch(parser.advance_char()) {\ncase'\"': {\nparser.increment_count();\nSIMDJSON_TRY( parser.parse_string(true) );\ngotoobject_key_state;\n}\ncase'}':\nparser.end_object();\ngotoscope_end;\ndefault:\nif(parser.advance_char() !='\"') {\nparser.log_error(\"Object does not start with a key\");\nreturnTAPE_ERROR;\n}\nparser.increment_count();\nSIMDJSON_TRY( parser.parse_string(true) );\ngotoobject_key_state;\n\nobject_key_state:\nif(unlikely( parser.advance_char() !=':')) { parser.log_error(\"Missing colon after key in object\");returnTAPE_ERROR; }\nswitch(parser.advance_char()) {\ncase'{':SIMDJSON_TRY( parser.start_object(false) );gotoobject_begin;\ncase'[':SIMDJSON_TRY( parser.start_array(false) );gotoarray_begin;\ncase'{': {\nif(parser.empty_object()) {break; };\nSIMDJSON_TRY( parser.start_object(false) );\ngotoobject_begin;\n}\ncase'[': {\nif(parser.empty_array()) {break; };\nSIMDJSON_TRY( parser.start_array(false) );\ngotoarray_begin;\n}\ncase'\"':SIMDJSON_TRY( parser.parse_string() );break;\ncase't':SIMDJSON_TRY( parser.parse_true_atom() );break;\ncase'f':SIMDJSON_TRY( parser.parse_false_atom() );break;\n@@ -347,17 +377,20 @@ WARN_UNUSED static really_inline error_code parse_structurals(dom_parser_impleme\n//Array parser states\n//\narray_begin:\nif(parser.peek_next_char() ==']') {\nparser.advance_char();\nparser.end_array();\ngotoscope_end;\n}\nparser.increment_count();\n\nmain_array_switch:\nswitch(parser.advance_char()) {\ncase'{':SIMDJSON_TRY( parser.start_object(true) );gotoobject_begin;\ncase'[':SIMDJSON_TRY( parser.start_array(true) );gotoarray_begin;\ncase'{': {\nif(parser.empty_object()) {break; };\nSIMDJSON_TRY( parser.start_object(true) );\ngotoobject_begin;\n}\ncase'[': {\nif(parser.empty_array()) {break; };\nSIMDJSON_TRY( parser.start_array(true) );\ngotoarray_begin;\n}\ncase'\"':SIMDJSON_TRY( parser.parse_string() );break;\ncase't':SIMDJSON_TRY( parser.parse_true_atom() );break;\ncase'f':SIMDJSON_TRY( parser.parse_false_atom() );break;\n"}
{"Commit title": "Fix fallback implementation", "Commit body": "@@ -73,8 +73,6 @@ class json_structural_indexer {\nreally_inlinevoidstep(constuint8_t*block, buf_block_reader<STEP_SIZE> &reader)noexcept;\nreally_inlinevoidnext(simd::simd8x64<uint8_t> in, json_block block,size_tidx);\nreally_inline error_codefinish(dom_parser_implementation &parser,size_tidx,size_tlen,boolpartial);\nstaticreally_inlineuint32_tfind_next_document_index(dom_parser_implementation &parser);\nstaticreally_inlinesize_ttrim_partial_utf8(constuint8_t*buf,size_tlen);\n\njson_scanner scanner{};\nutf8_checker checker{};\n@@ -197,91 +195,4 @@ really_inline error_code json_structural_indexer::finish(dom_parser_implementati\nreturnchecker.errors();\n}\n\n/**\n* This algorithm is used to quickly identify the last structural position that\n* makes up a complete document.\n*\n* It does this by going backwards and finding the last *document boundary* (a\n* place where one value follows another without a comma between them). If the\n* last document (the characters after the boundary) has an equal number of\n* start and end brackets, it is considered complete.\n*\n* Simply put, we iterate over the structural characters, starting from\n* the end. We consider that we found the end of a JSON document when the\n* first element of the pair is NOT one of these characters: '{' '[' ';' ','\n* and when the second element is NOT one of these characters: '}' '}' ';' ','.\n*\n* This simple comparison works most of the time, but it does not cover cases\n* where the batch's structural indexes contain a perfect amount of documents.\n* In such a case, we do not have access to the structural index which follows\n* the last document, therefore, we do not have access to the second element in\n* the pair, and means that we cannot identify the last document. To fix this\n* issue, we keep a count of the open and closed curly/square braces we found\n* while searching for the pair. When we find a pair AND the count of open and\n* closed curly/square braces is the same, we know that we just passed a\n* complete\n* document, therefore the last json buffer location is the end of the batch\n*/\nreally_inlineuint32_tjson_structural_indexer::find_next_document_index(dom_parser_implementation &parser) {\n//TODO don't count separately, just figure out depth\nautoarr_cnt =0;\nautoobj_cnt =0;\nfor(autoi = parser.n_structural_indexes-1; i >0; i--) {\nautoidxb = parser.structural_indexes[i];\nswitch(parser.buf[idxb]) {\ncase':':\ncase',':\ncontinue;\ncase'}':\nobj_cnt--;\ncontinue;\ncase']':\narr_cnt--;\ncontinue;\ncase'{':\nobj_cnt++;\nbreak;\ncase'[':\narr_cnt++;\nbreak;\n}\nautoidxa = parser.structural_indexes[i -1];\nswitch(parser.buf[idxa]) {\ncase'{':\ncase'[':\ncase':':\ncase',':\ncontinue;\n}\n//Last document is complete, so the next document will appear after!\nif(!arr_cnt && !obj_cnt) {\nreturnparser.n_structural_indexes;\n}\n//Last document is incomplete; mark the document at i + 1 as the next one\nreturni;\n}\nreturn0;\n}\n\n//Skip the last character if it is partial\nreally_inlinesize_tjson_structural_indexer::trim_partial_utf8(constuint8_t*buf,size_tlen) {\nif(unlikely(len <3)) {\nswitch(len) {\ncase2:\nif(buf[len-1] >=0b11000000) {returnlen-1; }//2-, 3- and 4-byte characters with only 1 byte left\nif(buf[len-2] >=0b11100000) {returnlen-2; }//3- and 4-byte characters with only 2 bytes left\nreturnlen;\ncase1:\nif(buf[len-1] >=0b11000000) {returnlen-1; }//2-, 3- and 4-byte characters with only 1 byte left\nreturnlen;\ncase0:\nreturnlen;\n}\n}\nif(buf[len-1] >=0b11000000) {returnlen-1; }//2-, 3- and 4-byte characters with only 1 byte left\nif(buf[len-2] >=0b11100000) {returnlen-2; }//3- and 4-byte characters with only 1 byte left\nif(buf[len-3] >=0b11110000) {returnlen-3; }//4-byte characters with only 3 bytes left\nreturnlen;\n}\n\n}//namespace stage1"}
{"Commit title": "Run all passing json against parse_many. Empty documents pass, too.", "Commit body": "@@ -83,17 +83,21 @@ bool validate(const char *dirname) {\nif(contains(\"EXCLUDE\", name)) {\n//skipping\nhow_many--;\n}elseif(starts_with(\"pass\", name)and(has_extension(extension1, name)orhas_extension(extension2, name))anderror) {\nis_file_as_expected[i] =false;\nprintf(\"warning: file %s should pass but it fails. Error is: %s\\n\",\nname,error_message(error));\nprintf(\"size of file in bytes: %zu\\n\", json.size());\neverything_fine =false;\n}elseif(starts_with(\"fail\", name)and(notstarts_with(\"fail10.json\", name))and!error) {\nis_file_as_expected[i] =false;\nprintf(\"warning: file %s should fail but it passes.\\n\", name);\nprintf(\"size of file in bytes: %zu\\n\", json.size());\neverything_fine =false;\n}elseif(starts_with(\"pass\", name)orstarts_with(\"fail10.json\", name)orstarts_with(\"fail70.json\", name)) {\nif(error) {\nis_file_as_expected[i] =false;\nprintf(\"warning: file %s should pass but it fails. Error is: %s\\n\",\nname,error_message(error));\nprintf(\"size of file in bytes: %zu\\n\", json.size());\neverything_fine =false;\n}\n}elseif(starts_with(\"fail\", name) ) {\nif(!error) {\nis_file_as_expected[i] =false;\nprintf(\"warning: file %s should fail but it passes.\\n\", name);\nprintf(\"size of file in bytes: %zu\\n\", json.size());\neverything_fine =false;\n}\n}\nfree(fullpath);\n}\n"}
{"Commit title": "The jsonstats utility becomes ever more powerful.", "Commit body": "@@ -1,4 +1,5 @@\n#include<iostream>\n#include<set>\n\n#include\"simdjson.h\"\n\n@@ -25,6 +26,7 @@ struct stat_s {\nsize_tunsigned_integer_count;\nsize_tfloat_count;\nsize_tstring_count;\nsize_tstring_byte_count;\nsize_tbackslash_count;\nsize_tnon_ascii_byte_count;\nsize_tobject_count;\n@@ -42,7 +44,11 @@ struct stat_s {\nsize_tmaximum_object_size;\nsize_tmaximum_array_size;\nsize_tstring_maximum_length;\nsize_trepeated_key_byte_count;\n\nboolvalid;\nstd::set<std::string_view> all_keys;\nstd::set<std::string_view> repeated_keys;\n};\n\nusingstat_t=structstat_s;\n@@ -80,6 +86,12 @@ void recurse(simdjson::dom::element element, stat_t &s, size_t depth) {\nsize_tcounter =0;\nfor(auto[key, value] : object) {\ncounter++;\nif(s.all_keys.find(key) != s.all_keys.end()) {\ns.repeated_keys.insert(key);\ns.repeated_key_byte_count+= key.size();\n}else{\ns.all_keys.insert(key);\n}\nif(is_ascii(key)) {\ns.ascii_key_count++;\ns.ascii_string_count++;\n@@ -91,6 +103,7 @@ void recurse(simdjson::dom::element element, stat_t &s, size_t depth) {\ns.string_maximum_length= key.size();\n}\ns.string_count++;\ns.string_byte_count+= key.size();\ns.key_count++;\nrecurse(value, s, depth +1);\n}\n@@ -133,10 +146,9 @@ void recurse(simdjson::dom::element element, stat_t &s, size_t depth) {\nif(is_ascii(v)) {\ns.ascii_string_count++;\n}\nstd::string_view strval;\nelement.get<std::string_view>().tie(strval, error);\nif(strval.size() > s.string_maximum_length) {\ns.string_maximum_length= strval.size();\ns.string_byte_count+= v.size();\nif(v.size() > s.string_maximum_length) {\ns.string_maximum_length= v.size();\n}\n}else{\nstd::cerr <<\"unrecognized node.\"<< std::endl;\n@@ -200,6 +212,7 @@ int main(int argc, char *argv[]) {\n\"unsigned_integer_count\"   = %10zu,\n\"float_count\"              = %10zu,\n\"string_count\"             = %10zu,\n\"string_byte_count\"        = %10zu,\n\"ascii_string_count\"       = %10zu,\n\"string_maximum_length\"    = %10zu,\n\"backslash_count\"          = %10zu,\n@@ -216,15 +229,19 @@ int main(int argc, char *argv[]) {\n\"key_count\"                = %10zu,\n\"ascii_key_count\"          = %10zu,\n\"key_maximum_length\"       = %10zu,\n\"key_distinct_count\"       = %10zu,\n\"repeated_key_distinct_count\"= %10zu,\n\"repeated_key_byte_count\"  = %10zu;\n\"maximum_depth\"            = %10zu\n}\n)\",\ns.integer_count,s.integer32_count,s.unsigned_integer32_count,s.unsigned_integer_count,\ns.float_count, s.string_count, s.ascii_string_count,\ns.float_count, s.string_count, s.string_byte_count, s.ascii_string_count,\ns.string_maximum_length, s.backslash_count, s.non_ascii_byte_count,\ns.object_count, s.maximum_object_size, s.array_count,\ns.maximum_array_size, s.null_count, s.true_count, s.false_count,\ns.byte_count, s.structural_indexes_count, s.key_count,\ns.ascii_key_count, s.key_maximum_length, s.maximum_depth);\ns.ascii_key_count, s.key_maximum_length, s.all_keys.size(), s.repeated_keys.size(),\ns.repeated_key_byte_count, s.maximum_depth);\nreturnEXIT_SUCCESS;\n}"}
{"Commit title": "This makes the float errors explicit.", "Commit body": "@@ -2,6 +2,7 @@\n\n#include<vector>\n#include<sstream>\n#include<limits>\n\ntemplate<typenameT>\nstaticbooldiff_results(benchmark::State &state,constT &result,constT &reference);\n@@ -19,6 +20,24 @@ struct result_differ {\n}\n};\n\ntemplate<>\nboolresult_differ<double>::diff(benchmark::State &state,constdouble&result,constdouble&reference) {\nif(result != reference) {\nstd::stringstream str;\n//We print it out using full precision.\nautoprior_precision = str.precision(std::numeric_limits<double>::max_digits10);\nstr <<\"result incorrect:\"<< result <<\"... reference:\"<< reference;\nstr.precision(prior_precision);//reset to prior state\nstr << std::hexfloat;//If there are floats, we want to see them in hexadecimal form!\nstr <<\"result incorrect (hexadecimal notation):\"<< result <<\"... reference:\"<< reference;\nstr << std::defaultfloat;//reset to prior state\nstate.SkipWithError(str.str().data());\nreturnfalse;\n}\nreturntrue;\n}\n\n\ntemplate<typenameT>\nstructresult_differ<std::vector<T>> {\nstaticbooldiff(benchmark::State &state,conststd::vector<T> &result,conststd::vector<T> &reference) {\n"}
{"Commit title": "Thinking about making the string buffer ASCII and moving the length o…", "Commit body": "@@ -89,20 +89,45 @@ really_inline bool parser::on_null_atom() noexcept {\nreally_inlineuint8_t*parser::on_start_string()noexcept{\n/*we advance the point, accounting for the fact that we have a NULL\n* termination*/\n//If we limit JSON documents to strictly less 4GB of\n//string content, then current_string_buf_loc\n//- doc.string_buf.get() fits in 32 bits. This leaves us\n//three free bytes.\nwrite_tape(current_string_buf_loc - doc.string_buf.get(), internal::tape_type::STRING);\nreturncurrent_string_buf_loc +sizeof(uint32_t);\nreturncurrent_string_buf_loc +sizeof(uint16_t);\n}\n\nreally_inlineboolparser::on_end_string(uint8_t*dst)noexcept{\nuint32_tstr_length =uint32_t(dst - (current_string_buf_loc +sizeof(uint32_t)));\n//TODO check for overflow in case someone has a crazy string (>=4GB?)\n//But only add the overflow check when the document itself exceeds 4GB\n//Currently unneeded because we refuse to parse docs larger or equal to 4GB.\nmemcpy(current_string_buf_loc, &str_length,sizeof(uint32_t));\n\n//We have two scenarios here. Either the string length is\n//less than 0x7fffff in which case, we have room in the string\n//header and all is good. Otherwise, we can encode the\n//string length in the document itself, taking care to\n//ensure that we do so in ASCII.\nif(likely(str_length <=0x7fffff)) {//likely\ndoc.tape[current_loc-1] |=uint64_t(str_length) <<32;\n//we have a string header that must be ASCII, unused in\n//this common case\ncurrent_string_buf_loc[0] =uint8_t(32);//space\ncurrent_string_buf_loc[1] =uint8_t(32);//space\n//we are done!\n}else{\n//oh gosh, we have a long string.\ndoc.tape[current_loc-1] |=uint64_t(0x800000| (str_length >>9)) <<32;\n//we have 9 bits left to code, which we do on the string buffer\n//using two bytes\ncurrent_string_buf_loc[0] =uint8_t(32+ ((str_length &0x1f0) >>4))\ncurrent_string_buf_loc[1] =32+uint8_t(str_length &0xf);\n}\n//NULL termination is still handy if you expect all your strings to\n//be NULL terminated? It comes at a small cost\n*dst =0;\ncurrent_string_buf_loc = dst +1;\n//be NULL terminated? It comes at a small cost and if it is\n//never used, we might as well drop it.\n//*dst = 0;\n//current_string_buf_loc = dst + 1;\nreturntrue;\n}\n\n"}
{"Commit title": "range utf-8 validation for avx2", "Commit body": "@@ -37,169 +37,181 @@ static inline __m256i push_last_2bytes_of_a_to_b(__m256i a, __m256i b) {\nreturn_mm256_alignr_epi8(b,_mm256_permute2x128_si256(a, b,0x21),14);\n}\n\n//all byte values must be no larger than 0xF4\nstaticinlinevoidavx_check_smaller_than_0xF4(__m256i current_bytes,\n__m256i *has_error) {\n//unsigned, saturates to 0 below max\n*has_error =_mm256_or_si256(\n*has_error,_mm256_subs_epu8(current_bytes,_mm256_set1_epi8(0xF4u)));\n}\n\nstaticinline__m256iavx_continuation_lengths(__m256i high_nibbles) {\nreturn_mm256_shuffle_epi8(\n_mm256_setr_epi8(1,1,1,1,1,1,1,1,//0xxx (ASCII)\n0,0,0,0,//10xx (continuation)\n2,2,//110x\n3,//1110\n4,//1111, next should be 0 (not checked here)\n1,1,1,1,1,1,1,1,//0xxx (ASCII)\n0,0,0,0,//10xx (continuation)\n2,2,//110x\n3,//1110\n4//1111, next should be 0 (not checked here)\n),\nhigh_nibbles);\n}\n\nstaticinline__m256iavx_carry_continuations(__m256i initial_lengths,\n__m256i previous_carries) {\n\n__m256i right1 =_mm256_subs_epu8(\npush_last_byte_of_a_to_b(previous_carries, initial_lengths),\n_mm256_set1_epi8(1));\n__m256i sum =_mm256_add_epi8(initial_lengths, right1);\n\n__m256i right2 =_mm256_subs_epu8(\npush_last_2bytes_of_a_to_b(previous_carries, sum),_mm256_set1_epi8(2));\nreturn_mm256_add_epi8(sum, right2);\n}\n\nstaticinlinevoidavx_check_continuations(__m256i initial_lengths,\n__m256i carries,\n__m256i *has_error) {\n\n//overlap || underlap\n//carry > length && length > 0 || !(carry > length) && !(length > 0)\n//(carries > length) == (lengths > 0)\n__m256i overunder =_mm256_cmpeq_epi8(\n_mm256_cmpgt_epi8(carries, initial_lengths),\n_mm256_cmpgt_epi8(initial_lengths,_mm256_setzero_si256()));\n\n*has_error =_mm256_or_si256(*has_error, overunder);\n}\n\n//when 0xED is found, next byte must be no larger than 0x9F\n//when 0xF4 is found, next byte must be no larger than 0x8F\n//next byte must be continuation, ie sign bit is set, so signed < is ok\nstaticinlinevoidavx_check_first_continuation_max(__m256i current_bytes,\n__m256i off1_current_bytes,\n__m256i *has_error) {\n__m256i maskED =\n_mm256_cmpeq_epi8(off1_current_bytes,_mm256_set1_epi8(0xEDu));\n__m256i maskF4 =\n_mm256_cmpeq_epi8(off1_current_bytes,_mm256_set1_epi8(0xF4u));\n\n__m256i badfollowED =_mm256_and_si256(\n_mm256_cmpgt_epi8(current_bytes,_mm256_set1_epi8(0x9Fu)), maskED);\n__m256i badfollowF4 =_mm256_and_si256(\n_mm256_cmpgt_epi8(current_bytes,_mm256_set1_epi8(0x8Fu)), maskF4);\n\n*has_error =\n_mm256_or_si256(*has_error,_mm256_or_si256(badfollowED, badfollowF4));\n}\n\n//map off1_hibits => error condition\n//hibits     off1    cur\n//C       => < C2 && true\n//E       => < E1 && < A0\n//F       => < F1 && < 90\n//else      false && false\nstaticinlinevoidavx_check_overlong(__m256i current_bytes,\n__m256i off1_current_bytes,\n__m256i hibits, __m256i previous_hibits,\n__m256i *has_error) {\n__m256i off1_hibits =push_last_byte_of_a_to_b(previous_hibits, hibits);\n__m256i initial_mins =_mm256_shuffle_epi8(\n_mm256_setr_epi8(-128, -128, -128, -128, -128, -128, -128, -128, -128,\n-128, -128, -128,//10xx => false\n0xC2u, -128,//110x\n0xE1u,//1110\n0xF1u,//1111\n-128, -128, -128, -128, -128, -128, -128, -128, -128,\n-128, -128, -128,//10xx => false\n0xC2u, -128,//110x\n0xE1u,//1110\n0xF1u),//1111\noff1_hibits);\n\n__m256i initial_under =_mm256_cmpgt_epi8(initial_mins, off1_current_bytes);\n\n__m256i second_mins =_mm256_shuffle_epi8(\n_mm256_setr_epi8(-128, -128, -128, -128, -128, -128, -128, -128, -128,\n-128, -128, -128,//10xx => false\n127,127,//110x => true\n0xA0u,//1110\n0x90u,//1111\n-128, -128, -128, -128, -128, -128, -128, -128, -128,\n-128, -128, -128,//10xx => false\n127,127,//110x => true\n0xA0u,//1110\n0x90u),//1111\noff1_hibits);\n__m256i second_under =_mm256_cmpgt_epi8(second_mins, current_bytes);\n*has_error =_mm256_or_si256(*has_error,\n_mm256_and_si256(initial_under, second_under));\nstaticinline__m256ipush_last_3bytes_of_a_to_b(__m256i a, __m256i b) {\nreturn_mm256_alignr_epi8(b,_mm256_permute2x128_si256(a, b,0x21),13);\n}\n\nstructavx_processed_utf_bytes{\n__m256i raw_bytes;\n__m256i high_nibbles;\n__m256i carried_continuations;\n__m256i input;\n__m256i first_len;\n};\n\nstaticinlinevoidavx_count_nibbles(__m256i bytes,\nstructavx_processed_utf_bytes*answer) {\nanswer->raw_bytes= bytes;\nanswer->high_nibbles=\n_mm256_and_si256(_mm256_srli_epi16(bytes,4),_mm256_set1_epi8(0x0F));\n}\n\n//check whether the current bytes are valid UTF-8\n//at the end of the function, previous gets updated\nstaticinlinestructavx_processed_utf_bytes\navx_check_utf8_bytes(__m256icurrent_bytes,\navx_check_utf8_bytes(__m256iinput,\nstructavx_processed_utf_bytes*previous,\n__m256i *has_error) {\nstructavx_processed_utf_bytespb {};\navx_count_nibbles(current_bytes, &pb);\n\navx_check_smaller_than_0xF4(current_bytes, has_error);\n\n__m256i initial_lengths =avx_continuation_lengths(pb.high_nibbles);\n\npb.carried_continuations=\navx_carry_continuations(initial_lengths, previous->carried_continuations);\n\navx_check_continuations(initial_lengths, pb.carried_continuations, has_error);\n\n__m256i off1_current_bytes =\npush_last_byte_of_a_to_b(previous->raw_bytes, pb.raw_bytes);\navx_check_first_continuation_max(current_bytes, off1_current_bytes,\nhas_error);\n\navx_check_overlong(current_bytes, off1_current_bytes, pb.high_nibbles,\nprevious->high_nibbles, has_error);\nreturnpb;\n/*\n* Map high nibble of \"First Byte\" to legal character length minus 1\n* 0x00 ~ 0xBF --> 0\n* 0xC0 ~ 0xDF --> 1\n* 0xE0 ~ 0xEF --> 2\n* 0xF0 ~ 0xFF --> 3\n*/\nconst__m256i first_len_tbl =\n_mm256_setr_epi8(\n0,0,0,0,0,0,0,0,0,0,0,0,1,1,2,3,\n0,0,0,0,0,0,0,0,0,0,0,0,1,1,2,3);\n\n/*Map \"First Byte\" to 8-th item of range table (0xC2 ~ 0xF4)*/\nconst__m256i first_range_tbl =\n_mm256_setr_epi8(\n0,0,0,0,0,0,0,0,0,0,0,0,8,8,8,8,\n0,0,0,0,0,0,0,0,0,0,0,0,8,8,8,8);\n\n/*\n* Range table, map range index to min and max values\n* Index 0    : 00 ~ 7F (First Byte, ascii)\n* Index 1,2,3: 80 ~ BF (Second, Third, Fourth Byte)\n* Index 4    : A0 ~ BF (Second Byte after E0)\n* Index 5    : 80 ~ 9F (Second Byte after ED)\n* Index 6    : 90 ~ BF (Second Byte after F0)\n* Index 7    : 80 ~ 8F (Second Byte after F4)\n* Index 8    : C2 ~ F4 (First Byte, non ascii)\n* Index 9~15 : illegal: i >= 127 && i <= -128\n*/\nconst__m256i range_min_tbl =\n_mm256_setr_epi8(\n0x00,0x80,0x80,0x80,0xA0,0x80,0x90,0x80,\n0xC2,0x7F,0x7F,0x7F,0x7F,0x7F,0x7F,0x7F,\n0x00,0x80,0x80,0x80,0xA0,0x80,0x90,0x80,\n0xC2,0x7F,0x7F,0x7F,0x7F,0x7F,0x7F,0x7F);\nconst__m256i range_max_tbl =\n_mm256_setr_epi8(\n0x7F,0xBF,0xBF,0xBF,0xBF,0x9F,0xBF,0x8F,\n0xF4,0x80,0x80,0x80,0x80,0x80,0x80,0x80,\n0x7F,0xBF,0xBF,0xBF,0xBF,0x9F,0xBF,0x8F,\n0xF4,0x80,0x80,0x80,0x80,0x80,0x80,0x80);\n\n/*\n* Tables for fast handling of four special First Bytes(E0,ED,F0,F4), after\n* which the Second Byte are not 80~BF. It contains \"range index adjustment\".\n* +------------+---------------+------------------+----------------+\n* | First Byte | original range| range adjustment | adjusted range |\n* +------------+---------------+------------------+----------------+\n* | E0         | 2             | 2                | 4              |\n* +------------+---------------+------------------+----------------+\n* | ED         | 2             | 3                | 5              |\n* +------------+---------------+------------------+----------------+\n* | F0         | 3             | 3                | 6              |\n* +------------+---------------+------------------+----------------+\n* | F4         | 4             | 4                | 8              |\n* +------------+---------------+------------------+----------------+\n*/\n/*index1 -> E0, index14 -> ED*/\nconst__m256i df_ee_tbl =\n_mm256_setr_epi8(\n0,2,0,0,0,0,0,0,0,0,0,0,0,0,3,0,\n0,2,0,0,0,0,0,0,0,0,0,0,0,0,3,0);\n/*index1 -> F0, index5 -> F4*/\nconst__m256i ef_fe_tbl =\n_mm256_setr_epi8(\n0,3,0,0,0,4,0,0,0,0,0,0,0,0,0,0,\n0,3,0,0,0,4,0,0,0,0,0,0,0,0,0,0);\n/*high_nibbles = input >> 4*/\nconst__m256i high_nibbles =\n_mm256_and_si256(_mm256_srli_epi16(input,4),_mm256_set1_epi8(0x0F));\n\n/*first_len = legal character length minus 1*/\n/*0 for 00~7F, 1 for C0~DF, 2 for E0~EF, 3 for F0~FF*/\n/*first_len = first_len_tbl[high_nibbles]*/\n__m256i first_len =_mm256_shuffle_epi8(first_len_tbl, high_nibbles);\n\n/*First Byte: set range index to 8 for bytes within 0xC0 ~ 0xFF*/\n/*range = first_range_tbl[high_nibbles]*/\n__m256i range =_mm256_shuffle_epi8(first_range_tbl, high_nibbles);\n\n/*Second Byte: set range index to first_len*/\n/*0 for 00~7F, 1 for C0~DF, 2 for E0~EF, 3 for F0~FF*/\n/*range |= (first_len, previous->first_len) << 1 byte*/\nrange =_mm256_or_si256(\nrange,push_last_byte_of_a_to_b(previous->first_len, first_len));\n\n/*Third Byte: set range index to saturate_sub(first_len, 1)*/\n/*0 for 00~7F, 0 for C0~DF, 1 for E0~EF, 2 for F0~FF*/\n__m256i tmp1, tmp2;\n\n/*tmp1 = saturate_sub(first_len, 1)*/\ntmp1 =_mm256_subs_epu8(first_len,_mm256_set1_epi8(1));\n/*tmp2 = saturate_sub(previous->first_len, 1)*/\ntmp2 =_mm256_subs_epu8(previous->first_len,_mm256_set1_epi8(1));\n\n/*range |= (tmp1, tmp2) << 2 bytes*/\nrange =_mm256_or_si256(range,push_last_2bytes_of_a_to_b(tmp2, tmp1));\n\n/*Fourth Byte: set range index to saturate_sub(first_len, 2)*/\n/*0 for 00~7F, 0 for C0~DF, 0 for E0~EF, 1 for F0~FF*/\n/*tmp1 = saturate_sub(first_len, 2)*/\ntmp1 =_mm256_subs_epu8(first_len,_mm256_set1_epi8(2));\n/*tmp2 = saturate_sub(previous->first_len, 2)*/\ntmp2 =_mm256_subs_epu8(previous->first_len,_mm256_set1_epi8(2));\n/*range |= (tmp1, tmp2) << 3 bytes*/\nrange =_mm256_or_si256(range,push_last_3bytes_of_a_to_b(tmp2, tmp1));\n\n/*\n* Now we have below range indices caluclated\n* Correct cases:\n* - 8 for C0~FF\n* - 3 for 1st byte after F0~FF\n* - 2 for 1st byte after E0~EF or 2nd byte after F0~FF\n* - 1 for 1st byte after C0~DF or 2nd byte after E0~EF or\n*         3rd byte after F0~FF\n* - 0 for others\n* Error cases:\n*   9,10,11 if non ascii First Byte overlaps\n*   E.g., F1 80 C2 90 --> 8 3 10 2, where 10 indicates error\n*/\n\n/*Adjust Second Byte range for special First Bytes(E0,ED,F0,F4)*/\n/*Overlaps lead to index 9~15, which are illegal in range table*/\n__m256i shift1, pos, range2;\n/*shift1 = (input, previous->input) << 1 byte*/\nshift1 =push_last_byte_of_a_to_b(previous->input, input);\npos =_mm256_sub_epi8(shift1,_mm256_set1_epi8(0xEF));\n/*\n* shift1:  | EF  F0 ... FE | FF  00  ... ...  DE | DF  E0 ... EE |\n* pos:     | 0   1      15 | 16  17           239| 240 241    255|\n* pos-240: | 0   0      0  | 0   0            0  | 0   1      15 |\n* pos+112: | 112 113    127|       >= 128        |     >= 128    |\n*/\ntmp1 =_mm256_subs_epu8(pos,_mm256_set1_epi8(240));\nrange2 =_mm256_shuffle_epi8(df_ee_tbl, tmp1);\ntmp2 =_mm256_adds_epu8(pos,_mm256_set1_epi8(112));\nrange2 =_mm256_add_epi8(range2,_mm256_shuffle_epi8(ef_fe_tbl, tmp2));\n\nrange =_mm256_add_epi8(range, range2);\n\n/*Load min and max values per calculated range index*/\n__m256i minv =_mm256_shuffle_epi8(range_min_tbl, range);\n__m256i maxv =_mm256_shuffle_epi8(range_max_tbl, range);\n\n*has_error =_mm256_or_si256(*has_error,_mm256_cmpgt_epi8(minv, input));\n*has_error =_mm256_or_si256(*has_error,_mm256_cmpgt_epi8(input, maxv));\n\nprevious->input= input;\nprevious->first_len= first_len;\n\nreturn*previous;\n}\n\ntemplate<>structutf8_checking_state<Architecture::HASWELL> {\n__m256i has_error;\navx_processed_utf_bytes previous;\nutf8_checking_state() {\nhas_error =_mm256_setzero_si256();\nprevious.raw_bytes=_mm256_setzero_si256();\nprevious.high_nibbles=_mm256_setzero_si256();\nprevious.carried_continuations=_mm256_setzero_si256();\nprevious.input=_mm256_setzero_si256();\nprevious.first_len=_mm256_setzero_si256();\n}\n};\n\n@@ -211,10 +223,10 @@ really_inline void check_utf8<Architecture::HASWELL>(\nif((_mm256_testz_si256(_mm256_or_si256(in.lo, in.hi), high_bit)) ==1) {\n//it is ascii, we just check continuation\nstate.has_error=_mm256_or_si256(\n_mm256_cmpgt_epi8(state.previous.carried_continuations,\n_mm256_cmpgt_epi8(state.previous.first_len,\n_mm256_setr_epi8(9,9,9,9,9,9,9,9,9,9,9,9,\n9,9,9,9,9,9,9,9,9,9,9,9,\n9,9,9,9,9,9,9,1)),\n9,9,9,9,9,2,1,0)),\nstate.has_error);\n}else{\n//it is not ascii so we have to do heavy work\n"}
{"Commit title": "Switching ARM64 to lookup algo. for UTF8 validation.", "Commit body": "@@ -29,7 +29,7 @@ really_inline void find_whitespace_and_operators(\nwhitespace = v.map([&](simd8<uint8_t> _v) {return_v.any_bits_set(0x18); }).to_bitmask();\n}\n\n#include\"generic/utf8_fastvalidate_algorithm.h\"\n#include\"generic/utf8_lookup_algorithm.h\"\n#include\"generic/stage1_find_marks.h\"\n\n}//namespace simdjson::arm64\n"}
{"Commit title": "Adding explicit constructor.", "Commit body": "@@ -128,6 +128,7 @@ struct event_collector {\nreturnlinux_events.is_working();\n}\n#else\nevent_collector() {}\nboolhas_events() {\nreturnfalse;\n}\n"}
{"Commit title": "Improving the tests.", "Commit body": "@@ -13,6 +13,33 @@\n\n#include\"simdjson/common_defs.h\"\n\n\n//ulp distance\n//Marc B. Reynolds, 2016-2019\n//Public Domain under http://unlicense.org, see link for details.\n//adapted by D. Lemire\ninlineuint32_tf32_ulp_dist(floata,floatb) {\nuint32_tua, ub;\nmemcpy(&ua, &a,sizeof(ua));\nmemcpy(&ub, &b,sizeof(ub));\nif((int32_t)(ub^ua) >=0)\nreturn(int32_t)(ua-ub) >=0? (ua-ub) : (ub-ua);\nreturnua+ub+0x80000000;\n}\n\n//ulp distance\n//Marc B. Reynolds, 2016-2019\n//Public Domain under http://unlicense.org, see link for details.\n//adapted by D. Lemire\ninlineuint64_tf64_ulp_dist(doublea,doubleb) {\nuint64_tua, ub;\nmemcpy(&ua, &a,sizeof(ua));\nmemcpy(&ub, &b,sizeof(ub));\nif((int64_t)(ub^ua) >=0)\nreturn(int64_t)(ua-ub) >=0? (ua-ub) : (ub-ua);\nreturnua+ub+0x80000000;\n}\n\nintparse_error;\nchar*fullpath;\nenum{ PARSE_WARNING, PARSE_ERROR };\n@@ -81,14 +108,16 @@ void foundFloat(double result, const uint8_t *buf) {\nexpected, result);\nfprintf(stderr,\"%.32s\\n\", buf);\nparse_error |= PARSE_ERROR;\nreturn;\n}\n//we want to get some reasonable relative accuracy\nelseif(fabs(expected-result)>\n1e-14*fmin(fabs(expected),fabs(result))) {\nuint64_tULP =f64_ulp_dist(expected,result);\nif(f64_ulp_dist(expected,result)>1) {\nfprintf(stderr,\"parsed %.128e from\\n\", result);\nfprintf(stderr,\"%.32s whereas strtod gives\\n\", buf);\nfprintf(stderr,\"%.128e,\", expected);\nfprintf(stderr,\"while parsing %s\\n\", fullpath);\nfprintf(stderr,\"===========  ULP:  %u,\", (unsignedint)ULP);\nparse_error |= PARSE_ERROR;\n}\n}\n"}
{"Commit title": "This seems beneficial.", "Commit body": "@@ -435,24 +435,52 @@ really_inline void flatten_bits(uint32_t *base_ptr, uint32_t &base,\nuint32_tidx,uint64_tbits) {\nuint32_tcnt =hamming(bits);\nuint32_tnext_base = base + cnt;\nwhile(bits !=0u) {\nbase_ptr[base +0] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nidx -=64;\nbase_ptr += base;\nif(bits !=0) {\nbase_ptr[0] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +1] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[1] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +2] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[2] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +3] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[3] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +4] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[4] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +5] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[5] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +6] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[6] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +7] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[7] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase +=8;\nbase_ptr +=8;\n}\nif(cnt >8) {\nbase_ptr[0] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[1] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[2] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[3] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[4] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[5] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[6] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[7] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr +=8;\n}\nif(cnt >16) {//unluckly\ndo{\nbase_ptr[0] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr++;\n}while(bits !=0);\n}\nbase = next_base;\n}\n"}
{"Commit title": "missing include", "Commit body": "@@ -8,6 +8,7 @@\n\n#ifdef__cpp_concepts\n#include<utility>\n#include<vector>\n\nnamespacesimdjson{\nnamespaceconcepts{\n"}
{"Commit title": "getting around -Weffc++", "Commit body": "@@ -40,7 +40,7 @@ bool to_string_example_no_except() {\n\n\nstructCar{\nCar()=default;\nCar(): make(), model(), year(), tire_pressure() {}\nstd::string make;\nstd::string model;\nint64_tyear;\n"}
{"Commit title": "Redesigning visit_primitive so that it is optimized for strings and", "Commit body": "@@ -303,15 +303,17 @@ simdjson_warn_unused simdjson_inline error_code json_iterator::visit_root_primit\n}\ntemplate<typenameV>\nsimdjson_warn_unused simdjson_inline error_codejson_iterator::visit_primitive(V &visitor,constuint8_t*value)noexcept{\n//Use the fact that most scalars are going to be either strings or numbers.\nif(*value =='\"') {\nreturnvisitor.visit_string(*this, value);\n}elseif(((*value -'0')  <10) || (*value =='-')) {\nreturnvisitor.visit_number(*this, value);\n}\n//true, false, null are uncommon.\nswitch(*value) {\ncase'\"':returnvisitor.visit_string(*this, value);\ncase't':returnvisitor.visit_true_atom(*this, value);\ncase'f':returnvisitor.visit_false_atom(*this, value);\ncase'n':returnvisitor.visit_null_atom(*this, value);\ncase'-':\ncase'0':case'1':case'2':case'3':case'4':\ncase'5':case'6':case'7':case'8':case'9':\nreturnvisitor.visit_number(*this, value);\ndefault:\nlog_error(\"Non-value found when value was expected!\");\nreturnTAPE_ERROR;\n"}
{"Commit title": "Disabling again for GCC 7", "Commit body": "@@ -180,6 +180,8 @@ simdjson_warn_unused simdjson_really_inline simdjson_result<bool> value_iterator\nreturnfalse;\n}\n\nSIMDJSON_PUSH_DISABLE_WARNINGS\nSIMDJSON_DISABLE_STRICT_OVERFLOW_WARNING\nsimdjson_warn_unused simdjson_really_inline simdjson_result<bool>value_iterator::find_field_unordered_raw(conststd::string_view key)noexcept{\n/**\n* When find_field_unordered_raw is called, we can either be pointing at the\n@@ -367,6 +369,7 @@ simdjson_warn_unused simdjson_really_inline simdjson_result<bool> value_iterator\n//never reach this point.\nreturnfalse;\n}\nSIMDJSON_POP_DISABLE_WARNINGS\n\nsimdjson_warn_unused simdjson_really_inline simdjson_result<raw_json_string>value_iterator::field_key()noexcept{\nassert_at_next();\n"}
{"Commit title": "Adding test.", "Commit body": "@@ -349,6 +349,27 @@ namespace document_stream_tests {\n}\n\n\nboolissue1668() {\nTEST_START();\nautojson =R\"([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100])\"_padded;\n\nondemand::parser odparser;\nondemand::document_stream odstream;\nautooderror = odparser.iterate_many(json.data(), json.length(),50).get(odstream);\n\nif(oderror) { std::cerr <<\"ondemand iterate_many error:\"<< oderror << std::endl;returnfalse; }\n\nfor(auto&doc: odstream) {\nondemand::value val;\nautoerr = doc.at_pointer(\"/40\").get(val);\nif(err) {\nstd::cout <<\"ondemand: error accessing jsonpointer:\"<< err << std::endl;\n}else{\nstd::cout <<\"ondemand:\"<< val << std::endl;\n}\n}\nTEST_SUCCEED();\n}\nbooldocument_stream_utf8_test() {\nTEST_START();\nfflush(NULL);\n@@ -424,6 +445,7 @@ namespace document_stream_tests {\n\nboolrun() {\nreturn\nissue1668() &&\nsimple_document_iteration() &&\nsimple_document_iteration_multiple_batches() &&\nsimple_document_iteration_with_parsing() &&\n"}
{"Commit title": "Better English.", "Commit body": "@@ -772,10 +772,9 @@ void basics_treewalk_1() {\nRewinding\n----------\n\nIn some instances, you may need to go through a document most than once. For that purpose, you may\ncall the`rewind()`method on the document instance. It invalidates all values, objects and arrays\nthat you have created so far, but it allows you to restart processing from the beginning without\nrescanning all of the input file again.\nIn some instances, you may need to go through a document more than once. For that purpose, you may\ncall the`rewind()`method on the document instance. It allows you to restart processing from the beginning without rescanning all of the input data again. It invalidates all values, objects and arrays\nthat you have created so far.\n\nIn the following example, we print on the screen the number of cars in the JSON input file\nbefore printout the data.\n@@ -801,7 +800,7 @@ before printout the data.\n}\n```\n\nPerformance note: the On Demand front-end does notmaterializesthe parsed numbers and other values. If you accessing everything twice, you may need to parse them twice. Thus the rewind functionality is\nPerformance note: the On Demand front-end does notmaterializethe parsed numbers and other values. If youareaccessing everything twice, you may need to parse them twice. Thus the rewind functionality is\nbest suited for cases where the first pass only scans the structure of the document.\n\nNewline-Delimited JSON (ndjson) and JSON lines\n"}
{"Commit title": "This makes the float errors explicit.", "Commit body": "@@ -2,6 +2,7 @@\n\n#include<vector>\n#include<sstream>\n#include<limits>\n\ntemplate<typenameT>\nstaticbooldiff_results(benchmark::State &state,constT &result,constT &reference);\n@@ -19,6 +20,24 @@ struct result_differ {\n}\n};\n\ntemplate<>\nboolresult_differ<double>::diff(benchmark::State &state,constdouble&result,constdouble&reference) {\nif(result != reference) {\nstd::stringstream str;\n//We print it out using full precision.\nautoprior_precision = str.precision(std::numeric_limits<double>::max_digits10);\nstr <<\"result incorrect:\"<< result <<\"... reference:\"<< reference;\nstr.precision(prior_precision);//reset to prior state\nstr << std::hexfloat;//If there are floats, we want to see them in hexadecimal form!\nstr <<\"result incorrect (hexadecimal notation):\"<< result <<\"... reference:\"<< reference;\nstr << std::defaultfloat;//reset to prior state\nstate.SkipWithError(str.str().data());\nreturnfalse;\n}\nreturntrue;\n}\n\n\ntemplate<typenameT>\nstructresult_differ<std::vector<T>> {\nstaticbooldiff(benchmark::State &state,conststd::vector<T> &result,conststd::vector<T> &reference) {\n"}
{"Commit title": "try to output failed fuzz cases", "Commit body": "@@ -15,6 +15,7 @@ jobs:\nruns-on:ubuntu-latest\nenv:\nallfuzzers:parser dump\nartifactsprefix:-artifact_prefix=fuzzfailure/\nsteps:\n-name:Install packages necessary for building\nrun:|\n@@ -44,7 +45,7 @@ jobs:\n-name:Verify that the oss-fuzz seed corpus passes without problems\nrun:|\nmkdir seedcorpus\nunzip -d seedcorpus seed_corpus.zip\nunzip -q -d seedcorpus seed_corpus.zip\nfor buildvariant in noavx withavx; do\nfor fuzzer in $allfuzzers; do\nbuild-ossfuzz-$buildvariant/fuzz/fuzz_$fuzzer seedcorpus -max_total_time=1\n@@ -54,17 +55,27 @@ jobs:\nrun:|\nfor fuzzer in $allfuzzers; do\nmkdir -p out/$fuzzer # in case this is a new fuzzer, or corpus.tar is broken\nbuild-ossfuzz-fast8/fuzz/fuzz_$fuzzer         out/$fuzzer -max_total_time=30\nbuild-ossfuzz-fast8/fuzz/fuzz_$fuzzer         out/$fuzzer -max_total_time=30 $artifactsprefix || touch failed\n# make sure the failing output is visible in the log\nif [ -e failed ] ; then\nls fuzzfailure/* |xargs -n1 base64\nexit 1\nfi\ndone\n-name:Run the other fuzzer variants for $fuzzer, with sanitizers etc\nrun:|\nfor fuzzer in $allfuzzers; do\nbuild-ossfuzz-withavx/fuzz/fuzz_$fuzzer       out/$fuzzer -max_total_time=20\nbuild-ossfuzz-noavx/fuzz/fuzz_$fuzzer         out/$fuzzer -max_total_time=10\nbuild-ossfuzz-noavx8/fuzz/fuzz_$fuzzer        out/$fuzzer -max_total_time=10\nbuild-ossfuzz-withavx/fuzz/fuzz_$fuzzer       out/$fuzzer -max_total_time=20 $artifactsprefix || touch failed\nbuild-ossfuzz-noavx/fuzz/fuzz_$fuzzer         out/$fuzzer -max_total_time=10 $artifactsprefix || touch failed\nbuild-ossfuzz-noavx8/fuzz/fuzz_$fuzzer        out/$fuzzer -max_total_time=10 $artifactsprefix || touch failed\nif [ -e failed ] ; then\n# make sure the failing output is visible in the log\nls fuzzfailure/* |xargs -n1 base64\nexit 1\nfi\necho disable msan runs, it fails inside the fuzzing engine and not the fuzzed code!\necho build-ossfuzz-msan-noavx8/fuzz/fuzz_$fuzzer   out/$fuzzer -max_total_time=10 -reload=0\necho build-ossfuzz-msan-withavx8/fuzz/fuzz_$fuzzer out/$fuzzer -max_total_time=10 -reload=0\necho build-ossfuzz-msan-noavx8/fuzz/fuzz_$fuzzer   out/$fuzzer -max_total_time=10 -reload=0$artifactsprefix\necho build-ossfuzz-msan-withavx8/fuzz/fuzz_$fuzzer out/$fuzzer -max_total_time=10 -reload=0$artifactsprefix\necho now have $(ls out/$fuzzer |wc -l) files in corpus\ndone\n-name:Minimize the corpus with the fast fuzzer\n"}
{"Commit title": "Fix arm64 build", "Commit body": "@@ -140,7 +140,7 @@ WARN_UNUSED error_code dom_parser_implementation::stage1(const uint8_t *_buf, si\n}\n\nWARN_UNUSEDboolimplementation::validate_utf8(constchar*buf,size_tlen)constnoexcept{\nreturnsimdjson::arm64::stage1::generic_validate_utf8(buf,len);\nreturnarm64::stage1::generic_validate_utf8(buf,len);\n}\n\nWARN_UNUSED error_codedom_parser_implementation::stage2(dom::document &_doc)noexcept{\n"}
{"Commit title": "Keep current JSON after checking primitive type", "Commit body": "@@ -279,7 +279,8 @@ WARN_UNUSED static really_inline error_code parse_structurals(dom_parser_impleme\n//Read first value\n//\n{\nswitch(parser.advance_char()) {\nconstuint8_t*value = parser.advance();\nswitch(*value) {\ncase'{': {\nif(parser.empty_object()) {gotodocument_end; }\nSIMDJSON_TRY( parser.start_object() );\n@@ -297,14 +298,14 @@ WARN_UNUSED static really_inline error_code parse_structurals(dom_parser_impleme\n}\ngotoarray_begin;\n}\ncase'\"':SIMDJSON_TRY( parser.parse_string(parser.current()) );gotodocument_end;\ncase't':SIMDJSON_TRY( parser.parse_root_true_atom(parser.current()) );gotodocument_end;\ncase'f':SIMDJSON_TRY( parser.parse_root_false_atom(parser.current()) );gotodocument_end;\ncase'n':SIMDJSON_TRY( parser.parse_root_null_atom(parser.current()) );gotodocument_end;\ncase'\"':SIMDJSON_TRY( parser.parse_string(value) );gotodocument_end;\ncase't':SIMDJSON_TRY( parser.parse_root_true_atom(value) );gotodocument_end;\ncase'f':SIMDJSON_TRY( parser.parse_root_false_atom(value) );gotodocument_end;\ncase'n':SIMDJSON_TRY( parser.parse_root_null_atom(value) );gotodocument_end;\ncase'-':\ncase'0':case'1':case'2':case'3':case'4':\ncase'5':case'6':case'7':case'8':case'9':\nSIMDJSON_TRY( parser.parse_root_number(parser.current()) );gotodocument_end;\nSIMDJSON_TRY( parser.parse_root_number(value) );gotodocument_end;\ndefault:\nparser.log_error(\"Document starts with a non-value character\");\nreturnTAPE_ERROR;\n@@ -327,7 +328,8 @@ object_begin: {\n\nobject_field: {\nif(unlikely( parser.advance_char() !=':')) { parser.log_error(\"Missing colon after key in object\");returnTAPE_ERROR; }\nswitch(parser.advance_char()) {\nconstuint8_t*value = parser.advance();\nswitch(*value) {\ncase'{': {\nif(parser.empty_object()) {break; };\nSIMDJSON_TRY( parser.start_object() );\n@@ -338,14 +340,14 @@ object_field: {\nSIMDJSON_TRY( parser.start_array() );\ngotoarray_begin;\n}\ncase'\"':SIMDJSON_TRY( parser.parse_string(parser.current()) );break;\ncase't':SIMDJSON_TRY( parser.parse_true_atom(parser.current()) );break;\ncase'f':SIMDJSON_TRY( parser.parse_false_atom(parser.current()) );break;\ncase'n':SIMDJSON_TRY( parser.parse_null_atom(parser.current()) );break;\ncase'\"':SIMDJSON_TRY( parser.parse_string(value) );break;\ncase't':SIMDJSON_TRY( parser.parse_true_atom(value) );break;\ncase'f':SIMDJSON_TRY( parser.parse_false_atom(value) );break;\ncase'n':SIMDJSON_TRY( parser.parse_null_atom(value) );break;\ncase'-':\ncase'0':case'1':case'2':case'3':case'4':\ncase'5':case'6':case'7':case'8':case'9':\nSIMDJSON_TRY( parser.parse_number(parser.current()) );break;\nSIMDJSON_TRY( parser.parse_number(value) );break;\ndefault:\nparser.log_error(\"Non-value found when value was expected!\");\nreturnTAPE_ERROR;\n@@ -384,7 +386,8 @@ array_begin: {\n}//array_begin:\n\narray_value: {\nswitch(parser.advance_char()) {\nconstuint8_t*value = parser.advance();\nswitch(*value) {\ncase'{': {\nif(parser.empty_object()) {break; };\nSIMDJSON_TRY( parser.start_object() );\n@@ -395,14 +398,14 @@ array_value: {\nSIMDJSON_TRY( parser.start_array() );\ngotoarray_begin;\n}\ncase'\"':SIMDJSON_TRY( parser.parse_string(parser.current()) );break;\ncase't':SIMDJSON_TRY( parser.parse_true_atom(parser.current()) );break;\ncase'f':SIMDJSON_TRY( parser.parse_false_atom(parser.current()) );break;\ncase'n':SIMDJSON_TRY( parser.parse_null_atom(parser.current()) );break;\ncase'\"':SIMDJSON_TRY( parser.parse_string(value) );break;\ncase't':SIMDJSON_TRY( parser.parse_true_atom(value) );break;\ncase'f':SIMDJSON_TRY( parser.parse_false_atom(value) );break;\ncase'n':SIMDJSON_TRY( parser.parse_null_atom(value) );break;\ncase'-':\ncase'0':case'1':case'2':case'3':case'4':\ncase'5':case'6':case'7':case'8':case'9':\nSIMDJSON_TRY( parser.parse_number(parser.current()) );break;\nSIMDJSON_TRY( parser.parse_number(value) );break;\ndefault:\nparser.log_error(\"Non-value found when value was expected!\");\nreturnTAPE_ERROR;\n"}
{"Commit title": "Thinking about making the string buffer ASCII and moving the length o…", "Commit body": "@@ -89,20 +89,45 @@ really_inline bool parser::on_null_atom() noexcept {\nreally_inlineuint8_t*parser::on_start_string()noexcept{\n/*we advance the point, accounting for the fact that we have a NULL\n* termination*/\n//If we limit JSON documents to strictly less 4GB of\n//string content, then current_string_buf_loc\n//- doc.string_buf.get() fits in 32 bits. This leaves us\n//three free bytes.\nwrite_tape(current_string_buf_loc - doc.string_buf.get(), internal::tape_type::STRING);\nreturncurrent_string_buf_loc +sizeof(uint32_t);\nreturncurrent_string_buf_loc +sizeof(uint16_t);\n}\n\nreally_inlineboolparser::on_end_string(uint8_t*dst)noexcept{\nuint32_tstr_length =uint32_t(dst - (current_string_buf_loc +sizeof(uint32_t)));\n//TODO check for overflow in case someone has a crazy string (>=4GB?)\n//But only add the overflow check when the document itself exceeds 4GB\n//Currently unneeded because we refuse to parse docs larger or equal to 4GB.\nmemcpy(current_string_buf_loc, &str_length,sizeof(uint32_t));\n\n//We have two scenarios here. Either the string length is\n//less than 0x7fffff in which case, we have room in the string\n//header and all is good. Otherwise, we can encode the\n//string length in the document itself, taking care to\n//ensure that we do so in ASCII.\nif(likely(str_length <=0x7fffff)) {//likely\ndoc.tape[current_loc-1] |=uint64_t(str_length) <<32;\n//we have a string header that must be ASCII, unused in\n//this common case\ncurrent_string_buf_loc[0] =uint8_t(32);//space\ncurrent_string_buf_loc[1] =uint8_t(32);//space\n//we are done!\n}else{\n//oh gosh, we have a long string.\ndoc.tape[current_loc-1] |=uint64_t(0x800000| (str_length >>9)) <<32;\n//we have 9 bits left to code, which we do on the string buffer\n//using two bytes\ncurrent_string_buf_loc[0] =uint8_t(32+ ((str_length &0x1f0) >>4))\ncurrent_string_buf_loc[1] =32+uint8_t(str_length &0xf);\n}\n//NULL termination is still handy if you expect all your strings to\n//be NULL terminated? It comes at a small cost\n*dst =0;\ncurrent_string_buf_loc = dst +1;\n//be NULL terminated? It comes at a small cost and if it is\n//never used, we might as well drop it.\n//*dst = 0;\n//current_string_buf_loc = dst + 1;\nreturntrue;\n}\n\n"}
{"Commit title": "Fix SAX benchmark to actually add tweets", "Commit body": "@@ -233,6 +233,7 @@ simdjson_really_inline bool sax_tweet_reader_visitor::in_container_child(json_it\nsimdjson_really_inlinevoidsax_tweet_reader_visitor::start_container(json_iterator &iter) {\nSIMDJSON_ASSUME(iter.depth<= MAX_SUPPORTED_DEPTH);//Asserts in debug mode\ncontainer =containers(iter.depth);\nif(container == containers::tweet) { tweets.push_back({}); }\nif(logger::LOG_ENABLED) { iter.log_start_value(STATE_NAMES[iter.depth]); }\n}\nsimdjson_really_inlinevoidsax_tweet_reader_visitor::end_container(json_iterator &iter) {\n"}
{"Commit title": "fixing continuation validation", "Commit body": "@@ -178,7 +178,7 @@ really_inline void check_utf8<Architecture::WESTMERE>(\nstate.has_error=\n_mm_or_si128(_mm_cmpgt_epi8(state.previous.first_len,\n_mm_setr_epi8(9,9,9,9,9,9,9,9,9,9,\n9,9,9,3,2,1)),\n9,9,9,2,1,0)),\nstate.has_error);\n}else{\n//it is not ascii so we have to do heavy work\n@@ -193,7 +193,7 @@ really_inline void check_utf8<Architecture::WESTMERE>(\nstate.has_error=\n_mm_or_si128(_mm_cmpgt_epi8(state.previous.first_len,\n_mm_setr_epi8(9,9,9,9,9,9,9,9,9,9,\n9,9,9,3,2,1)),\n9,9,9,2,1,0)),\nstate.has_error);\n}else{\n//it is not ascii so we have to do heavy work\n"}
{"Commit title": "Adding explicit constructor.", "Commit body": "@@ -128,6 +128,7 @@ struct event_collector {\nreturnlinux_events.is_working();\n}\n#else\nevent_collector() {}\nboolhas_events() {\nreturnfalse;\n}\n"}
{"Commit title": "Switching ARM64 to lookup algo. for UTF8 validation.", "Commit body": "@@ -29,7 +29,7 @@ really_inline void find_whitespace_and_operators(\nwhitespace = v.map([&](simd8<uint8_t> _v) {return_v.any_bits_set(0x18); }).to_bitmask();\n}\n\n#include\"generic/utf8_fastvalidate_algorithm.h\"\n#include\"generic/utf8_lookup_algorithm.h\"\n#include\"generic/stage1_find_marks.h\"\n\n}//namespace simdjson::arm64\n"}
{"Commit title": "Add cpuinfo to checkperf", "Commit body": "@@ -22,7 +22,7 @@ steps:\nimage:gcc:8\nenvironment:\nCHECKPERF_REPOSITORY:https://github.com/lemire/simdjson\ncommands:[ make checkperf ]\ncommands:[cat /proc/cpuinfo,make checkperf ]\n---\nkind:pipeline\nname:x64-build\n@@ -72,7 +72,7 @@ steps:\nimage:gcc:8\nenvironment:\nCHECKPERF_REPOSITORY:https://github.com/lemire/simdjson\ncommands:[ make checkperf ]\ncommands:[cat /proc/cpuinfo,make checkperf ]\n---\nkind:pipeline\nname:arm64-build\n"}
{"Commit title": "This seems beneficial.", "Commit body": "@@ -435,24 +435,52 @@ really_inline void flatten_bits(uint32_t *base_ptr, uint32_t &base,\nuint32_tidx,uint64_tbits) {\nuint32_tcnt =hamming(bits);\nuint32_tnext_base = base + cnt;\nwhile(bits !=0u) {\nbase_ptr[base +0] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nidx -=64;\nbase_ptr += base;\nif(bits !=0) {\nbase_ptr[0] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +1] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[1] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +2] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[2] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +3] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[3] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +4] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[4] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +5] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[5] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +6] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[6] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[base +7] =static_cast<uint32_t>(idx) -64+trailingzeroes(bits);\nbase_ptr[7] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase +=8;\nbase_ptr +=8;\n}\nif(cnt >8) {\nbase_ptr[0] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[1] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[2] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[3] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[4] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[5] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[6] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr[7] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr +=8;\n}\nif(cnt >16) {//unluckly\ndo{\nbase_ptr[0] = idx +trailingzeroes(bits);\nbits = bits & (bits -1);\nbase_ptr++;\n}while(bits !=0);\n}\nbase = next_base;\n}\n"}
{"Commit title": "fix: add macros to detected C++20 and C++23", "Commit body": "@@ -13,6 +13,16 @@\n#endif\n#endif\n\n// C++ 23\n#if!defined(SIMDJSON_CPLUSPLUS20)&&(SIMDJSON_CPLUSPLUS>=202302L)\n#defineSIMDJSON_CPLUSPLUS201\n#endif\n\n// C++ 20\n#if!defined(SIMDJSON_CPLUSPLUS20)&&(SIMDJSON_CPLUSPLUS>=202002L)\n#defineSIMDJSON_CPLUSPLUS201\n#endif\n\n// C++ 17\n#if!defined(SIMDJSON_CPLUSPLUS17)&&(SIMDJSON_CPLUSPLUS>=201703L)\n#defineSIMDJSON_CPLUSPLUS171\n"}
{"Commit title": "add ARM64EC to vs17-arm-ci workflow", "Commit body": "@@ -12,10 +12,11 @@ jobs:\ninclude:\n-{arch: ARM}\n-{arch: ARM64}\n-{arch: ARM64EC}\nsteps:\n-name:checkout\nuses:actions/checkout@v4\n-name:Use cmake\nrun:|\ncmake -A ${{ matrix.arch }} -DCMAKE_CROSSCOMPILING=1 -DSIMDJSON_DEVELOPER_MODE=ON -D SIMDJSON_GOOGLE_BENCHMARKS=OFF -DSIMDJSON_EXCEPTIONS=OFF -B build  &&\ncmake -A ${{ matrix.arch }} -DCMAKE_SYSTEM_VERSION=\"10.0.22621.0\" -DCMAKE_CROSSCOMPILING=1 -DSIMDJSON_DEVELOPER_MODE=ON -D SIMDJSON_GOOGLE_BENCHMARKS=OFF -DSIMDJSON_EXCEPTIONS=OFF -B build  &&\ncmake --build build --verbose"}
{"Commit title": "Make a local copy of structural_indexes pointer", "Commit body": "@@ -66,7 +66,7 @@ class json_structural_indexer {\ntemplate<size_tSTEP_SIZE>\nreally_inlinevoidstep(constuint8_t*block, buf_block_reader<STEP_SIZE> &reader)noexcept;\nreally_inlinevoidnext(simd::simd8x64<uint8_t> in, json_block block,size_tidx);\nreally_inline error_codefinish(parser &parser,size_tidx,size_tlen,boolstreaming);\nreally_inline error_codefinish(parser &parser,uint32_t*structural_indexes,size_tidx,size_tlen,boolstreaming);\n\njson_scanner scanner{};\nutf8_checker checker{};\n@@ -83,7 +83,7 @@ really_inline void json_structural_indexer::next(simd::simd8x64<uint8_t> in, jso\nunescaped_chars_error |= block.non_quote_inside_string(unescaped);\n}\n\nreally_inline error_codejson_structural_indexer::finish(parser &parser,size_tidx,size_tlen,boolstreaming) {\nreally_inline error_codejson_structural_indexer::finish(parser &parser,uint32_t*structural_indexes,size_tidx,size_tlen,boolstreaming) {\n//Write out the final iteration's structurals\nindexer.write(uint32_t(idx-64), prev_structurals);\n\n@@ -94,22 +94,22 @@ really_inline error_code json_structural_indexer::finish(parser &parser, size_t\nreturnUNESCAPED_CHARS;\n}\n\nparser.n_structural_indexes=uint32_t(indexer.tail-parser.structural_indexes());\nparser.n_structural_indexes=uint32_t(indexer.tail- structural_indexes);\n/*a valid JSON file cannot have zero structural indexes - we should have\n* found something*/\nif(unlikely(parser.n_structural_indexes==0u)) {\nreturnEMPTY;\n}\nif(unlikely(parser.structural_indexes()[parser.n_structural_indexes-1] > len)) {\nif(unlikely(structural_indexes[parser.n_structural_indexes-1] > len)) {\nreturnUNEXPECTED_ERROR;\n}\nif(len !=parser.structural_indexes()[parser.n_structural_indexes-1]) {\nif(len != structural_indexes[parser.n_structural_indexes-1]) {\n/*the string might not be NULL terminated, but we add a virtual NULL\n* ending character.*/\nparser.structural_indexes()[parser.n_structural_indexes++] =uint32_t(len);\nstructural_indexes[parser.n_structural_indexes++] =uint32_t(len);\n}\n/*make it safe to dereference one beyond this array*/\nparser.structural_indexes()[parser.n_structural_indexes] =0;\nstructural_indexes[parser.n_structural_indexes] =0;\nreturnchecker.errors();\n}\n\n@@ -158,8 +158,10 @@ template<size_t STEP_SIZE>\nerror_codejson_structural_indexer::index(constuint8_t*buf,size_tlen, parser &parser,boolstreaming)noexcept{\nif(unlikely(len > parser.capacity())) {returnCAPACITY; }\n\nautostructural_indexes = parser.structural_indexes();\n\nbuf_block_reader<STEP_SIZE>reader(buf, len);\njson_structural_indexerindexer(parser.structural_indexes());\njson_structural_indexerindexer(structural_indexes);\nwhile(reader.has_full_block()) {\nindexer.step<STEP_SIZE>(reader.full_block(), reader);\n}\n@@ -170,7 +172,7 @@ error_code json_structural_indexer::index(const uint8_t *buf, size_t len, parser\nindexer.step<STEP_SIZE>(block, reader);\n}\n\nreturnindexer.finish(parser, reader.block_index(), len, streaming);\nreturnindexer.finish(parser,structural_indexes,reader.block_index(), len, streaming);\n}\n\n}//namespace stage1"}
{"Commit title": "Move { and [ to the start of the switch", "Commit body": "@@ -200,6 +200,12 @@ struct structural_parser : structural_iterator {\n}\nWARN_UNUSED really_inlineret_address_tparse_value(constunified_machine_addresses &addresses,ret_address_tcontinue_state) {\nswitch(advance_char()) {\ncase'{':\nFAIL_IF(start_object(continue_state) );\nreturnaddresses.object_begin;\ncase'[':\nFAIL_IF(start_array(continue_state) );\nreturnaddresses.array_begin;\ncase'\"':\nFAIL_IF(parse_string() );\nreturncontinue_state;\n@@ -223,12 +229,6 @@ struct structural_parser : structural_iterator {\ncase'5':case'6':case'7':case'8':case'9':\nFAIL_IF(parse_number() );\nreturncontinue_state;\ncase'{':\nFAIL_IF(start_object(continue_state) );\nreturnaddresses.object_begin;\ncase'[':\nFAIL_IF(start_array(continue_state) );\nreturnaddresses.array_begin;\ndefault:\nlog_error(\"Non-value found when value was expected!\");\nreturnaddresses.error;\n"}
{"Commit title": "Redesigning visit_primitive so that it is optimized for strings and", "Commit body": "@@ -303,15 +303,17 @@ simdjson_warn_unused simdjson_inline error_code json_iterator::visit_root_primit\n}\ntemplate<typenameV>\nsimdjson_warn_unused simdjson_inline error_codejson_iterator::visit_primitive(V &visitor,constuint8_t*value)noexcept{\n//Use the fact that most scalars are going to be either strings or numbers.\nif(*value =='\"') {\nreturnvisitor.visit_string(*this, value);\n}elseif(((*value -'0')  <10) || (*value =='-')) {\nreturnvisitor.visit_number(*this, value);\n}\n//true, false, null are uncommon.\nswitch(*value) {\ncase'\"':returnvisitor.visit_string(*this, value);\ncase't':returnvisitor.visit_true_atom(*this, value);\ncase'f':returnvisitor.visit_false_atom(*this, value);\ncase'n':returnvisitor.visit_null_atom(*this, value);\ncase'-':\ncase'0':case'1':case'2':case'3':case'4':\ncase'5':case'6':case'7':case'8':case'9':\nreturnvisitor.visit_number(*this, value);\ndefault:\nlog_error(\"Non-value found when value was expected!\");\nreturnTAPE_ERROR;\n"}
{"Commit title": "Adding test.", "Commit body": "@@ -349,6 +349,27 @@ namespace document_stream_tests {\n}\n\n\nboolissue1668() {\nTEST_START();\nautojson =R\"([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100])\"_padded;\n\nondemand::parser odparser;\nondemand::document_stream odstream;\nautooderror = odparser.iterate_many(json.data(), json.length(),50).get(odstream);\n\nif(oderror) { std::cerr <<\"ondemand iterate_many error:\"<< oderror << std::endl;returnfalse; }\n\nfor(auto&doc: odstream) {\nondemand::value val;\nautoerr = doc.at_pointer(\"/40\").get(val);\nif(err) {\nstd::cout <<\"ondemand: error accessing jsonpointer:\"<< err << std::endl;\n}else{\nstd::cout <<\"ondemand:\"<< val << std::endl;\n}\n}\nTEST_SUCCEED();\n}\nbooldocument_stream_utf8_test() {\nTEST_START();\nfflush(NULL);\n@@ -424,6 +445,7 @@ namespace document_stream_tests {\n\nboolrun() {\nreturn\nissue1668() &&\nsimple_document_iteration() &&\nsimple_document_iteration_multiple_batches() &&\nsimple_document_iteration_with_parsing() &&\n"}
{"Commit title": "This enables /Ob3 under Visual Studio 2019", "Commit body": "@@ -81,6 +81,12 @@ else()\n#Recent version of Visual Studio expected (2017, 2019...). Prior versions are unsupported.\ntarget_compile_options(simdjson-internal-flagsINTERFACE/WX /W3 /sdl)\nendif()\nif(${CMAKE_VS_PLATFORM_TOOLSET}STRGREATER\"v141\")\nstring(REPLACE\"/Ob2\"\"/Ob2\"CMAKE_CXX_FLAGS_RELEASE\"${CMAKE_CXX_FLAGS_RELEASE}\")#VS 2019\nmessage(STATUS\" Visual Studio 2019 detected, new compile flags${CMAKE_CXX_FLAGS_RELEASE}\")\nendif()\n\n\nelse()\ntarget_compile_options(simdjson-internal-flagsINTERFACE-fPIC)\ntarget_compile_options(simdjson-internal-flagsINTERFACE-Werror -Wall -Wextra -Weffc++)\n"}
{"Commit title": "This makes the float errors explicit.", "Commit body": "@@ -2,6 +2,7 @@\n\n#include<vector>\n#include<sstream>\n#include<limits>\n\ntemplate<typenameT>\nstaticbooldiff_results(benchmark::State &state,constT &result,constT &reference);\n@@ -19,6 +20,24 @@ struct result_differ {\n}\n};\n\ntemplate<>\nboolresult_differ<double>::diff(benchmark::State &state,constdouble&result,constdouble&reference) {\nif(result != reference) {\nstd::stringstream str;\n//We print it out using full precision.\nautoprior_precision = str.precision(std::numeric_limits<double>::max_digits10);\nstr <<\"result incorrect:\"<< result <<\"... reference:\"<< reference;\nstr.precision(prior_precision);//reset to prior state\nstr << std::hexfloat;//If there are floats, we want to see them in hexadecimal form!\nstr <<\"result incorrect (hexadecimal notation):\"<< result <<\"... reference:\"<< reference;\nstr << std::defaultfloat;//reset to prior state\nstate.SkipWithError(str.str().data());\nreturnfalse;\n}\nreturntrue;\n}\n\n\ntemplate<typenameT>\nstructresult_differ<std::vector<T>> {\nstaticbooldiff(benchmark::State &state,conststd::vector<T> &result,conststd::vector<T> &reference) {\n"}
{"Commit title": "Typo", "Commit body": "@@ -57,7 +57,7 @@ really_inline json_character_block json_character_block::classify(const simd::si\nin.chunks[0].any_bits_set(0x7),\nin.chunks[1].any_bits_set(0x7),\nin.chunks[2].any_bits_set(0x7),\nin.chunks[3].any_bits_set(0x7))\nin.chunks[3].any_bits_set(0x7)\n).to_bitmask();\n\n//Functional programming causes trouble with Visual Studio:\n"}
{"Commit title": "Fix clang warning -Wused-but-marked-unused.", "Commit body": "@@ -98,8 +98,8 @@ constexpr size_t DEFAULT_MAX_DEPTH = 1024;\n\n#else//SIMDJSON_REGULAR_VISUAL_STUDIO\n\n#definereally_inlineinline__attribute__((always_inline, unused))\n#define never_inlineinline__attribute__((noinline, unused))\n#definereally_inlineinline__attribute__((always_inline))\n#define never_inlineinline__attribute__((noinline))\n\n#define UNUSED__attribute__((unused))\n#define WARN_UNUSED__attribute__((warn_unused_result))\n"}
{"Commit title": "Thinking about making the string buffer ASCII and moving the length o…", "Commit body": "@@ -89,20 +89,45 @@ really_inline bool parser::on_null_atom() noexcept {\nreally_inlineuint8_t*parser::on_start_string()noexcept{\n/*we advance the point, accounting for the fact that we have a NULL\n* termination*/\n//If we limit JSON documents to strictly less 4GB of\n//string content, then current_string_buf_loc\n//- doc.string_buf.get() fits in 32 bits. This leaves us\n//three free bytes.\nwrite_tape(current_string_buf_loc - doc.string_buf.get(), internal::tape_type::STRING);\nreturncurrent_string_buf_loc +sizeof(uint32_t);\nreturncurrent_string_buf_loc +sizeof(uint16_t);\n}\n\nreally_inlineboolparser::on_end_string(uint8_t*dst)noexcept{\nuint32_tstr_length =uint32_t(dst - (current_string_buf_loc +sizeof(uint32_t)));\n//TODO check for overflow in case someone has a crazy string (>=4GB?)\n//But only add the overflow check when the document itself exceeds 4GB\n//Currently unneeded because we refuse to parse docs larger or equal to 4GB.\nmemcpy(current_string_buf_loc, &str_length,sizeof(uint32_t));\n\n//We have two scenarios here. Either the string length is\n//less than 0x7fffff in which case, we have room in the string\n//header and all is good. Otherwise, we can encode the\n//string length in the document itself, taking care to\n//ensure that we do so in ASCII.\nif(likely(str_length <=0x7fffff)) {//likely\ndoc.tape[current_loc-1] |=uint64_t(str_length) <<32;\n//we have a string header that must be ASCII, unused in\n//this common case\ncurrent_string_buf_loc[0] =uint8_t(32);//space\ncurrent_string_buf_loc[1] =uint8_t(32);//space\n//we are done!\n}else{\n//oh gosh, we have a long string.\ndoc.tape[current_loc-1] |=uint64_t(0x800000| (str_length >>9)) <<32;\n//we have 9 bits left to code, which we do on the string buffer\n//using two bytes\ncurrent_string_buf_loc[0] =uint8_t(32+ ((str_length &0x1f0) >>4))\ncurrent_string_buf_loc[1] =32+uint8_t(str_length &0xf);\n}\n//NULL termination is still handy if you expect all your strings to\n//be NULL terminated? It comes at a small cost\n*dst =0;\ncurrent_string_buf_loc = dst +1;\n//be NULL terminated? It comes at a small cost and if it is\n//never used, we might as well drop it.\n//*dst = 0;\n//current_string_buf_loc = dst + 1;\nreturntrue;\n}\n\n"}
{"Commit title": "really_inline more things", "Commit body": "@@ -49,7 +49,7 @@ static inline uint32_t hex_to_u32_nocheck(\n//\n//Note: we assume that surrogates are treated separately\n//\ninlinesize_tcodepoint_to_utf8(uint32_tcp,uint8_t*c) {\nreally_inlinesize_tcodepoint_to_utf8(uint32_tcp,uint8_t*c) {\nif(cp <=0x7F) {\nc[0] =uint8_t(cp);\nreturn1;//ascii\n@@ -78,28 +78,13 @@ inline size_t codepoint_to_utf8(uint32_t cp, uint8_t *c) {\nreturn0;//bad r\n}\n\n////\n//The following code is used in number parsing. It is not\n//properly \"char utils\" stuff, but we move it here so that\n//it does not get copied multiple times in the binaries (once\n//per instruction set).\n///\n\nconstexprintFASTFLOAT_SMALLEST_POWER = -325;\nconstexprintFASTFLOAT_LARGEST_POWER =308;\n\nstructvalue128{\nuint64_tlow;\nuint64_thigh;\n};\n\n#ifdefSIMDJSON_IS_32BITS//_umul128 for x86, arm\n//this is a slow emulation routine for 32-bit\n//\nstaticinlineuint64_t__emulu(uint32_tx,uint32_ty) {\nstaticreally_inlineuint64_t__emulu(uint32_tx,uint32_ty) {\nreturnx * (uint64_t)y;\n}\nstaticinlineuint64_t_umul128(uint64_tab,uint64_tcd,uint64_t*hi) {\nstaticreally_inlineuint64_t_umul128(uint64_tab,uint64_tcd,uint64_t*hi) {\nuint64_tad =__emulu((uint32_t)(ab >>32), (uint32_t)cd);\nuint64_tbd =__emulu((uint32_t)ab, (uint32_t)cd);\nuint64_tadbc = ad +__emulu((uint32_t)ab, (uint32_t)(cd >>32));\n"}
{"Commit title": "Return bool from compute_float_64", "Commit body": "@@ -24,7 +24,7 @@ namespace numberparsing {\n//set to false. This should work *most of the time* (like 99% of the time).\n//We assume that power is in the [FASTFLOAT_SMALLEST_POWER,\n//FASTFLOAT_LARGEST_POWER] interval: the caller is responsible for this check.\nreally_inlinedoublecompute_float_64(int64_tpower,uint64_ti,boolnegative,bool*success) {\nreally_inlineboolcompute_float_64(int64_tpower,uint64_ti,boolnegative,double&d) {\n//we start with a fast path\n//It was described in\n//Clinger WD. How to read floating point numbers accurately.\n@@ -40,7 +40,7 @@ really_inline double compute_float_64(int64_t power, uint64_t i, bool negative,\n#endif\n//convert the integer into a double. This is lossless since\n//0 <= i <= 2^53 - 1.\ndoubled =double(i);\nd =double(i);\n//\n//The general idea is as follows.\n//If 0 <= s < 2^53 and if 10^0 <= p <= 10^22 then\n@@ -59,8 +59,7 @@ really_inline double compute_float_64(int64_t power, uint64_t i, bool negative,\nif(negative) {\nd = -d;\n}\n*success =true;\nreturnd;\nreturntrue;\n}\n//When 22 < power && power <  22 + 16, we could\n//hope for another, secondary fast path.  It wa\n@@ -85,7 +84,8 @@ really_inline double compute_float_64(int64_t power, uint64_t i, bool negative,\n//In the slow path, we need to adjust i so that it is > 1<<63 which is always\n//possible, except if i == 0, so we handle i == 0 separately.\nif(i ==0) {\nreturn0.0;\nd =0.0;\nreturntrue;\n}\n\n//We are going to need to do some 64-bit arithmetic to get a more precise product.\n@@ -135,8 +135,7 @@ really_inline double compute_float_64(int64_t power, uint64_t i, bool negative,\n//This does happen, e.g. with 7.3177701707893310e+15.\nif(((product_middle +1==0) && ((product_high &0x1FF) ==0x1FF) &&\n(product_low + i < product_low))) {//let us be prudent and bail out.\n*success =false;\nreturn0;\nreturnfalse;\n}\nupper = product_high;\nlower = product_middle;\n@@ -157,25 +156,24 @@ really_inline double compute_float_64(int64_t power, uint64_t i, bool negative,\n//floating-point values.\nif(unlikely((lower ==0) && ((upper &0x1FF) ==0) &&\n((mantissa &3) ==1))) {\n//if mantissa & 1 == 1 we might need to round up.\n//\n//Scenarios:\n//1. We are not in the middle. Then we should round up.\n//\n//2. We are right in the middle. Whether we round up depends\n//on the last significant bit: if it is \"one\" then we round\n//up (round to even) otherwise, we do not.\n//\n//So if the last significant bit is 1, we can safely round up.\n//Hence we only need to bail out if (mantissa & 3) == 1.\n//Otherwise we may need more accuracy or analysis to determine whether\n//we are exactly between two floating-point numbers.\n//It can be triggered with 1e23.\n//Note: because the factor_mantissa and factor_mantissa_low are\n//almost always rounded down (except for small positive powers),\n//almost always should round up.\n*success =false;\nreturn0;\n//if mantissa & 1 == 1 we might need to round up.\n//\n//Scenarios:\n//1. We are not in the middle. Then we should round up.\n//\n//2. We are right in the middle. Whether we round up depends\n//on the last significant bit: if it is \"one\" then we round\n//up (round to even) otherwise, we do not.\n//\n//So if the last significant bit is 1, we can safely round up.\n//Hence we only need to bail out if (mantissa & 3) == 1.\n//Otherwise we may need more accuracy or analysis to determine whether\n//we are exactly between two floating-point numbers.\n//It can be triggered with 1e23.\n//Note: because the factor_mantissa and factor_mantissa_low are\n//almost always rounded down (except for small positive powers),\n//almost always should round up.\nreturnfalse;\n}\n\nmantissa += mantissa &1;\n@@ -193,15 +191,12 @@ really_inline double compute_float_64(int64_t power, uint64_t i, bool negative,\nuint64_treal_exponent = c.exp- lz;\n//we have to check that real_exponent is in range, otherwise we bail out\nif(unlikely((real_exponent <1) || (real_exponent >2046))) {\n*success =false;\nreturn0;\nreturnfalse;\n}\nmantissa |= real_exponent <<52;\nmantissa |= (((uint64_t)negative) <<63);\ndoubled;\nmemcpy(&d, &mantissa,sizeof(d));\n*success =true;\nreturnd;\nreturntrue;\n}\n\nstaticboolparse_float_strtod(constuint8_t*ptr,double*outDouble) {\n@@ -392,9 +387,8 @@ really_inline error_code write_float(const uint8_t *const src, bool negative, ui\nwriter.skip_double();\nreturnerror;\n}\nboolsuccess =true;\ndoubled =compute_float_64(exponent, i, negative, &success);\nif(!success) {\ndoubled;\nif(!compute_float_64(exponent, i, negative, d)) {\n//we are almost never going to get here.\nif(!parse_float_strtod(src, &d)) {returnINVALID_NUMBER(src); }\n}\n@@ -713,12 +707,10 @@ UNUSED really_inline simdjson_result<double> parse_double(const uint8_t * src) n\n//\n//Assemble (or slow-parse) the float\n//\ndoubled;\nif(likely(!overflow)) {\nboolsuccess =true;\ndoubled =compute_float_64(exponent, i, negative, &success);\nif(success) {returnd; }\nif(compute_float_64(exponent, i, negative, d)) {returnd; }\n}\ndoubled;\nif(!parse_float_strtod(src-negative, &d)) {\nreturnNUMBER_ERROR;\n}\n"}
{"Commit title": "Fix benchfeatures.rb on Ruby 1.9", "Commit body": "@@ -1,3 +1,5 @@\n# encoding: utf-8\n\nclassChunkWriter\ndefinitialize(output_dir,miss_templates,file_size=640*1000,block_size=64)\n@@output_dir=output_dir\n@@ -12,7 +14,7 @@ def prepare_chunk(chunks, include_newline)\nend.join(\"\")\nend\n\ndefwrite_files(filename,start1,repeat1,end1,repeat2:'',include_newline:true)\ndefwrite_files(filename,start1,repeat1,end1,repeat2='',include_newline=true)\nstart1=prepare_chunk(start1,include_newline)\nrepeat1=prepare_chunk(repeat1,include_newline)\nend1=prepare_chunk(end1,include_newline)\n@@ -87,8 +89,8 @@ def write_chunks(file, start1, repeat1, end1, size)\nmiss_templates=File.expand_path(\"miss-templates\",File.dirname(__FILE__))\nDir.mkdir(output_dir)unlessFile.directory?(output_dir)\nw=ChunkWriter.new(output_dir,miss_templates)\nw.write_files\"utf-8\",'[\"֏\",\"֏\",{}',',\"֏\",\"֏\",{}',',\"֏\",\"֏\",\"֏\"]',repeat2:',\"ab\",\"ab\",{}'\nw.write_files\"escape\",'[\"\\\\\"\",\"\\\\\"\",{}',',\"\\\\\"\",\"\\\\\"\",{}',',\"\\\\\"\",\"\\\\\"\",\"\\\\\"\"]',repeat2:',\"ab\",\"ab\",{}'\nw.write_files\"utf-8\",'[\"֏\",\"֏\",{}',',\"֏\",\"֏\",{}',',\"֏\",\"֏\",\"֏\"]',',\"ab\",\"ab\",{}'\nw.write_files\"escape\",'[\"\\\\\"\",\"\\\\\"\",{}',',\"\\\\\"\",\"\\\\\"\",{}',',\"\\\\\"\",\"\\\\\"\",\"\\\\\"\"]',',\"ab\",\"ab\",{}'\nw.write_files\"0-structurals\",'\"ab\"','',''\n# w.write_files \"1-structurals\",  [ '[', '\"ab\"' ], [ ',', '\"ab\"' ], [ ',', '{', '}', ']' ]\n# w.write_files \"2-structurals\",  '[\"ab\"', ',\"ab\"', [',{', '}]']\n"}
{"Commit title": "The jsonstats utility becomes ever more powerful.", "Commit body": "@@ -1,4 +1,5 @@\n#include<iostream>\n#include<set>\n\n#include\"simdjson.h\"\n\n@@ -25,6 +26,7 @@ struct stat_s {\nsize_tunsigned_integer_count;\nsize_tfloat_count;\nsize_tstring_count;\nsize_tstring_byte_count;\nsize_tbackslash_count;\nsize_tnon_ascii_byte_count;\nsize_tobject_count;\n@@ -42,7 +44,11 @@ struct stat_s {\nsize_tmaximum_object_size;\nsize_tmaximum_array_size;\nsize_tstring_maximum_length;\nsize_trepeated_key_byte_count;\n\nboolvalid;\nstd::set<std::string_view> all_keys;\nstd::set<std::string_view> repeated_keys;\n};\n\nusingstat_t=structstat_s;\n@@ -80,6 +86,12 @@ void recurse(simdjson::dom::element element, stat_t &s, size_t depth) {\nsize_tcounter =0;\nfor(auto[key, value] : object) {\ncounter++;\nif(s.all_keys.find(key) != s.all_keys.end()) {\ns.repeated_keys.insert(key);\ns.repeated_key_byte_count+= key.size();\n}else{\ns.all_keys.insert(key);\n}\nif(is_ascii(key)) {\ns.ascii_key_count++;\ns.ascii_string_count++;\n@@ -91,6 +103,7 @@ void recurse(simdjson::dom::element element, stat_t &s, size_t depth) {\ns.string_maximum_length= key.size();\n}\ns.string_count++;\ns.string_byte_count+= key.size();\ns.key_count++;\nrecurse(value, s, depth +1);\n}\n@@ -133,10 +146,9 @@ void recurse(simdjson::dom::element element, stat_t &s, size_t depth) {\nif(is_ascii(v)) {\ns.ascii_string_count++;\n}\nstd::string_view strval;\nelement.get<std::string_view>().tie(strval, error);\nif(strval.size() > s.string_maximum_length) {\ns.string_maximum_length= strval.size();\ns.string_byte_count+= v.size();\nif(v.size() > s.string_maximum_length) {\ns.string_maximum_length= v.size();\n}\n}else{\nstd::cerr <<\"unrecognized node.\"<< std::endl;\n@@ -200,6 +212,7 @@ int main(int argc, char *argv[]) {\n\"unsigned_integer_count\"   = %10zu,\n\"float_count\"              = %10zu,\n\"string_count\"             = %10zu,\n\"string_byte_count\"        = %10zu,\n\"ascii_string_count\"       = %10zu,\n\"string_maximum_length\"    = %10zu,\n\"backslash_count\"          = %10zu,\n@@ -216,15 +229,19 @@ int main(int argc, char *argv[]) {\n\"key_count\"                = %10zu,\n\"ascii_key_count\"          = %10zu,\n\"key_maximum_length\"       = %10zu,\n\"key_distinct_count\"       = %10zu,\n\"repeated_key_distinct_count\"= %10zu,\n\"repeated_key_byte_count\"  = %10zu;\n\"maximum_depth\"            = %10zu\n}\n)\",\ns.integer_count,s.integer32_count,s.unsigned_integer32_count,s.unsigned_integer_count,\ns.float_count, s.string_count, s.ascii_string_count,\ns.float_count, s.string_count, s.string_byte_count, s.ascii_string_count,\ns.string_maximum_length, s.backslash_count, s.non_ascii_byte_count,\ns.object_count, s.maximum_object_size, s.array_count,\ns.maximum_array_size, s.null_count, s.true_count, s.false_count,\ns.byte_count, s.structural_indexes_count, s.key_count,\ns.ascii_key_count, s.key_maximum_length, s.maximum_depth);\ns.ascii_key_count, s.key_maximum_length, s.all_keys.size(), s.repeated_keys.size(),\ns.repeated_key_byte_count, s.maximum_depth);\nreturnEXIT_SUCCESS;\n}"}
{"Commit title": "range utf-8 validation for neon", "Commit body": "@@ -15,171 +15,185 @@\n#include<cstdio>\n#include<cstring>\n\nnamespacesimdjson{\nnamespace{//private namespace\n/*\n* legal utf-8 byte sequence\n* http://www.unicode.org/versions/Unicode6.0.0/ch03.pdf - page 94\n*\n*  Code Points        1st       2s       3s       4s\n* U+0000..U+007F     00..7F\n* U+0080..U+07FF     C2..DF   80..BF\n* U+0800..U+0FFF     E0       A0..BF   80..BF\n* U+1000..U+CFFF     E1..EC   80..BF   80..BF\n* U+D000..U+D7FF     ED       80..9F   80..BF\n* U+E000..U+FFFF     EE..EF   80..BF   80..BF\n* U+10000..U+3FFFF   F0       90..BF   80..BF   80..BF\n* U+40000..U+FFFFF   F1..F3   80..BF   80..BF   80..BF\n* U+100000..U+10FFFF F4       80..8F   80..BF   80..BF\n*\n* Map high nibble of \"First Byte\" to legal character length minus 1\n* 0x00 ~ 0xBF --> 0\n* 0xC0 ~ 0xDF --> 1\n* 0xE0 ~ 0xEF --> 2\n* 0xF0 ~ 0xFF --> 3\n*/\nnamespacesimdjson{\n\n//all byte values must be no larger than 0xF4\nstaticinlinevoidcheck_smaller_than_0xF4(int8x16_tcurrent_bytes,\nint8x16_t*has_error) {\n//unsigned, saturates to 0 below max\n*has_error =vorrq_s8(\n*has_error,vreinterpretq_s8_u8(vqsubq_u8(\nvreinterpretq_u8_s8(current_bytes),vdupq_n_u8(0xF4))));\n}\n\nstaticconstint8_t_nibbles[] = {\n1,1,1,1,1,1,1,1,//0xxx (ASCII)\n0,0,0,0,//10xx (continuation)\n2,2,//110x\n3,//1110\n4,//1111, next should be 0 (not checked here)\nstaticconstuint8_t_first_len_tbl[] = {\n0,0,0,0,0,0,0,0,0,0,0,0,1,1,2,3,\n};\n\nstaticinlineint8x16_tcontinuation_lengths(int8x16_thigh_nibbles) {\nreturnvqtbl1q_s8(vld1q_s8(_nibbles),vreinterpretq_u8_s8(high_nibbles));\n}\n\nstaticinlineint8x16_tcarry_continuations(int8x16_tinitial_lengths,\nint8x16_tprevious_carries) {\n\nint8x16_tright1 =vreinterpretq_s8_u8(vqsubq_u8(\nvreinterpretq_u8_s8(vextq_s8(previous_carries, initial_lengths,16-1)),\nvdupq_n_u8(1)));\nint8x16_tsum =vaddq_s8(initial_lengths, right1);\n\nint8x16_tright2 =vreinterpretq_s8_u8(\nvqsubq_u8(vreinterpretq_u8_s8(vextq_s8(previous_carries, sum,16-2)),\nvdupq_n_u8(2)));\nreturnvaddq_s8(sum, right2);\n}\n\nstaticinlinevoidcheck_continuations(int8x16_tinitial_lengths,\nint8x16_tcarries,\nint8x16_t*has_error) {\n\n//overlap || underlap\n//carry > length && length > 0 || !(carry > length) && !(length > 0)\n//(carries > length) == (lengths > 0)\nuint8x16_toverunder =vceqq_u8(vcgtq_s8(carries, initial_lengths),\nvcgtq_s8(initial_lengths,vdupq_n_s8(0)));\n\n*has_error =vorrq_s8(*has_error,vreinterpretq_s8_u8(overunder));\n}\n\n//when 0xED is found, next byte must be no larger than 0x9F\n//when 0xF4 is found, next byte must be no larger than 0x8F\n//next byte must be continuation, ie sign bit is set, so signed < is ok\nstaticinlinevoidcheck_first_continuation_max(int8x16_tcurrent_bytes,\nint8x16_toff1_current_bytes,\nint8x16_t*has_error) {\nuint8x16_tmaskED =vceqq_s8(off1_current_bytes,vdupq_n_s8(0xED));\nuint8x16_tmaskF4 =vceqq_s8(off1_current_bytes,vdupq_n_s8(0xF4));\n\nuint8x16_tbadfollowED =\nvandq_u8(vcgtq_s8(current_bytes,vdupq_n_s8(0x9F)), maskED);\nuint8x16_tbadfollowF4 =\nvandq_u8(vcgtq_s8(current_bytes,vdupq_n_s8(0x8F)), maskF4);\n\n*has_error =vorrq_s8(\n*has_error,vreinterpretq_s8_u8(vorrq_u8(badfollowED, badfollowF4)));\n}\n\nstaticconstint8_t_initial_mins[] = {\n-128,         -128, -128, -128, -128, -128,\n-128,         -128, -128, -128, -128, -128,//10xx => false\n(int8_t)0xC2, -128,//110x\n(int8_t)0xE1,//1110\n(int8_t)0xF1,\n/*Map \"First Byte\" to 8-th item of range table (0xC2 ~ 0xF4)*/\nstaticconstuint8_t_first_range_tbl[] = {\n0,0,0,0,0,0,0,0,0,0,0,0,8,8,8,8,\n};\n\nstaticconstint8_t_second_mins[] = {\n-128,         -128, -128, -128, -128, -128,\n-128,         -128, -128, -128, -128, -128,//10xx => false\n127,127,//110x => true\n(int8_t)0xA0,//1110\n(int8_t)0x90,\n/*\n* Range table, map range index to min and max values\n* Index 0    : 00 ~ 7F (First Byte, ascii)\n* Index 1,2,3: 80 ~ BF (Second, Third, Fourth Byte)\n* Index 4    : A0 ~ BF (Second Byte after E0)\n* Index 5    : 80 ~ 9F (Second Byte after ED)\n* Index 6    : 90 ~ BF (Second Byte after F0)\n* Index 7    : 80 ~ 8F (Second Byte after F4)\n* Index 8    : C2 ~ F4 (First Byte, non ascii)\n* Index 9~15 : illegal: u >= 255 && u <= 0\n*/\nstaticconstuint8_t_range_min_tbl[] = {\n0x00,0x80,0x80,0x80,0xA0,0x80,0x90,0x80,\n0xC2,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,\n};\nstaticconstuint8_t_range_max_tbl[] = {\n0x7F,0xBF,0xBF,0xBF,0xBF,0x9F,0xBF,0x8F,\n0xF4,0x00,0x00,0x00,0x00,0x00,0x00,0x00,\n};\n\n//map off1_hibits => error condition\n//hibits     off1    cur\n//C       => < C2 && true\n//E       => < E1 && < A0\n//F       => < F1 && < 90\n//else      false && false\nstaticinlinevoidcheck_overlong(int8x16_tcurrent_bytes,\nint8x16_toff1_current_bytes,\nint8x16_thibits,int8x16_tprevious_hibits,\nint8x16_t*has_error) {\nint8x16_toff1_hibits =vextq_s8(previous_hibits, hibits,16-1);\nint8x16_tinitial_mins =\nvqtbl1q_s8(vld1q_s8(_initial_mins),vreinterpretq_u8_s8(off1_hibits));\n\nuint8x16_tinitial_under =vcgtq_s8(initial_mins, off1_current_bytes);\n\nint8x16_tsecond_mins =\nvqtbl1q_s8(vld1q_s8(_second_mins),vreinterpretq_u8_s8(off1_hibits));\nuint8x16_tsecond_under =vcgtq_s8(second_mins, current_bytes);\n*has_error =vorrq_s8(\n*has_error,vreinterpretq_s8_u8(vandq_u8(initial_under, second_under)));\n/*\n* This table is for fast handling four special First Bytes(E0,ED,F0,F4), after\n* which the Second Byte are not 80~BF. It contains \"range index adjustment\".\n* - The idea is to minus byte with E0, use the result(0~31) as the index to\n*   lookup the \"range index adjustment\". Then add the adjustment to original\n*   range index to get the correct range.\n* - Range index adjustment\n*   +------------+---------------+------------------+----------------+\n*   | First Byte | original range| range adjustment | adjusted range |\n*   +------------+---------------+------------------+----------------+\n*   | E0         | 2             | 2                | 4              |\n*   +------------+---------------+------------------+----------------+\n*   | ED         | 2             | 3                | 5              |\n*   +------------+---------------+------------------+----------------+\n*   | F0         | 3             | 3                | 6              |\n*   +------------+---------------+------------------+----------------+\n*   | F4         | 4             | 4                | 8              |\n*   +------------+---------------+------------------+----------------+\n* - Below is a uint8x16x2 table, data is interleaved in NEON register. So I'm\n*   putting it vertically. 1st column is for E0~EF, 2nd column for F0~FF.\n*/\nstaticconstuint8_t_range_adjust_tbl[] = {\n/*index -> 0~15  16~31 <- index*/\n/*E0 ->*/2,3,/*<- F0*/\n0,0,\n0,0,\n0,0,\n0,4,/*<- F4*/\n0,0,\n0,0,\n0,0,\n0,0,\n0,0,\n0,0,\n0,0,\n0,0,\n/*ED ->*/3,0,\n0,0,\n0,0,\n};\n}\n\nstructprocessed_utf_bytes{\nint8x16_traw_bytes;\nint8x16_thigh_nibbles;\nint8x16_tcarried_continuations;\nuint8x16_tinput;\nuint8x16_tfirst_len;\n};\n\nstaticinlinevoidcount_nibbles(int8x16_tbytes,\nstructprocessed_utf_bytes*answer) {\nanswer->raw_bytes= bytes;\nanswer->high_nibbles=\nvreinterpretq_s8_u8(vshrq_n_u8(vreinterpretq_u8_s8(bytes),4));\n}\n\n//check whether the current bytes are valid UTF-8\n//at the end of the function, previous gets updated\nstaticinlinestructprocessed_utf_bytes\ncheck_utf8_bytes(int8x16_tcurrent_bytes,structprocessed_utf_bytes*previous,\nint8x16_t*has_error) {\nstructprocessed_utf_bytespb;\ncount_nibbles(current_bytes, &pb);\n\ncheck_smaller_than_0xF4(current_bytes, has_error);\n\nint8x16_tinitial_lengths =continuation_lengths(pb.high_nibbles);\n\npb.carried_continuations=\ncarry_continuations(initial_lengths, previous->carried_continuations);\n\ncheck_continuations(initial_lengths, pb.carried_continuations, has_error);\n\nint8x16_toff1_current_bytes =\nvextq_s8(previous->raw_bytes, pb.raw_bytes,16-1);\ncheck_first_continuation_max(current_bytes, off1_current_bytes, has_error);\n\ncheck_overlong(current_bytes, off1_current_bytes, pb.high_nibbles,\nprevious->high_nibbles, has_error);\nreturnpb;\ncheck_utf8_bytes(uint8x16_tinput,\nstructprocessed_utf_bytes*previous,\nuint8x16_t*has_error) {\n/*Cached tables*/\nconstuint8x16_tfirst_len_tbl =vld1q_u8(_first_len_tbl);\nconstuint8x16_tfirst_range_tbl =vld1q_u8(_first_range_tbl);\nconstuint8x16_trange_min_tbl =vld1q_u8(_range_min_tbl);\nconstuint8x16_trange_max_tbl =vld1q_u8(_range_max_tbl);\nconstuint8x16x2_trange_adjust_tbl =vld2q_u8(_range_adjust_tbl);\n\n/*Cached values*/\nconstuint8x16_tconst_1 =vdupq_n_u8(1);\nconstuint8x16_tconst_2 =vdupq_n_u8(2);\nconstuint8x16_tconst_e0 =vdupq_n_u8(0xE0);\n\n/*high_nibbles = input >> 4*/\nconstuint8x16_thigh_nibbles =vshrq_n_u8(input,4);\n\n/*first_len = legal character length minus 1*/\n/*0 for 00~7F, 1 for C0~DF, 2 for E0~EF, 3 for F0~FF*/\n/*first_len = first_len_tbl[high_nibbles]*/\nconstuint8x16_tfirst_len =\nvqtbl1q_u8(first_len_tbl, high_nibbles);\n\n/*First Byte: set range index to 8 for bytes within 0xC0 ~ 0xFF*/\n/*range = first_range_tbl[high_nibbles]*/\nuint8x16_trange =vqtbl1q_u8(first_range_tbl, high_nibbles);\n\n/*Second Byte: set range index to first_len*/\n/*0 for 00~7F, 1 for C0~DF, 2 for E0~EF, 3 for F0~FF*/\n/*range |= (first_len, previous->first_len) << 1 byte*/\nrange =\nvorrq_u8(range,vextq_u8(previous->first_len, first_len,15));\n\n/*Third Byte: set range index to saturate_sub(first_len, 1)*/\n/*0 for 00~7F, 0 for C0~DF, 1 for E0~EF, 2 for F0~FF*/\nuint8x16_ttmp1, tmp2;\n/*tmp1 = saturate_sub(first_len, 1)*/\ntmp1 =vqsubq_u8(first_len, const_1);\n/*tmp2 = saturate_sub(previous->first_len, 1)*/\ntmp2 =vqsubq_u8(previous->first_len, const_1);\n/*range |= (tmp1, tmp2) << 2 bytes*/\nrange =vorrq_u8(range,vextq_u8(tmp2, tmp1,14));\n\n/*Fourth Byte: set range index to saturate_sub(first_len, 2)*/\n/*0 for 00~7F, 0 for C0~DF, 0 for E0~EF, 1 for F0~FF*/\n/*tmp1 = saturate_sub(first_len, 2)*/\ntmp1 =vqsubq_u8(first_len, const_2);\n/*tmp2 = saturate_sub(prev_first_len, 2)*/\ntmp2 =vqsubq_u8(previous->first_len, const_2);\n/*range |= (tmp1, tmp2) << 3 bytes*/\nrange =vorrq_u8(range,vextq_u8(tmp2, tmp1,13));\n\n/*\n* Now we have below range indices caluclated\n* Correct cases:\n* - 8 for C0~FF\n* - 3 for 1st byte after F0~FF\n* - 2 for 1st byte after E0~EF or 2nd byte after F0~FF\n* - 1 for 1st byte after C0~DF or 2nd byte after E0~EF or\n*         3rd byte after F0~FF\n* - 0 for others\n* Error cases:\n*   9,10,11 if non ascii First Byte overlaps\n*   E.g., F1 80 C2 90 --> 8 3 10 2, where 10 indicates error\n*/\n\n/*Adjust Second Byte range for special First Bytes(E0,ED,F0,F4)*/\n/*See _range_adjust_tbl[] definition for details*/\n/*Overlaps lead to index 9~15, which are illegal in range table*/\nuint8x16_tshift1 =vextq_u8(previous->input, input,15);\nuint8x16_tpos =vsubq_u8(shift1, const_e0);\nrange =vaddq_u8(range,vqtbl2q_u8(range_adjust_tbl, pos));\n\n/*Load min and max values per calculated range index*/\nuint8x16_tminv =vqtbl1q_u8(range_min_tbl, range);\nuint8x16_tmaxv =vqtbl1q_u8(range_max_tbl, range);\n\n/*Check value range*/\n*has_error =vorrq_u8(*has_error,vcltq_u8(input, minv));\n*has_error =vorrq_u8(*has_error,vcgtq_u8(input, maxv));\n\nprevious->input= input;\nprevious->first_len= first_len;\n\nreturn*previous;\n}\n\ntemplate<>\nstructutf8_checking_state<Architecture::ARM64> {\nint8x16_thas_error{};\nuint8x16_thas_error{};\nprocessed_utf_bytes previous{};\n};\n\n@@ -202,32 +216,28 @@ really_inline void check_utf8<Architecture::ARM64>(\nsimd_input<Architecture::ARM64> in,\nutf8_checking_state<Architecture::ARM64> &state) {\nif(check_ascii_neon(in)) {\n//All bytes are ascii. Therefore the byte that was just before must be\n//ascii too. We only check the byte that was just before simd_input. Nines\n//are arbitrary values.\nconstint8x16_tverror =\n(int8x16_t){9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,1};\nconstuint8x16_tverror =\n(uint8x16_t){9,9,9,9,9,9,9,9,9,9,9,9,9,3,2,1};\nstate.has_error=\nvorrq_s8(vreinterpretq_s8_u8(\nvcgtq_s8(state.previous.carried_continuations, verror)),\nvorrq_u8(vcgtq_u8(state.previous.first_len, verror),\nstate.has_error);\n}else{\n//it is not ascii so we have to do heavy work\nstate.previous=check_utf8_bytes(vreinterpretq_s8_u8(in.i0),\nstate.previous=check_utf8_bytes(in.i0,\n&(state.previous), &(state.has_error));\nstate.previous=check_utf8_bytes(vreinterpretq_s8_u8(in.i1),\nstate.previous=check_utf8_bytes(in.i1,\n&(state.previous), &(state.has_error));\nstate.previous=check_utf8_bytes(vreinterpretq_s8_u8(in.i2),\nstate.previous=check_utf8_bytes(in.i2,\n&(state.previous), &(state.has_error));\nstate.previous=check_utf8_bytes(vreinterpretq_s8_u8(in.i3),\nstate.previous=check_utf8_bytes(in.i3,\n&(state.previous), &(state.has_error));\n}\n}\n\ntemplate<>\nreally_inline ErrorValues check_utf8_errors<Architecture::ARM64>(\nutf8_checking_state<Architecture::ARM64> &state) {\nuint64x2_tv64 =vreinterpretq_u64_s8(state.has_error);\nuint64x2_tv64 =vreinterpretq_u64_u8(state.has_error);\nuint32x2_tv32 =vqmovn_u64(v64);\nuint64x1_tresult =vreinterpret_u64_u32(v32);\nreturnvget_lane_u64(result,0) !=0? simdjson::UTF8_ERROR\n"}
{"Commit title": "Thinking about making the string buffer ASCII and moving the length o…", "Commit body": "@@ -89,20 +89,45 @@ really_inline bool parser::on_null_atom() noexcept {\nreally_inlineuint8_t*parser::on_start_string()noexcept{\n/*we advance the point, accounting for the fact that we have a NULL\n* termination*/\n//If we limit JSON documents to strictly less 4GB of\n//string content, then current_string_buf_loc\n//- doc.string_buf.get() fits in 32 bits. This leaves us\n//three free bytes.\nwrite_tape(current_string_buf_loc - doc.string_buf.get(), internal::tape_type::STRING);\nreturncurrent_string_buf_loc +sizeof(uint32_t);\nreturncurrent_string_buf_loc +sizeof(uint16_t);\n}\n\nreally_inlineboolparser::on_end_string(uint8_t*dst)noexcept{\nuint32_tstr_length =uint32_t(dst - (current_string_buf_loc +sizeof(uint32_t)));\n//TODO check for overflow in case someone has a crazy string (>=4GB?)\n//But only add the overflow check when the document itself exceeds 4GB\n//Currently unneeded because we refuse to parse docs larger or equal to 4GB.\nmemcpy(current_string_buf_loc, &str_length,sizeof(uint32_t));\n\n//We have two scenarios here. Either the string length is\n//less than 0x7fffff in which case, we have room in the string\n//header and all is good. Otherwise, we can encode the\n//string length in the document itself, taking care to\n//ensure that we do so in ASCII.\nif(likely(str_length <=0x7fffff)) {//likely\ndoc.tape[current_loc-1] |=uint64_t(str_length) <<32;\n//we have a string header that must be ASCII, unused in\n//this common case\ncurrent_string_buf_loc[0] =uint8_t(32);//space\ncurrent_string_buf_loc[1] =uint8_t(32);//space\n//we are done!\n}else{\n//oh gosh, we have a long string.\ndoc.tape[current_loc-1] |=uint64_t(0x800000| (str_length >>9)) <<32;\n//we have 9 bits left to code, which we do on the string buffer\n//using two bytes\ncurrent_string_buf_loc[0] =uint8_t(32+ ((str_length &0x1f0) >>4))\ncurrent_string_buf_loc[1] =32+uint8_t(str_length &0xf);\n}\n//NULL termination is still handy if you expect all your strings to\n//be NULL terminated? It comes at a small cost\n*dst =0;\ncurrent_string_buf_loc = dst +1;\n//be NULL terminated? It comes at a small cost and if it is\n//never used, we might as well drop it.\n//*dst = 0;\n//current_string_buf_loc = dst + 1;\nreturntrue;\n}\n\n"}
{"Commit title": "Fix SAX benchmark to actually add tweets", "Commit body": "@@ -233,6 +233,7 @@ simdjson_really_inline bool sax_tweet_reader_visitor::in_container_child(json_it\nsimdjson_really_inlinevoidsax_tweet_reader_visitor::start_container(json_iterator &iter) {\nSIMDJSON_ASSUME(iter.depth<= MAX_SUPPORTED_DEPTH);//Asserts in debug mode\ncontainer =containers(iter.depth);\nif(container == containers::tweet) { tweets.push_back({}); }\nif(logger::LOG_ENABLED) { iter.log_start_value(STATE_NAMES[iter.depth]); }\n}\nsimdjson_really_inlinevoidsax_tweet_reader_visitor::end_container(json_iterator &iter) {\n"}
{"Commit title": "Adding explicit constructor.", "Commit body": "@@ -128,6 +128,7 @@ struct event_collector {\nreturnlinux_events.is_working();\n}\n#else\nevent_collector() {}\nboolhas_events() {\nreturnfalse;\n}\n"}
{"Commit title": "investigate adding BMI2 to the haswell kernel", "Commit body": "@@ -4,7 +4,7 @@\n#include\"simdjson/haswell/intrinsics.h\"\n\n#if!SIMDJSON_CAN_ALWAYS_RUN_HASWELL\nSIMDJSON_TARGET_REGION(\"avx2,bmi,pclmul,lzcnt,popcnt\")\nSIMDJSON_TARGET_REGION(\"avx2,bmi,bmi2,pclmul,lzcnt,popcnt\")\n#endif\n\n#include\"simdjson/haswell/bitmanipulation.h\"\n"}
{"Commit title": "Make a local copy of structural_indexes pointer", "Commit body": "@@ -66,7 +66,7 @@ class json_structural_indexer {\ntemplate<size_tSTEP_SIZE>\nreally_inlinevoidstep(constuint8_t*block, buf_block_reader<STEP_SIZE> &reader)noexcept;\nreally_inlinevoidnext(simd::simd8x64<uint8_t> in, json_block block,size_tidx);\nreally_inline error_codefinish(parser &parser,size_tidx,size_tlen,boolstreaming);\nreally_inline error_codefinish(parser &parser,uint32_t*structural_indexes,size_tidx,size_tlen,boolstreaming);\n\njson_scanner scanner{};\nutf8_checker checker{};\n@@ -83,7 +83,7 @@ really_inline void json_structural_indexer::next(simd::simd8x64<uint8_t> in, jso\nunescaped_chars_error |= block.non_quote_inside_string(unescaped);\n}\n\nreally_inline error_codejson_structural_indexer::finish(parser &parser,size_tidx,size_tlen,boolstreaming) {\nreally_inline error_codejson_structural_indexer::finish(parser &parser,uint32_t*structural_indexes,size_tidx,size_tlen,boolstreaming) {\n//Write out the final iteration's structurals\nindexer.write(uint32_t(idx-64), prev_structurals);\n\n@@ -94,22 +94,22 @@ really_inline error_code json_structural_indexer::finish(parser &parser, size_t\nreturnUNESCAPED_CHARS;\n}\n\nparser.n_structural_indexes=uint32_t(indexer.tail-parser.structural_indexes());\nparser.n_structural_indexes=uint32_t(indexer.tail- structural_indexes);\n/*a valid JSON file cannot have zero structural indexes - we should have\n* found something*/\nif(unlikely(parser.n_structural_indexes==0u)) {\nreturnEMPTY;\n}\nif(unlikely(parser.structural_indexes()[parser.n_structural_indexes-1] > len)) {\nif(unlikely(structural_indexes[parser.n_structural_indexes-1] > len)) {\nreturnUNEXPECTED_ERROR;\n}\nif(len !=parser.structural_indexes()[parser.n_structural_indexes-1]) {\nif(len != structural_indexes[parser.n_structural_indexes-1]) {\n/*the string might not be NULL terminated, but we add a virtual NULL\n* ending character.*/\nparser.structural_indexes()[parser.n_structural_indexes++] =uint32_t(len);\nstructural_indexes[parser.n_structural_indexes++] =uint32_t(len);\n}\n/*make it safe to dereference one beyond this array*/\nparser.structural_indexes()[parser.n_structural_indexes] =0;\nstructural_indexes[parser.n_structural_indexes] =0;\nreturnchecker.errors();\n}\n\n@@ -158,8 +158,10 @@ template<size_t STEP_SIZE>\nerror_codejson_structural_indexer::index(constuint8_t*buf,size_tlen, parser &parser,boolstreaming)noexcept{\nif(unlikely(len > parser.capacity())) {returnCAPACITY; }\n\nautostructural_indexes = parser.structural_indexes();\n\nbuf_block_reader<STEP_SIZE>reader(buf, len);\njson_structural_indexerindexer(parser.structural_indexes());\njson_structural_indexerindexer(structural_indexes);\nwhile(reader.has_full_block()) {\nindexer.step<STEP_SIZE>(reader.full_block(), reader);\n}\n@@ -170,7 +172,7 @@ error_code json_structural_indexer::index(const uint8_t *buf, size_t len, parser\nindexer.step<STEP_SIZE>(block, reader);\n}\n\nreturnindexer.finish(parser, reader.block_index(), len, streaming);\nreturnindexer.finish(parser,structural_indexes,reader.block_index(), len, streaming);\n}\n\n}//namespace stage1"}
{"Commit title": "Parse 18-digit unsigned integers quickly", "Commit body": "@@ -542,6 +542,22 @@ really_inline bool parse_number(UNUSED const uint8_t *const src,\n}\n}else{\nif(unlikely(digit_count >=18)) {//this is uncommon!!!\n//18-digit positive numbers never overflow, but we have to figure out whether they will\n//fit in a signed integer or not.\nif(!negative && digit_count ==18) {\nif(i <=uint64_t(INT64_MAX)) {\nwriter.append_s64(i);\n#ifdefJSON_TEST_NUMBERS//for unit testing\nfound_integer(i, src);\n#endif\n}else{\nwriter.append_u64(i);\n#ifdefJSON_TEST_NUMBERS//for unit testing\nfound_unsigned_integer(i, src);\n#endif\n}\nreturntrue;\n}\n//there is a good chance that we had an overflow, so we need\n//need to recover: we parse the whole thing again.\nboolsuccess =parse_large_integer(src, writer, found_minus);\n"}
{"Commit title": "Redesigning visit_primitive so that it is optimized for strings and", "Commit body": "@@ -303,15 +303,17 @@ simdjson_warn_unused simdjson_inline error_code json_iterator::visit_root_primit\n}\ntemplate<typenameV>\nsimdjson_warn_unused simdjson_inline error_codejson_iterator::visit_primitive(V &visitor,constuint8_t*value)noexcept{\n//Use the fact that most scalars are going to be either strings or numbers.\nif(*value =='\"') {\nreturnvisitor.visit_string(*this, value);\n}elseif(((*value -'0')  <10) || (*value =='-')) {\nreturnvisitor.visit_number(*this, value);\n}\n//true, false, null are uncommon.\nswitch(*value) {\ncase'\"':returnvisitor.visit_string(*this, value);\ncase't':returnvisitor.visit_true_atom(*this, value);\ncase'f':returnvisitor.visit_false_atom(*this, value);\ncase'n':returnvisitor.visit_null_atom(*this, value);\ncase'-':\ncase'0':case'1':case'2':case'3':case'4':\ncase'5':case'6':case'7':case'8':case'9':\nreturnvisitor.visit_number(*this, value);\ndefault:\nlog_error(\"Non-value found when value was expected!\");\nreturnTAPE_ERROR;\n"}
{"Commit title": "update doc/performance.md.", "Commit body": "@@ -160,6 +160,6 @@ You should not expect the simdjson library to cause *downclocking* of your recen\n-[Whenever 512-bit AVX-512 instructions are used](https://lemire.me/blog/2018/09/07/avx-512-when-and-how-to-use-these-new-instructions/).\n-Whenever heavy 256-bit or wider instructions are used. Heavy instructions are those involving floating point operations or integer multiplications (since these execute on the floating point unit).\n\nThe simdjson library does notcurrently support AVX-512 instructions and it does notmake use of heavy 256-bit instructions. We do use vectorized multiplications, but only using 128-bit registers. Thus there should be no downclocking due to simdjson on recent processors.\nThe simdjson library does not make use of heavy 256-bit instructions. We do use vectorized multiplications, but only using 128-bit registers. Thus there should be no downclocking due to simdjson on recent processors, except when AVX-512 is detected.\n\nYou may still be worried about which SIMD instruction set is used by simdjson.  Thankfully,[you can always determine and change which architecture-specific implementation is used](implementation-selection.md)by simdjson. Thus even if your CPU supports AVX2, you do not need to use AVX2. You are in control."}
{"Commit title": "update dependency nlohmann/json for benchmarks to current version 3.10.5", "Commit body": "@@ -70,11 +70,11 @@ int main() {}\ntarget_include_directories(jsmnSYSTEMPUBLIC\"${jsmn_SOURCE_DIR}\")\ntarget_compile_definitions(jsmnINTERFACESIMDJSON_COMPETITION_JSMN)\n\nmessage(STATUS\"Importing json (nlohmann/json@v3.9.1)\")\nmessage(STATUS\"Importing json (nlohmann/json@v3.10.5)\")\nset(nlohmann_json_SOURCE_DIR\"${dep_root}/json\")\nif(NOTEXISTS\"${nlohmann_json_SOURCE_DIR}\")\nfile(DOWNLOAD\n\"https://github.com/nlohmann/json/releases/download/v3.9.1/json.hpp\"\n\"https://github.com/nlohmann/json/releases/download/v3.10.5/json.hpp\"\n\"${nlohmann_json_SOURCE_DIR}/nlohmann/json.hpp\")\nendif()\nadd_library(nlohmann_jsonINTERFACE)\n"}
{"Commit title": "Disabling again for GCC 7", "Commit body": "@@ -180,6 +180,8 @@ simdjson_warn_unused simdjson_really_inline simdjson_result<bool> value_iterator\nreturnfalse;\n}\n\nSIMDJSON_PUSH_DISABLE_WARNINGS\nSIMDJSON_DISABLE_STRICT_OVERFLOW_WARNING\nsimdjson_warn_unused simdjson_really_inline simdjson_result<bool>value_iterator::find_field_unordered_raw(conststd::string_view key)noexcept{\n/**\n* When find_field_unordered_raw is called, we can either be pointing at the\n@@ -367,6 +369,7 @@ simdjson_warn_unused simdjson_really_inline simdjson_result<bool> value_iterator\n//never reach this point.\nreturnfalse;\n}\nSIMDJSON_POP_DISABLE_WARNINGS\n\nsimdjson_warn_unused simdjson_really_inline simdjson_result<raw_json_string>value_iterator::field_key()noexcept{\nassert_at_next();\n"}
{"Commit title": "Disabling again for GCC 7", "Commit body": "@@ -180,6 +180,8 @@ simdjson_warn_unused simdjson_really_inline simdjson_result<bool> value_iterator\nreturnfalse;\n}\n\nSIMDJSON_PUSH_DISABLE_WARNINGS\nSIMDJSON_DISABLE_STRICT_OVERFLOW_WARNING\nsimdjson_warn_unused simdjson_really_inline simdjson_result<bool>value_iterator::find_field_unordered_raw(conststd::string_view key)noexcept{\n/**\n* When find_field_unordered_raw is called, we can either be pointing at the\n@@ -367,6 +369,7 @@ simdjson_warn_unused simdjson_really_inline simdjson_result<bool> value_iterator\n//never reach this point.\nreturnfalse;\n}\nSIMDJSON_POP_DISABLE_WARNINGS\n\nsimdjson_warn_unused simdjson_really_inline simdjson_result<raw_json_string>value_iterator::field_key()noexcept{\nassert_at_next();\n"}
{"Commit title": "Optimized the arm64 implementation of simd8x64::compress", "Commit body": "@@ -57,6 +57,19 @@ simdjson_really_inline uint8x16_t make_uint8x16_t(uint8_t x1,  uint8_t x2,  uint\nreturnx;\n}\n\nsimdjson_really_inlineuint8x8_tmake_uint8x8_t(uint8_tx1,uint8_tx2,uint8_tx3,uint8_tx4,\nuint8_tx5,uint8_tx6,uint8_tx7,uint8_tx8) {\nuint8x8_tx{};\nx =vset_lane_u8(x1, x,0);\nx =vset_lane_u8(x2, x,1);\nx =vset_lane_u8(x3, x,2);\nx =vset_lane_u8(x4, x,3);\nx =vset_lane_u8(x5, x,4);\nx =vset_lane_u8(x6, x,5);\nx =vset_lane_u8(x7, x,6);\nx =vset_lane_u8(x8, x,7);\nreturnx;\n}\n\n//We have to do the same work for make_int8x16_t\nsimdjson_really_inlineint8x16_tmake_int8x16_t(int8_tx1,int8_tx2,int8_tx3,int8_tx4,\n@@ -289,6 +302,27 @@ simdjson_really_inline int8x16_t make_int8x16_t(int8_t x1,  int8_t x2,  int8_t x\nvst1q_u8(reinterpret_cast<uint8_t*>(output), answer);\n}\n\n//Copies all bytes corresponding to a 0 in the low half of the mask (interpreted as a\n//bitset) to output1, then those corresponding to a 0 in the high half to output2.\ntemplate<typenameL>\nsimdjson_really_inlinevoidcompress_halves(uint16_tmask, L *output1, L *output2)const{\nusinginternal::thintable_epi8;\nuint8_tmask1 =uint8_t(mask);//least significant 8 bits\nuint8_tmask2 =uint8_t(mask >>8);//most significant 8 bits\nuint8x8_tcompactmask1 =vcreate_u8(thintable_epi8[mask1]);\nuint8x8_tcompactmask2 =vcreate_u8(thintable_epi8[mask2]);\n//we increment by 0x08 the second half of the mask\n#ifdefSIMDJSON_REGULAR_VISUAL_STUDIO\nuint8x8_tinc =make_uint8x8_t(0x08,0x08,0x08,0x08,0x08,0x08,0x08,0x08);\n#else\nuint8x8_tinc = {0x08,0x08,0x08,0x08,0x08,0x08,0x08,0x08};\n#endif\ncompactmask2 =vadd_u8(compactmask2, inc);\n//store each result (with the second store possibly overlapping the first)\nvst1_u8((uint8_t*)output1,vqtbl1_u8(*this, compactmask1));\nvst1_u8((uint8_t*)output2,vqtbl1_u8(*this, compactmask2));\n}\n\ntemplate<typenameL>\nsimdjson_really_inline simd8<L>lookup_16(\nL replace0,  L replace1,  L replace2,  L replace3,\n@@ -440,10 +474,13 @@ simdjson_really_inline int8x16_t make_int8x16_t(int8_t x1,  int8_t x2,  int8_t x\n\n\nsimdjson_really_inlinevoidcompress(uint64_tmask, T * output)const{\nthis->chunks[0].compress(uint16_t(mask), output);\nthis->chunks[1].compress(uint16_t(mask >>16), output +16-count_ones(mask &0xFFFF));\nthis->chunks[2].compress(uint16_t(mask >>32), output +32-count_ones(mask &0xFFFFFFFF));\nthis->chunks[3].compress(uint16_t(mask >>48), output +48-count_ones(mask &0xFFFFFFFFFFFF));\nuint64_tpopcounts =vget_lane_u64(vreinterpret_u64_u8(vcnt_u8(vcreate_u8(~mask))),0);\n//compute the prefix sum of the popcounts of each byte\nuint64_toffsets = popcounts *0x0101010101010101;\nthis->chunks[0].compress_halves(uint16_t(mask), output, &output[popcounts &0xFF]);\nthis->chunks[1].compress_halves(uint16_t(mask >>16), &output[(offsets >>8) &0xFF], &output[(offsets >>16) &0xFF]);\nthis->chunks[2].compress_halves(uint16_t(mask >>32), &output[(offsets >>24) &0xFF], &output[(offsets >>32) &0xFF]);\nthis->chunks[3].compress_halves(uint16_t(mask >>48), &output[(offsets >>40) &0xFF], &output[(offsets >>48) &0xFF]);\n}\n\nsimdjson_really_inlineuint64_tto_bitmask()const{\n"}
{"Commit title": "Adding test for issue 1729.", "Commit body": "@@ -13,6 +13,145 @@ namespace document_stream_tests {\nASSERT_EQUAL(val,5);\nreturntrue;\n}\nboolissue1729() {\nTEST_START();\nstd::string json =R\"({\n\"t\": 5649,\n\"ng\": 5,\n\"lu\": 4086,\n\"g\": [{\n\"t\": \"Temps\",\n\"xvy\": 0,\n\"pd\": 60,\n\"sz\": 3,\n\"l\": [\n\"Low Temp\",\n\"High Temp\",\n\"Smooth Avg\"\n],\n\"c\": [\n\"green\",\n\"orange\",\n\"cyan\"\n],\n\"d\": [\n80.48750305,\n80.82499694,\n80.65625\n]\n},\n{\n\"t\": \"Pump Status\",\n\"xvy\": 0,\n\"pd\": 60,\n\"sz\": 3,\n\"l\": [\n\"Pump\",\n\"Thermal\",\n\"Light\"\n],\n\"c\": [\n\"green\",\n\"orange\",\n\"cyan\"\n],\n\"d\": [\n0,\n0,\n0\n]\n},\n{\n\"t\": \"Lux\",\n\"xvy\": 0,\n\"pd\": 60,\n\"sz\": 4,\n\"l\": [\n\"Value\",\n\"Smooth\",\n\"Low\",\n\"High\"\n],\n\"c\": [\n\"green\",\n\"orange\",\n\"cyan\",\n\"yellow\"\n],\n\"d\": [\n2274.62939453,\n2277.45947265,\n4050,\n4500\n]\n},\n{\n\"t\": \"Temp Diff\",\n\"xvy\": 0,\n\"pd\": 60,\n\"sz\": 4,\n\"l\": [\n\"dFarenheit\",\n\"sum\",\n\"Low\",\n\"High\"\n],\n\"c\": [\n\"green\",\n\"orange\",\n\"cyan\",\n\"yellow\"\n],\n\"d\": [\n0,\n0,\n0.5,\n10\n]\n},\n{\n\"t\": \"Power\",\n\"xvy\": 0,\n\"pd\": 60,\n\"sz\": 3,\n\"l\": [\n\"watts (est. ligth) \",\n\"watts (1.9~gpm) \",\n\"watts (1gpm) \"\n],\n\"c\": [\n\"green\",\n\"orange\",\n\"cyan\"\n],\n\"d\": [\n181.78063964,\n114.88922882,\n59.35943603\n]\n}\n]\n})\";\n\nondemand::parser parser;\nondemand::document_stream stream;\nautooderror = parser.iterate_many(json).get(stream);\nif(oderror) {\nstd::cerr <<\"ondemand iterate_many error:\"<< oderror << std::endl;\nreturnfalse;\n}\nautoi = stream.begin();\nfor(; i != stream.end(); ++i) {\nondemand::document_reference doc;\nautoerr = (*i).get(doc);\nif(err != SUCCESS) {\nstd::cerr <<\"ondemand iterate_many error:\"<< err << std::endl;\nreturnfalse;\n}\n}\nTEST_SUCCEED();\n}\n\n\nboolissue1683() {\nTEST_START();\n@@ -536,6 +675,7 @@ namespace document_stream_tests {\n\nboolrun() {\nreturn\nissue1729() &&\nfuzzaccess() &&\nissue1683() &&\nissue1668() &&\n"}
{"Commit title": "Adding test.", "Commit body": "@@ -349,6 +349,27 @@ namespace document_stream_tests {\n}\n\n\nboolissue1668() {\nTEST_START();\nautojson =R\"([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100])\"_padded;\n\nondemand::parser odparser;\nondemand::document_stream odstream;\nautooderror = odparser.iterate_many(json.data(), json.length(),50).get(odstream);\n\nif(oderror) { std::cerr <<\"ondemand iterate_many error:\"<< oderror << std::endl;returnfalse; }\n\nfor(auto&doc: odstream) {\nondemand::value val;\nautoerr = doc.at_pointer(\"/40\").get(val);\nif(err) {\nstd::cout <<\"ondemand: error accessing jsonpointer:\"<< err << std::endl;\n}else{\nstd::cout <<\"ondemand:\"<< val << std::endl;\n}\n}\nTEST_SUCCEED();\n}\nbooldocument_stream_utf8_test() {\nTEST_START();\nfflush(NULL);\n@@ -424,6 +445,7 @@ namespace document_stream_tests {\n\nboolrun() {\nreturn\nissue1668() &&\nsimple_document_iteration() &&\nsimple_document_iteration_multiple_batches() &&\nsimple_document_iteration_with_parsing() &&\n"}
{"Commit title": "This makes the float errors explicit.", "Commit body": "@@ -2,6 +2,7 @@\n\n#include<vector>\n#include<sstream>\n#include<limits>\n\ntemplate<typenameT>\nstaticbooldiff_results(benchmark::State &state,constT &result,constT &reference);\n@@ -19,6 +20,24 @@ struct result_differ {\n}\n};\n\ntemplate<>\nboolresult_differ<double>::diff(benchmark::State &state,constdouble&result,constdouble&reference) {\nif(result != reference) {\nstd::stringstream str;\n//We print it out using full precision.\nautoprior_precision = str.precision(std::numeric_limits<double>::max_digits10);\nstr <<\"result incorrect:\"<< result <<\"... reference:\"<< reference;\nstr.precision(prior_precision);//reset to prior state\nstr << std::hexfloat;//If there are floats, we want to see them in hexadecimal form!\nstr <<\"result incorrect (hexadecimal notation):\"<< result <<\"... reference:\"<< reference;\nstr << std::defaultfloat;//reset to prior state\nstate.SkipWithError(str.str().data());\nreturnfalse;\n}\nreturntrue;\n}\n\n\ntemplate<typenameT>\nstructresult_differ<std::vector<T>> {\nstaticbooldiff(benchmark::State &state,conststd::vector<T> &result,conststd::vector<T> &reference) {\n"}
{"Commit title": "Revamp design documentation to match new design", "Commit body": "@@ -29,11 +29,6 @@ auto doc = parser.iterate(json);\nfor(autotweet : doc[\"statuses\"]) {\nstd::string_view text        = tweet[\"text\"];\nstd::string_view screen_name = tweet[\"user\"][\"screen_name\"];\nstd::string_view screen_name;\n{\nondemand::object user        = tweet[\"user\"];\nscreen_name                  = user[\"screen_name\"];\n}\nuint64_t         retweets    = tweet[\"retweet_count\"];\nuint64_t         favorites   = tweet[\"favorite_count\"];\ncout << screen_name << \" (\" << retweets << \" retweets / \" << favorites << \" favorites): \" << text << endl;\n@@ -66,7 +61,10 @@ Such code would be apply to a JSON document such as the following JSON mimicking\n\"retweet_count\":82,\n\"favorite_count\":42\n}\n]\n],\n\"search_metadata\": {\n\"count\":100,\n}\n}\n```\n\n@@ -91,7 +89,6 @@ The On Demand approach is designed around several principles:\n***Validate What You Use:**On Demand deliberately validates the values you use and the structure leading to it, but nothing else. The goal is a guarantee that the value you asked for is the correct one and is not malformed: there must be no confusion over whether you got the right value.\n\n\n\nTo understand why On Demand is different, it is helpful to review the major\napproaches to parsing and parser APIs in use today.\n\n@@ -119,8 +116,7 @@ for (auto tweet : doc[\"statuses\"]) {\nstd::string_view text        = tweet[\"text\"];\nstd::string_view screen_name = tweet[\"user\"][\"screen_name\"];\nuint64_t         retweets    = tweet[\"retweet_count\"];\nuint64_t         favorites   = tweet[\"favorite_count\"];\ncout << screen_name << \" (\" << retweets << \" retweets / \" << favorites << \" favorites): \" << text << endl;\ncout << screen_name << \" (\" << retweets << \" retweets): \" << text << endl;\n}\n```\n\n@@ -273,9 +269,10 @@ To help visualize the algorithm, we'll walk through the example C++ given at the\n```json\n{\n\"statuses\": [\n{ \"id\": 1, \"text\": \"first!\", \"user\": { \"screen_name\": \"lemire\", \"name\": \"Daniel\" }, \"favorite_count\": 100, \"retweet_count\": 40 },\n{ \"id\": 2, \"text\": \"second!\", \"user\": { \"screen_name\": \"jkeiser2\", \"name\": \"John\" }, \"favorite_count\": 2, \"retweet_count\": 3 }\n]\n{ \"id\": 1, \"text\": \"first!\", \"user\": { \"screen_name\": \"lemire\", \"name\": \"Daniel\" }, \"retweet_count\": 40 },\n{ \"id\": 2, \"text\": \"second!\", \"user\": { \"screen_name\": \"jkeiser2\", \"name\": \"John\" }, \"retweet_count\": 3 }\n],\n\"search_metadata\": { \"count\": 2 }\n}\n```\n\n@@ -318,57 +315,84 @@ To help visualize the algorithm, we'll walk through the example C++ given at the\nrely on error chaining, so it is possible to delay error checks: we shall shortly explain error\nchaining more fully.\n\nNOTE: You should always have such a`document`instance (here`doc`) and it should remain in scope for the duration\nof your parsing function. E.g., you should not use the returned document as a temporary (e.g.,`auto x = parser.iterate(json).get_object();`)\nfollowed by other operations as the destruction of the`document`instance makes all of the derived instances\nill-defined.\n>NOTE: You should always have such a`document`instance (here`doc`) and it should remain in scope for the duration\n>of your parsing function. E.g., you should not use the returned document as a temporary (e.g.,`auto x = parser.iterate(json).get_object();`)\n>followed by other operations as the destruction of the`document`instance makes all of the derived instances\n>ill-defined.\n\nAt this point, the iterator is at the start of the JSON:\n\n```json\n{\n^ (depth 1)\n\n\"statuses\": [\n{\"id\":1,\"text\":\"first!\",\"user\": {\"screen_name\":\"lemire\",\"name\":\"Daniel\"},\"retweet_count\":40},\n{\"id\":2,\"text\":\"second!\",\"user\": {\"screen_name\":\"jkeiser2\",\"name\":\"John\"},\"retweet_count\":3}\n],\n\"search_metadata\": {\"count\":2}\n}\n```\n\n3. We iterate over the \"statuses\" field using a typical C++ iterator, reading past the initial\n`{ \"statuses\": [ {`.\n\n```c++\nfor (ondemand::object tweet : doc[\"statuses\"]) {\n```\nThis shorthand does much, and it is helpful to see what it expands to.\nComments in front of each one explain what's going on:\n```c++\n// Validate that the top-level value is an object: check for {\nondemand::object top = doc.get_object();\n\n// Find the field statuses by:\n// 1. Check whether the object is empty (check for }). (We do not really need to do this unless the key lookup fails!)\n// 2. Check if we're at the field by looking for the string \"statuses\" using byte-by-byte comparison.\n// 3. Validate that there is a `:` after it.\nauto tweets_field = top[\"statuses\"];\n\n// Validate that the field value is an array: check for [\n// Also mark the array as finished if there is a ] next, which would cause the while () statement to exit immediately.\nondemand::array tweets = tweets_field.get_array();\n// These three method calls do nothing substantial (the real checking happens in get_array() and ++)\n// != checks whether the array is marked as finished (if we have found a ]).\nondemand::array_iterator tweets_iter = tweets.begin();\nwhile (tweets_iter != tweets.end()) {\nauto tweet_value = *tweets_iter;\n\n// Validate that the array element is an object: check for {\nondemand::object tweet = tweet_value.get_object();\n...\n}\n```\nWhat is not explained in this code expansion is*error chaining*.\nGenerally, you can use`document`methods on a`simdjson_result<...>`value; any errors will\njust be passed down the chain. Many method calls\ncan be chained in this manner. So`for (object tweet : doc[\"statuses\"])`, which is the equivalent of\n`object tweet = *(doc.get_object()[\"statuses\"].get_array().begin()).get_object()`, could fail in any of\n6 method calls, and the error will only be checked at the end,\nwhen you attempt to cast the final`simdjson_result<object>`to object. Upon casting, an exception is\nthrown if there was an error.\n\nNOTE: while the document can be queried once for a key as if it were an object, it is not an actual object\ninstance. If you need to treat it as an object (e.g., to query more than one keys), you can cast it as\nsuch`ondemand::object root_object = doc.get_object();`.\nThis shorthand does a lot, and it is helpful to see what it expands to.\nComments in front of each one explain what's going on:\n\n```c++\n// Validate that the top-level value is an object: check for {. Increase depth to 2 (root > field).\nondemand::object top = doc.get_object();\n\n// Find the field statuses by:\n// 1. Check whether the object is empty (check for }). (We do not really need to do this unless\n//    the key lookup fails!)\n// 2. Check if we're at the field by looking for the string \"statuses\" using byte-by-byte comparison.\n// 3. Validate that there is a `:` after it.\nauto tweets_field = top[\"statuses\"];\n\n// - Validate that the field value is an array: check for [\n// - If the array is empty (if there is a ] next), decrease depth back to 0.\n// - If not, increase depth to 3 (root > statuses > tweet).\nondemand::array tweets = tweets_field.get_array();\n// These three method calls do nothing substantial (the real checking happens in get_array() and ++)\n// != checks whether the array is finished (if we found a ] and decreased depth back to 0).\nondemand::array_iterator tweets_iter = tweets.begin();\nwhile (tweets_iter != tweets.end()) {\nauto tweet_value = *tweets_iter;\n\n// - Validate that the array element is an object: check for {\n// - If the object is empty (if there is a } next), decrease depth back to 1.\n// - If not, increase depth to 4 (root > statuses > tweet > field).\nondemand::object tweet = tweet_value.get_object();\n...\n}\n```\n\n> NOTE: What is not explained in this code expansion is *error chaining*.\n> Generally, you can use `document` methods on a `simdjson_result<...>` value; any errors will\n> just be passed down the chain. Many method calls\n> can be chained in this manner. So `for (object tweet : doc[\"statuses\"])`, which is the equivalent of\n> `object tweet = *(doc.get_object()[\"statuses\"].get_array().begin()).get_object()`, could fail in any of\n> 6 method calls, and the error will only be checked at the end,\n> when you attempt to cast the final `simdjson_result<object>` to object. Upon casting, an exception is\n> thrown if there was an error.\n\n```json\n{\n\"statuses\": [\n{\"id\":1,\"text\":\"first!\",\"user\": {\"screen_name\":\"lemire\",\"name\":\"Daniel\"},\"retweet_count\":40},\n^ (depth 4 - root > statuses > tweet > field)\n\n{\"id\":2,\"text\":\"second!\",\"user\": {\"screen_name\":\"jkeiser2\",\"name\":\"John\"},\"retweet_count\":3}\n],\n\"search_metadata\": {\"count\":2}\n}\n```\n\n4. We get the `\"text\"` field as a string.\n\n@@ -382,45 +406,109 @@ To help visualize the algorithm, we'll walk through the example C++ given at the\n\nThe second field is matched (`\"text\"`), so we validate the `:` and move to the actual value.\n\nNOTE:`[\"text\"]`does a*raw match*, comparing the key directly against the raw JSON. This means\nthat keys with escapes in them may not be matched and the letter case must match exactly.\n> NOTE: `[\"text\"]` does a *raw match*, comparing the key directly against the raw JSON. This\n> allows simdjson to do field lookup very, very quickly when the keys you want to match have\n> letters, numbers and punctuation. However, this means that fields with escapes in them will not\n> be matched.\n\nTo convert to a string, we check for `\"` and use simdjson's fast unescaping algorithm to copy\n`first!` (plus a terminating `\\0`) into a buffer managed by the `document`. This buffer stores\nall strings from a single iteration. The next string will be written after the `\\0`.\n\nA `string_view` is returned which points to that buffer, and contains the length.\n\nWe advance to the comma, and decrease depth to 3 (root > statuses > tweet).\n\nAt this point, we are here in the JSON:\n\n```json\n{\n\"statuses\": [\n{\"id\":1,\"text\":\"first!\",\"user\": {\"screen_name\":\"lemire\",\"name\":\"Daniel\"},\"retweet_count\":40},\n^ (depth 2 - root > statuses > tweet)\n\n{\"id\":2,\"text\":\"second!\",\"user\": {\"screen_name\":\"jkeiser2\",\"name\":\"John\"},\"retweet_count\":3}\n],\n\"search_metadata\": {\"count\":2}\n}\n```\n\n4. We get the `\"screen_name\"` from the `\"user\"` object.\n\n```c++\nondemand::object user        = tweet[\"user\"];\nscreen_name                  = user[\"screen_name\"];\n```\n\nFirst, `[\"user\"]`checks whether there are any more object fields by lookingforeither `,`or\n`}`. Then it matches `\"user\"`andvalidates the `:`.\nFirst, `[\"user\"]`finds the `,`, discovers the next key is `\"user\"`, validates that the `:`\nis there, and increases depth to 4 (root > statuses > tweet > field).\n\n`[\"screen_name\"]` then converts to object, checkingfor`{`, and finds`\"screen_name\"`.\nNext, the cast to ondemand::object checks for `{` and increases depth to 5 (root > statuses >\ntweet > user > field).\n\n`[\"screen_name\"]` finds the first field `\"screen_name\"` and validates the `:`.\n\nTo convert the result to usable string (i.e., the screen name `lemire`), the characters are written to the document's\nstring buffer (after possibly escaping them), which now has *two* string_views pointing into it, and looks like `first!\\0lemire\\0`.\n\nFinally, the temporary user object is destroyed, causing it to skip the remainder of the object\n(`}`).\nThe iterator advances to the comma and decreases depth back to 4 (root > statuses > tweet > user).\n\nAt this point, the iterator is here in the JSON:\n\nNOTE: You may only have one active array or object active at any given time. An array or an object becomes\nactive when the`ondemand::object`or`ondemand::array`is created, and it releases its 'focus' when\nits destructor is called. If you create an array or an object located inside a parent object or array,\nthe child array or object becomes active while the parent becomes temporarily inactive. If you access\nseveral sibling objects or arrays, you must ensure that the destructor is called by scoping each access\n(see Iteration Safety section below for further details).\n```json\n{\n\"statuses\": [\n{\"id\":1,\"text\":\"first!\",\"user\": {\"screen_name\":\"lemire\",\"name\":\"Daniel\"},\"retweet_count\":40},\n^ (depth 4 - root > statuses > tweet > user)\n\n{\"id\":2,\"text\":\"second!\",\"user\": {\"screen_name\":\"jkeiser2\",\"name\":\"John\"},\"retweet_count\":3}\n],\n\"search_metadata\": {\"count\":2}\n}\n```\n\n5.We get`\"retweet_count\"`and`\"favorite_count\"`as unsignedintegers.\n5. We get `\"retweet_count\"` asanunsignedinteger.\n\n```c++\nuint64_t         retweets    = tweet[\"retweet_count\"];\nuint64_tfavorites   = tweet[\"favorite_count\"];\n```\n\nFirst, `[\"retweet_count\"]` checks whether the previous field value is finished (if it was, depth\nwould be 3 (root > statuses > tweet). Since it's not, we skip JSON until depth is 3. This brings\nthe iterator to the `,` after the user object:\n\n```json\n{\n\"statuses\": [\n{\"id\":1,\"text\":\"first!\",\"user\": {\"screen_name\":\"lemire\",\"name\":\"Daniel\"},\"retweet_count\":40},\n^ (depth 4 - root > statuses > tweet > user)\n\n{\"id\":2,\"text\":\"second!\",\"user\": {\"screen_name\":\"jkeiser2\",\"name\":\"John\"},\"retweet_count\":3}\n],\n\"search_metadata\": {\"count\":2}\n}\n```\n\nBecause of the cast to uint64_t, simdjson knows it's parsing an unsigned integer. This lets\nus use a fast parser which *only* knows how to parse digits. It validates that it is an integer\nby rejecting negative numbers, strings, and other values based on the fact that they are not the\ndigits 0-9. This type specificity is part of why parsing with on demand is so fast: you lose all\nthe code that has to understand those other types.\n\nThe iterator is advanced to the `}`, and depth decreased back to 3 (root > statuses > tweet).\n\nAt this point, we are here in the JSON:\n\n```json\n{\n\"statuses\": [\n{\"id\":1,\"text\":\"first!\",\"user\": {\"screen_name\":\"lemire\",\"name\":\"Daniel\"},\"retweet_count\":40},\n^ (depth 3 - root > statuses > tweet)\n\n{\"id\":2,\"text\":\"second!\",\"user\": {\"screen_name\":\"jkeiser2\",\"name\":\"John\"},\"retweet_count\":3}\n],\n\"search_metadata\": {\"count\":2}\n}\n```\n\n6. We loop to the next tweet.\n@@ -441,25 +529,77 @@ To help visualize the algorithm, we'll walk through the example C++ given at the\n}\n```\n\nFirst, the`tweet`destructor runs, skipping the remainder of the object which in this case is\njust`}`.\nFirst, `iter++` (remember, this is the array of tweets) checks whether the previous object was\nfully iterated. It was not--depth is 3 (root > statuses > tweet), so we skip until it's 2--which\nin this case just means consuming the `}`, leaving the iterator at the next comma. Depth is now 2\n(root > statuses).\n\nNext, `iter++` finds the `,` and advances past it to the `{`, increasing depth to 3 (root >\nstatuses > tweet).\n\nFinally, `ondemand::object tweet = *iter` validates the `{` and increases depth to 4 (root >\nstatuses > tweet > field). This leaves the iterator here:\n\n```json\n{\n\"statuses\": [\n{\"id\":1,\"text\":\"first!\",\"user\": {\"screen_name\":\"lemire\",\"name\":\"Daniel\"},\"retweet_count\":40},\n{\"id\":2,\"text\":\"second!\",\"user\": {\"screen_name\":\"jkeiser2\",\"name\":\"John\"},\"retweet_count\":3}\n^ (depth 3 - root > statuses > tweet)\n],\n\"search_metadata\": {\"count\":2}\n}\n```\n\n7. This tweet is processed just like the previous one, leaving the iterator here:\n\n```json\n{\n\"statuses\": [\n{\"id\":1,\"text\":\"first!\",\"user\": {\"screen_name\":\"lemire\",\"name\":\"Daniel\"},\"retweet_count\":40},\n{\"id\":2,\"text\":\"second!\",\"user\": {\"screen_name\":\"jkeiser2\",\"name\":\"John\"},\"retweet_count\":3}\n^ (depth 3 - root > statuses > tweet)\n],\n\"search_metadata\": {\"count\":2}\n}\n```\n\nNext,`iter++`checks whether there are more values and finds`,`. The loop continues.\n8. The loop ends. Recall the relevant parts of the statuses loop:\n\n```c++\nwhile (iter != statuses.end()) {\nondemand::object tweet = *iter;\n...\niter++;\n}\n```\n\nFinally,`ondemand::object tweet = *iter`checks for`{`andreturns the object.\nFirst, `iter++` finishes up any children, consuming the `}` andleaving depth at 2 (root > statuses).\n\nThis tweet is processed just like the previous one.\nNext, `iter++` notices the `]` and ends the array by decreasing depth to 1. This leaves the iterator\nhere in the JSON:\n\n7.We finish the last tweet.\n```json\n{\n\"statuses\": [\n{\"id\":1,\"text\":\"first!\",\"user\": {\"screen_name\":\"lemire\",\"name\":\"Daniel\"},\"retweet_count\":40},\n{\"id\":2,\"text\":\"second!\",\"user\": {\"screen_name\":\"jkeiser2\",\"name\":\"John\"},\"retweet_count\":3}\n],\n^ (depth 1 - root)\n\"search_metadata\": {\"count\":2}\n}\n```\n\nAt the end of the loop, the`tweet`is first destroyed, skipping the remainder of the tweet\nobject (`}`).\n9. The remainder of the file is skipped.\n\nThe`iter++`instruction from`for (ondemand::object tweet : doc[\"statuses\"])`then checks whether there are\nmore values and finds that there are none (`]`). It marks the array iteration as finished and the for\nloop terminates.\nBecause no more action is taken, JSON processing stops: processing only occurs when you ask for\nvalues.\n\nThen the outer object is destroyed, skipping everything up to the`}`.\nThis means you can very efficiently do things like read a single value from a JSON file, or take\nthe top N, for example. It also means the things you don't use won't be fully validated. This is\na general principle of On Demand: don't validate what you don't use. We still fully validate\nvalues you do use, however, as well as the objects and arrays that lead to them, so that you can\nbe sure you get the information you need.\n\nDesign Features\n---------------\n"}
{"Commit title": "Including the array header.", "Commit body": "@@ -1,6 +1,6 @@\n#include<cmath>\n#include<cstdint>\n\n#include<array>\nnamespacesimdjson{\n/*!\nimplements the Grisu2 algorithm for binary to decimal floating-point\n"}
{"Commit title": "Fix outdated cmake version for travis", "Commit body": "@@ -35,8 +35,16 @@ matrix:\n\n\nscript:\n-if [[ \"${TRAVIS_CPU_ARCH}\" == \"ppc64le\" ]]; then\nsudo apt-get install libuv1 rhash libstdc++6;\nwget https://anaconda.org/conda-forge/cmake/3.17.0/download/linux-ppc64le/cmake-3.17.0-hfb1cb51_0.tar.bz2;\nmkdir $HOME/cmake;\ntar -xjvf cmake-3.17.0-hfb1cb51_0.tar.bz2 -C $HOME/cmake;\nexport PATH=$HOME/cmake/bin:$PATH;\nfi\n-eval \"${MATRIX_EVAL}\"\n-export CMAKE_FLAGS=\"-DSIMDJSON_BUILD_STATIC=ON -DSIMDJSON_IMPLEMENTATION=ppc64;fallback\"\n-export CTEST_FLAGS=\"-j4 --output-on-failure -E checkperf\"\n-mkdir build\n-cd build\n-cmake $CMAKE_FLAGS ..\n"}
{"Commit title": "add ndjson to github action fuzzing", "Commit body": "@@ -13,7 +13,7 @@ jobs:\nruns-on:ubuntu-latest\nenv:\n#fuzzers that change behaviour with SIMDJSON_FORCE_IMPLEMENTATION\ndefaultimplfuzzers:atpointer dump dump_raw_tape element minify  parser print_json\ndefaultimplfuzzers:atpointer dump dump_raw_tape element minifyndjsonparser print_json\n#fuzzers that loop over the implementations themselves, or don't need to switch.\nimplfuzzers:implementations minifyimpl ondemand utf8\nimplementations:haswell westmere fallback\n"}
{"Commit title": "Fix SAX benchmark to actually add tweets", "Commit body": "@@ -233,6 +233,7 @@ simdjson_really_inline bool sax_tweet_reader_visitor::in_container_child(json_it\nsimdjson_really_inlinevoidsax_tweet_reader_visitor::start_container(json_iterator &iter) {\nSIMDJSON_ASSUME(iter.depth<= MAX_SUPPORTED_DEPTH);//Asserts in debug mode\ncontainer =containers(iter.depth);\nif(container == containers::tweet) { tweets.push_back({}); }\nif(logger::LOG_ENABLED) { iter.log_start_value(STATE_NAMES[iter.depth]); }\n}\nsimdjson_really_inlinevoidsax_tweet_reader_visitor::end_container(json_iterator &iter) {\n"}
{"Commit title": "fix step value", "Commit body": "@@ -1282,7 +1282,7 @@ std::shared_ptr<sensor_msgs::msg::Image> AirsimROSWrapper::get_depth_img_msg_fro\ndepth_img_msg->data.resize(img_response.image_data_float.size() *sizeof(float));\nmemcpy(depth_img_msg->data.data(), img_response.image_data_float.data(), depth_img_msg->data.size());\ndepth_img_msg->encoding=\"32FC1\";\ndepth_img_msg->step=img_response.image_data_uint8.size() / img_response.height;\ndepth_img_msg->step=depth_img_msg->data.size() / img_response.height;\ndepth_img_msg->is_bigendian=0;\ndepth_img_msg->header.stamp=rclcpp::Time(img_response.time_stamp);\ndepth_img_msg->header.frame_id= frame_id;\n"}
{"Commit title": "Fix clang-format violation", "Commit body": "@@ -347,5 +347,4 @@ void ACameraDirector::notifyViewModeChanged()\nUWorld* world =GetWorld();\nUGameViewportClient* gameViewport = world->GetGameViewport();\ngameViewport->bDisableWorldRendering= nodisplay;\n\n}"}
{"Commit title": "Update drone_env.py", "Commit body": "@@ -27,7 +27,7 @@ def __init__(self, ip_address, step_length, image_shape):\nself._setup_flight()\n\nself.image_request=airsim.ImageRequest(\n3,airsim.ImageType.DepthPerspective,True,False\n\"0\",airsim.ImageType.DepthPerspective,True,False\n)\n\ndef__del__(self):\n@@ -76,7 +76,7 @@ def _do_action(self, action):\nquad_vel.y_val+quad_offset[1],\nquad_vel.z_val+quad_offset[2],\n5,\n).join()\n)\n\ndef_compute_reward(self):\nthresh_dist=7\n"}
{"Commit title": "Use relative line numbers instead of addresses when placing arrows.", "Commit body": "@@ -874,10 +874,18 @@ void DisassemblyLeftPanel::paintEvent(QPaintEvent *event)\n\nusingLineInfo = std::pair<RVA,int>;\nstd::vector<LineInfo> lineOffsets;\nlineOffsets.reserve(lines.size());\nlineOffsets.reserve(lines.size() + arrows.size());\n\nRVA minViewOffset =0, maxViewOffset =0;\n\nif(lines.size() >0) {\nminViewOffset = maxViewOffset = lines[0].offset;\n}\n\nfor(inti =0; i < lines.size(); i++) {\nlineOffsets.emplace_back(lines[i].offset, i);\nminViewOffset =std::min(minViewOffset, lines[i].offset);\nmaxViewOffset =std::max(maxViewOffset, lines[i].offset);\nif(lines[i].arrow!= RVA_INVALID) {\nArrow a { lines[i].offset, lines[i].arrow};\nboolcontains =std::find_if(std::begin(arrows),std::end(arrows),\n@@ -891,24 +899,54 @@ void DisassemblyLeftPanel::paintEvent(QPaintEvent *event)\n}\n}\n\nautoaddOffsetOutsideScreen = [&](RVA offset) {\nif(offset < minViewOffset || offset > maxViewOffset) {\nlineOffsets.emplace_back(offset, -1);\n}\n};\n\n//Assign sequential numbers to offsets outside screen while preserving their relative order.\n//Preserving relative order helps reducing reordering while scrolling. Using sequential numbers\n//allows using data structures designed for dense ranges.\nfor(auto&arrow : arrows) {\naddOffsetOutsideScreen(arrow.min);\naddOffsetOutsideScreen(arrow.max);\n}\nstd::sort(lineOffsets.begin(), lineOffsets.end());\nlineOffsets.erase(std::unique(lineOffsets.begin(), lineOffsets.end()), lineOffsets.end());\nsize_tfirstVisibleLine =std::find_if(lineOffsets.begin(), lineOffsets.end(),\n[](constLineInfo &line) {returnline.second==0; })\n- lineOffsets.begin();\nfor(inti =int(firstVisibleLine) -1; i >=0; i--) {\n//-1 to ensure end of arrrow is drawn outside screen\nlineOffsets[i].second= i - firstVisibleLine -1;\n}\nsize_tfirstLineAfter =\nstd::find_if(lineOffsets.begin(), lineOffsets.end(),\n[&](constLineInfo &line) {returnline.first> maxViewOffset; })\n- lineOffsets.begin();\nfor(size_ti = firstLineAfter; i < lineOffsets.size(); i++) {\nlineOffsets[i].second= lines.size() + (i - firstLineAfter)\n+1;//+1 to ensure end of arrrow is drawn outside screen\n}\n\nautooffsetToLine = [&](RVA offset) ->int{\n//binary search because linesPixPosition is sorted by offset\nif(lineOffsets.empty()) {\nreturn0;\n}\nif(offset < lineOffsets[0].first) {\nreturn-2;\nreturnlineOffsets[0].second-1;\n}\nautores =lower_bound(std::begin(lineOffsets),std::end(lineOffsets), offset,\n[](constLineInfo &it, RVA offset) {returnit.first< offset; });\nif(res ==std::end(lineOffsets)) {\nreturnlines.size()+2;\nreturnlineOffsets.back().second+1;\n}\nreturnres->second;\n};\n\nRVA visibleTop = lineOffsets[0].first, visibleBottom = lineOffsets.back().first;\nautofitsInScreen = [&](constArrow &a) {returnvisibleBottom - visibleTop < a.length(); };\nautofitsInScreen = [&](constArrow &a) {returnmaxViewOffset - minViewOffset < a.length(); };\n\nstd::sort(std::begin(arrows),std::end(arrows), [&](constArrow &l,constArrow &r) {\nintlScreen =fitsInScreen(l), rScreen =fitsInScreen(r);\n@@ -918,25 +956,28 @@ void DisassemblyLeftPanel::paintEvent(QPaintEvent *event)\nreturnl.max!= r.max? l.max< r.max: l.min> r.min;\n});\n\nRVA max =0;\nRVA min = RVA_MAX;\nintminLine =0, maxLine =0;\nfor(auto&it : arrows) {\nmin=std::min(it.min, min);\nmax=std::max(it.max, max);\nminLine=std::min(offsetToLine(it.min), minLine);\nmaxLine=std::max(offsetToLine(it.max), maxLine);\nit.level=0;\n}\n\nconstintMAX_ARROW_LINES =1<<18;\nuint32_tmaxLevel =0;\nif(!arrows.empty()) {\nMinMaxAccumulateTree<uint32_t>maxLevelTree(max - min +2);\nif(!arrows.empty() && maxLine - minLine < MAX_ARROW_LINES) {\n//Limit maximum tree range to MAX_ARROW_LINES as sanity check, since the tree is designed\n//for dense ranges. Under normal conditions due to amount lines fitting screen and number\n//of arrows remembered should be few hundreds at most.\nMinMaxAccumulateTree<uint32_t>maxLevelTree(maxLine - minLine +2);\nfor(Arrow &arrow : arrows) {\nRVAtop = arrow.min>= min ? arrow.min- min +1:0;\nRVAbottom =std::min(arrow.max-min, max - min) +2;\ninttop =offsetToLine(arrow.min) - minLine;\nintbottom =offsetToLine(arrow.max)-minLine +1;\nautominMax = maxLevelTree.rangeMinMax(top, bottom);\nif(minMax.first>1) {\narrow.level=1;\narrow.level=1;//place bellow existing lines\n}else{\narrow.level= minMax.second+1;\narrow.level= minMax.second+1;//place on top of existing lines\nmaxLevel =std::max(maxLevel, arrow.level);\n}\nmaxLevelTree.updateRange(top, bottom, arrow.level);\n@@ -975,7 +1016,7 @@ void DisassemblyLeftPanel::paintEvent(QPaintEvent *event)\n//Draw the lines\np.drawLine(rightOffset, currentLineYPos, rightOffset - lineOffset, currentLineYPos);//left\np.drawLine(rightOffset - lineOffset, currentLineYPos, rightOffset - lineOffset,\nlineArrowY);//horizontal\nlineArrowY);//vertical\n\np.drawLine(rightOffset - lineOffset, lineArrowY, rightOffset, lineArrowY);//right\n\n"}
{"Commit title": "Add large bin foreground analysis issue", "Commit body": "@@ -17,3 +17,10 @@ Some people report that they have keyboard issues. Usually it is because\nthe Xorg layout is wrong. You can check it with: ``setxkbmap -query``\nMost of the time using ``setxkbmap us`` solves the issue, but it might\nnot be enough and require a more advanced Xorg configuration.\n\nInitial Analysis takes a long time\n\nAs background analysis feature is not implemented in Cutter (yet), opening a large\nbinary will take a while to perform initial analysis and Cutter may seem unresponsive.\nWe recommend you use the analysis slider and try different analysis options to find\na balance for both useful analysis and time taken."}
{"Commit title": "add downloaded from website option", "Commit body": "@@ -9,7 +9,7 @@ about: Create a report to help us improve\n*Cutter version:\n*Obtained from:\n-[x]Built from source\n-[ ]Downloaded from release\n-[ ]Downloaded fromwebsite or GitHubrelease\n-[ ]Distribution repository\n*File format:\n\n"}
{"Commit title": "Check clicked mouse button", "Commit body": "@@ -257,7 +257,7 @@ void HexWidget::mousePressEvent(QMouseEvent *event)\nQPointpos(event->pos());\npos.rx() +=horizontalScrollBar()->value();\n\nif(itemArea.contains(pos)) {\nif(event->button() == Qt::LeftButton &&itemArea.contains(pos)) {\nupdatingSelection =true;\nsetCursorAddr(screenPosToAddr(pos));\nselection.init(cursor.addr);\n@@ -267,10 +267,9 @@ void HexWidget::mousePressEvent(QMouseEvent *event)\n\nvoidHexWidget::mouseReleaseEvent(QMouseEvent *event)\n{\nupdatingSelection =false;\n/*if (selection.isValid) {\nqDebug() << \"Selection start\" << QString::number(selection.begin, 16) << \"Selection end\" << QString::number(selection.end, 16);\n}*/\nif(event->button() == Qt::LeftButton) {\nupdatingSelection =false;\n}\n}\n\nvoidHexWidget::wheelEvent(QWheelEvent *event)\n"}
{"Commit title": "Fix alignment of 64bit platforms", "Commit body": "@@ -241,8 +241,12 @@\nalignment. User can customize by defining the RAPIDJSON_ALIGN function macro.,\n*/\n#ifndefRAPIDJSON_ALIGN\n#ifRAPIDJSON_64BIT == 1\n#defineRAPIDJSON_ALIGN(x) ((x +7u) & ~7u)\n#else\n#defineRAPIDJSON_ALIGN(x) ((x +3u) & ~3u)\n#endif\n#endif\n\n///////////////////////////////////////////////////////////////////////////////\n//RAPIDJSON_UINT64_C2\n"}
{"Commit title": "Fix compilation errors on 32-bit gcc/clang", "Commit body": "@@ -579,7 +579,7 @@ class GenericValue {\nbreak;\n\ncasekCopyStringFlag:\nAllocator::Free(GetStringPointer());\nAllocator::Free(const_cast<Ch*>(GetStringPointer()));\nbreak;\n\ndefault:\n@@ -1654,9 +1654,9 @@ class GenericValue {\n#ifRAPIDJSON_48BITPOINTER_OPTIMIZATION\ncharpayload[sizeof(SizeType) *2+6];//2 x SizeType + lower 48-bit pointer\n#elifRAPIDJSON_64BIT\ncharpayload[Sizeof(SizeType) *2+sizeof(void*) +6];//6 padding bytes\ncharpayload[sizeof(SizeType) *2+sizeof(void*) +6];//6 padding bytes\n#else\ncharpayload[Sizeof(SizeType) *2+sizeof(void*) +2];//2 padding bytes\ncharpayload[sizeof(SizeType) *2+sizeof(void*) +2];//2 padding bytes\n#endif\nuint16_tflags;\n};\n@@ -1731,12 +1731,12 @@ class GenericValue {\nFlag f;\n};//16 bytes in 32-bit mode, 24 bytes in 64-bit mode, 16 bytes in 64-bit with RAPIDJSON_48BITPOINTER_OPTIMIZATION\n\nRAPIDJSON_FORCEINLINE Ch*GetStringPointer()const{returnRAPIDJSON_GETPOINTER(Ch, data_.s.str); }\nRAPIDJSON_FORCEINLINEconstCh*GetStringPointer()const{returnRAPIDJSON_GETPOINTER(Ch, data_.s.str); }\nRAPIDJSON_FORCEINLINEconstCh*SetStringPointer(constCh* str) {returnRAPIDJSON_SETPOINTER(Ch, data_.s.str, str); }\nRAPIDJSON_FORCEINLINE GenericValue*GetElementsPointer()const{returnRAPIDJSON_GETPOINTER(GenericValue, data_.a.elements); }\nRAPIDJSON_FORCEINLINEconstGenericValue*SetElementsPointer(constGenericValue* elements) {returnRAPIDJSON_SETPOINTER(GenericValue, data_.a.elements, elements); }\nRAPIDJSON_FORCEINLINE GenericValue*SetElementsPointer(GenericValue* elements) {returnRAPIDJSON_SETPOINTER(GenericValue, data_.a.elements, elements); }\nRAPIDJSON_FORCEINLINE Member*GetMembersPointer()const{returnRAPIDJSON_GETPOINTER(Member, data_.o.members); }\nRAPIDJSON_FORCEINLINEconstMember*SetMembersPointer(constMember* members) {returnRAPIDJSON_SETPOINTER(Member, data_.o.members, members); }\nRAPIDJSON_FORCEINLINE Member*SetMembersPointer(Member* members) {returnRAPIDJSON_SETPOINTER(Member, data_.o.members, members); }\n\n//Initialize this value as array with initial data, without calling destructor.\nvoidSetArrayRaw(GenericValue* values, SizeType count, Allocator& allocator) {\n"}
{"Commit title": "Fix alignment of 64bit platforms", "Commit body": "@@ -241,8 +241,12 @@\nalignment. User can customize by defining the RAPIDJSON_ALIGN function macro.,\n*/\n#ifndefRAPIDJSON_ALIGN\n#ifRAPIDJSON_64BIT == 1\n#defineRAPIDJSON_ALIGN(x) ((x +7u) & ~7u)\n#else\n#defineRAPIDJSON_ALIGN(x) ((x +3u) & ~3u)\n#endif\n#endif\n\n///////////////////////////////////////////////////////////////////////////////\n//RAPIDJSON_UINT64_C2\n"}
{"Commit title": "Added the ability to handle parsing of multiline string values.", "Commit body": "@@ -152,7 +152,8 @@ enum ParseFlag {\nkParseCommentsFlag=32,//!< Allow one-line (//) and multi-line (/**/) comments.\nkParseNumbersAsStringsFlag=64,//!< Parse all numbers (ints/doubles) as strings.\nkParseTrailingCommasFlag=128,//!< Allow trailing commas at the end of objects and arrays.\nkParseNanAndInfFlag=256,//!< Allow parsing NaN, Inf, Infinity, -Inf and -Infinity as doubles.\nkParseNanAndInfFlag=256,//!< Allow parsing NaN, Inf, Infinity, -Inf and -Infinity as doubles.\nkParseMultiLineValueFlag=512,//!< Allow parsing multi-line string values\nkParseDefaultFlags= RAPIDJSON_PARSE_DEFAULT_FLAGS//!< Default parse flags. Can be customized by defining RAPIDJSON_PARSE_DEFAULT_FLAGS\n};\n\n@@ -973,7 +974,7 @@ class GenericReader {\n}\nelse{\nStackStream<typenameTargetEncoding::Ch>stackStream(stack_);\nParseStringToStream<parseFlags, SourceEncoding, TargetEncoding>(s, stackStream);\nParseStringToStream<parseFlags, SourceEncoding, TargetEncoding>(s, stackStream, isKey);\nRAPIDJSON_PARSE_ERROR_EARLY_RETURN_VOID;\nSizeType length =static_cast<SizeType>(stackStream.Length()) -1;\nconsttypenameTargetEncoding::Ch*conststr = stackStream.Pop();\n@@ -986,7 +987,7 @@ class GenericReader {\n//Parse string to an output is\n//This function handles the prefix/suffix double quotes, escaping, and optional encoding validation.\ntemplate<unsignedparseFlags,typenameSEncoding,typenameTEncoding,typenameInputStream,typenameOutputStream>\nRAPIDJSON_FORCEINLINEvoidParseStringToStream(InputStream& is, OutputStream& os) {\nRAPIDJSON_FORCEINLINEvoidParseStringToStream(InputStream& is, OutputStream& os,boolisKey =false) {\n//!@cond RAPIDJSON_HIDDEN_FROM_DOXYGEN\n#defineZ160,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\nstaticconstcharescape[256] = {\n@@ -1039,7 +1040,17 @@ class GenericReader {\n}\nelseif(RAPIDJSON_UNLIKELY(static_cast<unsigned>(c) <0x20)) {//RFC 4627: unescaped = %x20-21 / %x23-5B / %x5D-10FFFF\nif(c =='\\0')\nRAPIDJSON_PARSE_ERROR(kParseErrorStringMissQuotationMark, is.Tell());\nRAPIDJSON_PARSE_ERROR(kParseErrorStringMissQuotationMark, is.Tell());\nelseif(!isKey && c =='\\n'&& (parseFlags &kParseMultiLineValueFlag)) {\nis.Take();\nos.Put('\\\\');\nos.Put('n');\n}\nelseif(!isKey && c =='\\t'&& (parseFlags &kParseMultiLineValueFlag)) {\nis.Take();\nos.Put('\\\\');\nos.Put('t');\n}\nelse\nRAPIDJSON_PARSE_ERROR(kParseErrorStringInvalidEncoding, is.Tell());\n}\n"}
{"Commit title": "Avoid pointer arithmetic on null pointer to remove undefined behavior", "Commit body": "@@ -17,6 +17,7 @@\n\n#include\"../allocators.h\"\n#include\"swap.h\"\n#include<cstddef>\n\n#ifdefined(__clang__)\nRAPIDJSON_DIAG_PUSH\n@@ -114,7 +115,7 @@ class Stack {\ntemplate<typenameT>\nRAPIDJSON_FORCEINLINEvoidReserve(size_tcount =1) {\n//Expand the stack if needed\nif(RAPIDJSON_UNLIKELY(stackTop_ +sizeof(T) * count > stackEnd_))\nif(RAPIDJSON_UNLIKELY(static_cast<std::ptrdiff_t>(sizeof(T) * count)>(stackEnd_- stackTop_)))\nExpand<T>(count);\n}\n\n@@ -127,7 +128,7 @@ class Stack {\ntemplate<typenameT>\nRAPIDJSON_FORCEINLINE T*PushUnsafe(size_tcount =1) {\nRAPIDJSON_ASSERT(stackTop_);\nRAPIDJSON_ASSERT(stackTop_ +sizeof(T) * count <= stackEnd_);\nRAPIDJSON_ASSERT(static_cast<std::ptrdiff_t>(sizeof(T) * count)<=(stackEnd_- stackTop_));\nT* ret =reinterpret_cast<T*>(stackTop_);\nstackTop_ +=sizeof(T) * count;\nreturnret;\n"}
{"Commit title": "Fixed gcc effc++ warning in dtoa.h", "Commit body": "@@ -16,6 +16,11 @@\nnamespacerapidjson{\nnamespaceinternal{\n\n#ifdef__GNUC__\nRAPIDJSON_DIAG_PUSH\nRAPIDJSON_DIAG_OFF(effc++)\n#endif\n\nstructDiyFp{\nDiyFp() {}\n\n@@ -408,6 +413,10 @@ inline char* dtoa(double value, char* buffer) {\n}\n}\n\n#ifdef__GNUC__\nRAPIDJSON_DIAG_POP\n#endif\n\n}//namespace internal\n}//namespace rapidjson\n\n"}
{"Commit title": "GenericDocument: forward allocator to GenericReader", "Commit body": "@@ -719,7 +719,7 @@ class GenericDocument : public GenericValue<Encoding, Allocator> {\ntemplate<unsignedparseFlags,typenameSourceEncoding,typenameInputStream>\nGenericDocument&ParseStream(InputStream& is) {\nValueType::SetNull();//Remove existing root if exist\nGenericReader<SourceEncoding, Encoding, Allocator> reader;\nGenericReader<SourceEncoding, Encoding, Allocator>reader(&GetAllocator());\nif(reader.templateParse<parseFlags>(is, *this)) {\nRAPIDJSON_ASSERT(stack_.GetSize() ==sizeof(ValueType));//Got one and only one root object\nthis->RawAssign(*stack_.templatePop<ValueType>(1));//Add this-> to prevent issue 13.\n"}
{"Commit title": "use a faster log2 > log10 conversion without floats", "Commit body": "@@ -238,13 +238,11 @@ inline DiyFp GetCachedPowerByIndex(size_t index) {\ninlineDiyFpGetCachedPower(inte,int* K) {\n\n//int k = static_cast<int>(ceil((-61 - e) * 0.30102999566398114)) + 374;\ndoubledk = (-61- e) *0.30102999566398114+347;//dk must be positive, so can do ceiling in positive\nintk =static_cast<int>(dk);\nif(dk - k >0.0)\nk++;\n/*78913 ~ log(2) * 2^18*/\nunsignedk =347U+8U-static_cast<unsigned>(((61+ e) *78913) >>18);\n\nunsignedindex=static_cast<unsigned>((k >>3) +1);\n*K =-(-348+static_cast<int>(index<<3));//decimal exponent no need lookup table\nunsignedindex=k /8U;\n*K =348-static_cast<int>(k & ~7U);//decimal exponent no need lookup table\n\nreturnGetCachedPowerByIndex(index);\n}\n"}
{"Commit title": "Added the ability to handle parsing of multiline string values.", "Commit body": "@@ -152,7 +152,8 @@ enum ParseFlag {\nkParseCommentsFlag=32,//!< Allow one-line (//) and multi-line (/**/) comments.\nkParseNumbersAsStringsFlag=64,//!< Parse all numbers (ints/doubles) as strings.\nkParseTrailingCommasFlag=128,//!< Allow trailing commas at the end of objects and arrays.\nkParseNanAndInfFlag=256,//!< Allow parsing NaN, Inf, Infinity, -Inf and -Infinity as doubles.\nkParseNanAndInfFlag=256,//!< Allow parsing NaN, Inf, Infinity, -Inf and -Infinity as doubles.\nkParseMultiLineValueFlag=512,//!< Allow parsing multi-line string values\nkParseDefaultFlags= RAPIDJSON_PARSE_DEFAULT_FLAGS//!< Default parse flags. Can be customized by defining RAPIDJSON_PARSE_DEFAULT_FLAGS\n};\n\n@@ -973,7 +974,7 @@ class GenericReader {\n}\nelse{\nStackStream<typenameTargetEncoding::Ch>stackStream(stack_);\nParseStringToStream<parseFlags, SourceEncoding, TargetEncoding>(s, stackStream);\nParseStringToStream<parseFlags, SourceEncoding, TargetEncoding>(s, stackStream, isKey);\nRAPIDJSON_PARSE_ERROR_EARLY_RETURN_VOID;\nSizeType length =static_cast<SizeType>(stackStream.Length()) -1;\nconsttypenameTargetEncoding::Ch*conststr = stackStream.Pop();\n@@ -986,7 +987,7 @@ class GenericReader {\n//Parse string to an output is\n//This function handles the prefix/suffix double quotes, escaping, and optional encoding validation.\ntemplate<unsignedparseFlags,typenameSEncoding,typenameTEncoding,typenameInputStream,typenameOutputStream>\nRAPIDJSON_FORCEINLINEvoidParseStringToStream(InputStream& is, OutputStream& os) {\nRAPIDJSON_FORCEINLINEvoidParseStringToStream(InputStream& is, OutputStream& os,boolisKey =false) {\n//!@cond RAPIDJSON_HIDDEN_FROM_DOXYGEN\n#defineZ160,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\nstaticconstcharescape[256] = {\n@@ -1039,7 +1040,17 @@ class GenericReader {\n}\nelseif(RAPIDJSON_UNLIKELY(static_cast<unsigned>(c) <0x20)) {//RFC 4627: unescaped = %x20-21 / %x23-5B / %x5D-10FFFF\nif(c =='\\0')\nRAPIDJSON_PARSE_ERROR(kParseErrorStringMissQuotationMark, is.Tell());\nRAPIDJSON_PARSE_ERROR(kParseErrorStringMissQuotationMark, is.Tell());\nelseif(!isKey && c =='\\n'&& (parseFlags &kParseMultiLineValueFlag)) {\nis.Take();\nos.Put('\\\\');\nos.Put('n');\n}\nelseif(!isKey && c =='\\t'&& (parseFlags &kParseMultiLineValueFlag)) {\nis.Take();\nos.Put('\\\\');\nos.Put('t');\n}\nelse\nRAPIDJSON_PARSE_ERROR(kParseErrorStringInvalidEncoding, is.Tell());\n}\n"}
{"Commit title": "Avoid pointer arithmetic on null pointer to remove undefined behavior", "Commit body": "@@ -17,6 +17,7 @@\n\n#include\"../allocators.h\"\n#include\"swap.h\"\n#include<cstddef>\n\n#ifdefined(__clang__)\nRAPIDJSON_DIAG_PUSH\n@@ -114,7 +115,7 @@ class Stack {\ntemplate<typenameT>\nRAPIDJSON_FORCEINLINEvoidReserve(size_tcount =1) {\n//Expand the stack if needed\nif(RAPIDJSON_UNLIKELY(stackTop_ +sizeof(T) * count > stackEnd_))\nif(RAPIDJSON_UNLIKELY(static_cast<std::ptrdiff_t>(sizeof(T) * count)>(stackEnd_- stackTop_)))\nExpand<T>(count);\n}\n\n@@ -127,7 +128,7 @@ class Stack {\ntemplate<typenameT>\nRAPIDJSON_FORCEINLINE T*PushUnsafe(size_tcount =1) {\nRAPIDJSON_ASSERT(stackTop_);\nRAPIDJSON_ASSERT(stackTop_ +sizeof(T) * count <= stackEnd_);\nRAPIDJSON_ASSERT(static_cast<std::ptrdiff_t>(sizeof(T) * count)<=(stackEnd_- stackTop_));\nT* ret =reinterpret_cast<T*>(stackTop_);\nstackTop_ +=sizeof(T) * count;\nreturnret;\n"}
{"Commit title": "Fix alignment of 64bit platforms", "Commit body": "@@ -241,8 +241,12 @@\nalignment. User can customize by defining the RAPIDJSON_ALIGN function macro.,\n*/\n#ifndefRAPIDJSON_ALIGN\n#ifRAPIDJSON_64BIT == 1\n#defineRAPIDJSON_ALIGN(x) ((x +7u) & ~7u)\n#else\n#defineRAPIDJSON_ALIGN(x) ((x +3u) & ~3u)\n#endif\n#endif\n\n///////////////////////////////////////////////////////////////////////////////\n//RAPIDJSON_UINT64_C2\n"}
{"Commit title": "avoid gcc parse errors", "Commit body": "@@ -404,7 +404,7 @@ class GenericPointer {\nreturntokens_[i].index< rhs.tokens_[i].index;\n\nif(tokens_[i].length!= rhs.tokens_[i].length)\nreturntokens_[i].length< rhs.tokens_[i].length;\nreturn(tokens_[i].length)<(rhs.tokens_[i].length);\n\nif(intcmp =std::memcmp(tokens_[i].name, rhs.tokens_[i].name,sizeof(Ch) * tokens_[i].length))\nreturncmp <0;\n"}
{"Commit title": "Fix alignment of 64bit platforms", "Commit body": "@@ -241,8 +241,12 @@\nalignment. User can customize by defining the RAPIDJSON_ALIGN function macro.,\n*/\n#ifndefRAPIDJSON_ALIGN\n#ifRAPIDJSON_64BIT == 1\n#defineRAPIDJSON_ALIGN(x) ((x +7u) & ~7u)\n#else\n#defineRAPIDJSON_ALIGN(x) ((x +3u) & ~3u)\n#endif\n#endif\n\n///////////////////////////////////////////////////////////////////////////////\n//RAPIDJSON_UINT64_C2\n"}
{"Commit title": "It is sufficient to check finish state in iterative parsing.", "Commit body": "@@ -968,7 +968,7 @@ class GenericReader {\n}\n\nstack_.Clear();\nreturnstate == IterativeParsingFinishState&& !HasParseError();\nreturnstate == IterativeParsingFinishState;\n}\n\nstaticconstsize_tkDefaultStackCapacity=256;//!< Default stack capacity in bytes for storing a single decoded string.\n"}
{"Commit title": "fix version number in package.json", "Commit body": "@@ -1,6 +1,6 @@\n{\n\"name\":\"rapidjson\",\n\"version\":\"1.0.4\",\n\"version\":\"1.1.0\",\n\"description\":\"![](doc/logo/rapidjson.png)\",\n\"main\":\"include_dirs.js\",\n\"directories\": {\n"}
{"Commit title": "Pass GCC compilation", "Commit body": "@@ -80,7 +80,7 @@ static bool IsUTF8(unsigned char* s) {\n}\n\nTEST_F(Misc, Hoehrmann_IsUTF8) {\nfor(inti =0; i <kTrialCount; i++) {\nfor(size_ti =0; i <kTrialCount; i++) {\nEXPECT_TRUE(IsUTF8((unsignedchar*)json_));\n}\n}\n@@ -135,10 +135,15 @@ inline unsigned CountDecimalDigit_fast(unsigned n) {\n1000000000\n};\n\n#ifdefined(_M_IX86) || defined(_M_X64)\nunsignedlongi =0;\n//uint32_t t = (32 - __builtin_clz(n | 1)) * 1233 >> 12;\n_BitScanReverse(&i, n |1);\nuint32_tt = (i +1) *1233>>12;\n#elifdefined(__GNUC__)\nuint32_tt = (32-__builtin_clz(n |1)) *1233>>12;\n#else\n#error\n#endif\nreturnt - (n < powers_of_10[t]) +1;\n}\n\n@@ -166,22 +171,24 @@ inline unsigned CountDecimalDigit64_fast(uint64_t n) {\n10000000000000000000U\n};\n\nunsignedlongi =0;\n//uint32_t t = (32 - __builtin_clz(n | 1)) * 1233 >> 12;\n\n#if_M_IX86\n#ifdefined(_M_IX86)\nuint64_tm = n |1;\nunsignedlongi =0;\nif(_BitScanReverse(&i, m >>32))\ni +=32;\nelse\n_BitScanReverse(&i, m &0xFFFFFFFF);\n#elif_M_X64\nuint32_tt = (i +1) *1233>>12;\n#elifdefined(_M_X64)\nunsignedlongi =0;\n_BitScanReverse64(&i, n |1);\nuint32_tt = (i +1) *1233>>12;\n#elifdefined(__GNUC__)\nuint32_tt = (64-__builtin_clzll(n |1)) *1233>>12;\n#else\n#error\n#endif\n\nuint32_tt = (i +1) *1233>>12;\nreturnt - (n < powers_of_10[t]) +1;\n}\n\n@@ -223,7 +230,7 @@ TEST_F(Misc, CountDecimalDigit_fast) {\nTEST_F(Misc, CountDecimalDigit64_VerifyFast) {\nuint64_ti =1, j;\ndo{\nprintf(\"%\"PRIu64\"\\n\", i);\n//printf(\"%\" PRIu64 \"\\n\", i);\nASSERT_EQ(CountDecimalDigit64_enroll4(i),CountDecimalDigit64_fast(i));\nj = i;\ni *=3;\n@@ -350,8 +357,8 @@ static const char digits[201] =\n\"8081828384858687888990919293949596979899\";\n\n//Prevent code being optimized out\n#defineOUTPUT_LENGTH(length) printf(\"\", length)\n//#define OUTPUT_LENGTH(length) printf(\"%d\\n\", length)\n//#define OUTPUT_LENGTH(length) printf(\"\", length)\n#defineOUTPUT_LENGTH(length) printf(\"%d\\n\", length)\n\ntemplate<typenameOutputStream>\nclassWriter1{\n@@ -794,8 +801,8 @@ void itoa_Writer_StringBuffer() {\nrapidjson::StringBuffer sb;\nWriterwriter(sb);\n\nfor(inti =0; i <kItoaTrialCount; i++) {\nfor(intj =0; j < randvalCount; j++) {\nfor(size_ti =0; i <kItoaTrialCount; i++) {\nfor(size_tj =0; j < randvalCount; j++) {\nwriter.WriteInt(randval[j]);\nlength += sb.GetSize();\nsb.Clear();\n@@ -810,8 +817,8 @@ void itoa_Writer_InsituStringStream() {\n\ncharbuffer[32];\nWriter writer;\nfor(inti =0; i <kItoaTrialCount; i++) {\nfor(intj =0; j < randvalCount; j++) {\nfor(size_ti =0; i <kItoaTrialCount; i++) {\nfor(size_tj =0; j < randvalCount; j++) {\nrapidjson::InsituStringStreamss(buffer);\nwriter.Reset(ss);\nchar* begin = ss.PutBegin();\n@@ -826,7 +833,7 @@ template <typename Writer>\nvoiditoa64_Writer_StringBufferVerify() {\nrapidjson::StringBuffer sb;\nWriterwriter(sb);\nfor(intj =0; j < randvalCount; j++) {\nfor(size_tj =0; j < randvalCount; j++) {\ncharbuffer[32];\nint64_tx = randval[j] * randval[j];\nsprintf(buffer,\"%\"PRIi64, x);\n@@ -839,7 +846,7 @@ void itoa64_Writer_StringBufferVerify() {\ntemplate<typenameWriter>\nvoiditoa64_Writer_InsituStringStreamVerify() {\nWriter writer;\nfor(intj =0; j < randvalCount; j++) {\nfor(size_tj =0; j < randvalCount; j++) {\ncharbuffer[32];\nint64_tx = randval[j] * randval[j];\nsprintf(buffer,\"%\"PRIi64, x);\n@@ -861,8 +868,8 @@ void itoa64_Writer_StringBuffer() {\nrapidjson::StringBuffer sb;\nWriterwriter(sb);\n\nfor(inti =0; i <kItoaTrialCount; i++) {\nfor(intj =0; j < randvalCount; j++) {\nfor(size_ti =0; i <kItoaTrialCount; i++) {\nfor(size_tj =0; j < randvalCount; j++) {\nwriter.WriteInt64(randval[j] * randval[j]);\nlength += sb.GetSize();\nsb.Clear();\n@@ -877,8 +884,8 @@ void itoa64_Writer_InsituStringStream() {\n\ncharbuffer[32];\nWriter writer;\nfor(inti =0; i <kItoaTrialCount; i++) {\nfor(intj =0; j < randvalCount; j++) {\nfor(size_ti =0; i <kItoaTrialCount; i++) {\nfor(size_tj =0; j < randvalCount; j++) {\nrapidjson::InsituStringStreamss(buffer);\nwriter.Reset(ss);\nchar* begin = ss.PutBegin();\n@@ -925,8 +932,8 @@ TEST_F(Misc, itoa64_Writer4_InsituStringStream) { itoa64_Writer_InsituStringStre\n\nTEST_F(Misc, itoa_sprintf) {\nsize_tlength =0;\nfor(inti =0; i <kItoaTrialCount; i++) {\nfor(intj =0; j < randvalCount; j++) {\nfor(size_ti =0; i <kItoaTrialCount; i++) {\nfor(size_tj =0; j < randvalCount; j++) {\ncharbuffer[32];\nlength +=sprintf(buffer,\"%d\", randval[j]);\n}\n@@ -936,8 +943,8 @@ TEST_F(Misc, itoa_sprintf) {\n\nTEST_F(Misc, itoa64_sprintf) {\nsize_tlength =0;\nfor(inti =0; i <kItoaTrialCount; i++) {\nfor(intj =0; j < randvalCount; j++) {\nfor(size_ti =0; i <kItoaTrialCount; i++) {\nfor(size_tj =0; j < randvalCount; j++) {\ncharbuffer[32];\nint64_tx = randval[j] * randval[j];\nlength +=sprintf(buffer,\"%\"PRIi64, x);\n@@ -950,8 +957,8 @@ TEST_F(Misc, itoa_strtk) {\nsize_tlength =0;\nstd::string s;\ns.reserve(32);\nfor(inti =0; i <kItoaTrialCount; i++) {\nfor(intj =0; j < randvalCount; j++) {\nfor(size_ti =0; i <kItoaTrialCount; i++) {\nfor(size_tj =0; j < randvalCount; j++) {\ns =strtk::type_to_string(randval[j]);\nlength += s.size();\n}\n@@ -963,8 +970,8 @@ TEST_F(Misc, itoa64_strtk) {\nsize_tlength =0;\nstd::string s;\ns.reserve(32);\nfor(inti =0; i <kItoaTrialCount; i++) {\nfor(intj =0; j < randvalCount; j++) {\nfor(size_ti =0; i <kItoaTrialCount; i++) {\nfor(size_tj =0; j < randvalCount; j++) {\nint64_tx = randval[j] * randval[j];\ns =strtk::type_to_string(x);\nlength += s.size();\n@@ -976,8 +983,8 @@ TEST_F(Misc, itoa64_strtk) {\nTEST_F(Misc, itoa_cppformat) {\nsize_tlength =0;\ncharbuffer[32];\nfor(inti =0; i <kItoaTrialCount; i++) {\nfor(intj =0; j < randvalCount; j++) {\nfor(size_ti =0; i <kItoaTrialCount; i++) {\nfor(size_tj =0; j < randvalCount; j++) {\nchar* p = buffer;\nfmt::FormatDec(p, randval[j]);\nlength += (p - buffer);\n@@ -989,8 +996,8 @@ TEST_F(Misc, itoa_cppformat) {\nTEST_F(Misc, itoa64_cppformat) {\nsize_tlength =0;\ncharbuffer[32];\nfor(inti =0; i <kItoaTrialCount; i++) {\nfor(intj =0; j < randvalCount; j++) {\nfor(size_ti =0; i <kItoaTrialCount; i++) {\nfor(size_tj =0; j < randvalCount; j++) {\nchar* p = buffer;\nint64_tx = randval[j] * randval[j];\nfmt::FormatDec(p, x);\n"}
{"Commit title": "Deprecate SIMD version of SkipWhitespace to prevent BOF", "Commit body": "@@ -501,15 +501,16 @@ inline const char *SkipWhitespace_SIMD(const char* p, const char* end) {\n#endif//RAPIDJSON_NEON\n\n#ifdefRAPIDJSON_SIMD\n//! Template function specialization for InsituStringStream\ntemplate<>inlinevoidSkipWhitespace(InsituStringStream& is) {\nis.src_=const_cast<char*>(SkipWhitespace_SIMD(is.src_));\n}\n\n//! Template function specialization for StringStream\ntemplate<>inlinevoidSkipWhitespace(StringStream& is) {\nis.src_=SkipWhitespace_SIMD(is.src_);\n}\n//!DEPRECATED: 16-bytes null sting check causes buffer overflow\n////! Template function specialization for InsituStringStream\n//template<> inline void SkipWhitespace(InsituStringStream& is) {\n//is.src_ = const_cast<char*>(SkipWhitespace_SIMD(is.src_));\n//}\n\n////! Template function specialization for StringStream\n//template<> inline void SkipWhitespace(StringStream& is) {\n//is.src_ = SkipWhitespace_SIMD(is.src_);\n//}\n\ntemplate<>inlinevoidSkipWhitespace(EncodedInputStream<UTF8<>, MemoryStream>& is) {\nis.is_.src_=SkipWhitespace_SIMD(is.is_.src_, is.is_.end_);\n"}
{"Commit title": "Added the ability to handle parsing of multiline string values.", "Commit body": "@@ -152,7 +152,8 @@ enum ParseFlag {\nkParseCommentsFlag=32,//!< Allow one-line (//) and multi-line (/**/) comments.\nkParseNumbersAsStringsFlag=64,//!< Parse all numbers (ints/doubles) as strings.\nkParseTrailingCommasFlag=128,//!< Allow trailing commas at the end of objects and arrays.\nkParseNanAndInfFlag=256,//!< Allow parsing NaN, Inf, Infinity, -Inf and -Infinity as doubles.\nkParseNanAndInfFlag=256,//!< Allow parsing NaN, Inf, Infinity, -Inf and -Infinity as doubles.\nkParseMultiLineValueFlag=512,//!< Allow parsing multi-line string values\nkParseDefaultFlags= RAPIDJSON_PARSE_DEFAULT_FLAGS//!< Default parse flags. Can be customized by defining RAPIDJSON_PARSE_DEFAULT_FLAGS\n};\n\n@@ -973,7 +974,7 @@ class GenericReader {\n}\nelse{\nStackStream<typenameTargetEncoding::Ch>stackStream(stack_);\nParseStringToStream<parseFlags, SourceEncoding, TargetEncoding>(s, stackStream);\nParseStringToStream<parseFlags, SourceEncoding, TargetEncoding>(s, stackStream, isKey);\nRAPIDJSON_PARSE_ERROR_EARLY_RETURN_VOID;\nSizeType length =static_cast<SizeType>(stackStream.Length()) -1;\nconsttypenameTargetEncoding::Ch*conststr = stackStream.Pop();\n@@ -986,7 +987,7 @@ class GenericReader {\n//Parse string to an output is\n//This function handles the prefix/suffix double quotes, escaping, and optional encoding validation.\ntemplate<unsignedparseFlags,typenameSEncoding,typenameTEncoding,typenameInputStream,typenameOutputStream>\nRAPIDJSON_FORCEINLINEvoidParseStringToStream(InputStream& is, OutputStream& os) {\nRAPIDJSON_FORCEINLINEvoidParseStringToStream(InputStream& is, OutputStream& os,boolisKey =false) {\n//!@cond RAPIDJSON_HIDDEN_FROM_DOXYGEN\n#defineZ160,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\nstaticconstcharescape[256] = {\n@@ -1039,7 +1040,17 @@ class GenericReader {\n}\nelseif(RAPIDJSON_UNLIKELY(static_cast<unsigned>(c) <0x20)) {//RFC 4627: unescaped = %x20-21 / %x23-5B / %x5D-10FFFF\nif(c =='\\0')\nRAPIDJSON_PARSE_ERROR(kParseErrorStringMissQuotationMark, is.Tell());\nRAPIDJSON_PARSE_ERROR(kParseErrorStringMissQuotationMark, is.Tell());\nelseif(!isKey && c =='\\n'&& (parseFlags &kParseMultiLineValueFlag)) {\nis.Take();\nos.Put('\\\\');\nos.Put('n');\n}\nelseif(!isKey && c =='\\t'&& (parseFlags &kParseMultiLineValueFlag)) {\nis.Take();\nos.Put('\\\\');\nos.Put('t');\n}\nelse\nRAPIDJSON_PARSE_ERROR(kParseErrorStringInvalidEncoding, is.Tell());\n}\n"}
{"Commit title": "- Fixed a build issue by initializing \"index\" in the header file", "Commit body": "@@ -899,7 +899,7 @@ class Schema {\n}\n}\n\nSizeTypeindex;\nSizeTypeindex=0;\nif(FindPropertyIndex(ValueType(str, len).Move(), &index)) {\nif(context.patternPropertiesSchemaCount>0) {\ncontext.patternPropertiesSchemas[context.patternPropertiesSchemaCount++] = properties_[index].schema;\n"}
{"Commit title": "Fix -Werror=effc++ errors with GNU 6.3.1", "Commit body": "@@ -16,6 +16,14 @@ struct MyHandler {\nconstchar* type;\nstd::string data;\n\nMyHandler() : type(), data() {Null(); }\nMyHandler(constMyHandler& cpy) : type(cpy.type),data(cpy.data) {}\nMyHandler&operator=(constMyHandler& cpy) {\ntype = cpy.type;\ndata = cpy.data;\nreturn*this;\n}\n\nboolNull() { type =\"Null\"; data.clear();returntrue; }\nboolBool(boolb) { type =\"Bool:\"; data = b?\"true\":\"false\";returntrue; }\nboolInt(inti) { type =\"Int:\"; data =stringify(i);returntrue; }\n"}
{"Commit title": "Fix -Werror=effc++ errors with GNU 6.3.1", "Commit body": "@@ -13,8 +13,8 @@ template <typename T> std::string stringify(T x) {\n}\n\nstructMyHandler{\nconstchar* type;\nstd::string data;\nconstchar* type=NULL;\nstd::string data=\"\";\n\nboolNull() { type =\"Null\"; data.clear();returntrue; }\nboolBool(boolb) { type =\"Bool:\"; data = b?\"true\":\"false\";returntrue; }\n"}
{"Commit title": "Fix a non-type template parameter type mismatch", "Commit body": "@@ -413,7 +413,7 @@ RAPIDJSON_NAMESPACE_END\nRAPIDJSON_NAMESPACE_BEGIN\ntemplate<boolx>structSTATIC_ASSERTION_FAILURE;\ntemplate<>structSTATIC_ASSERTION_FAILURE<true> {enum{ value =1}; };\ntemplate<intx>structStaticAssertTest{};\ntemplate<size_tx>structStaticAssertTest{};\nRAPIDJSON_NAMESPACE_END\n\n#defineRAPIDJSON_JOIN(X, Y) RAPIDJSON_DO_JOIN(X, Y)\n"}
{"Commit title": "readertest: Suppress \"dangling-else\" warning on GCC 7 and later", "Commit body": "@@ -28,7 +28,7 @@ RAPIDJSON_DIAG_PUSH\nRAPIDJSON_DIAG_OFF(effc++)\nRAPIDJSON_DIAG_OFF(float-equal)\nRAPIDJSON_DIAG_OFF(missing-noreturn)\n#if__GNUC__ >=6\n#if__GNUC__ >=7\nRAPIDJSON_DIAG_OFF(dangling-else)\n#endif\n#endif//__GNUC__\n"}
